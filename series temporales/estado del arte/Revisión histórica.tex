\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{csvsimple}
\usepackage[spanish,es-noshorthands,es-ucroman,es-tabla]{babel}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{amsmath}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Estado del arte en predicción de series temporales}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{J.J. Asensio, P.A. Castillo}
\authorrunning{J.J. Asensio et al.}

\institute{
Departamento de Arquitectura y Tecnología de Computadores \\
ETSIIT, CITIC-UGR \\
Universidad de Granada, España \\
\email{\{asensio, pacv\}@ugr.es}
}

\maketitle
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 
bonito abstract test
\end{abstract}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introducción}
\label{sec:intro}
Una serie temporal es una secuencia de observaciones dispuestas a intervalos regulares de aparición. Existen multitud de ejemplos donde se registran datos de esta forma, por citar algunos: precios de acciones en bolsa, temperatura mínima y máxima de cada día, accidentes domésticos… etc.

Aquí se destacan los trabajos de investigación publicados en revistas patrocinadas por el Instituto Internacional de Predictores (IIF), aunque también se cubren publicaciones clave de otras revistas. Se ofrece así una guía selectiva para la literatura en predicción de series temporales, cubriendo el periodo 1982-2005 y resumiendo cerca de 940 artículos que incluyen aproximadamente 340 artículos del IIF. También se revisan artículos y libros clave publicados en otros sitios que han sido altamente influyentes para varios desarrollos en este campo. Los trabajos referidos comprenden 380 artículos de revista y 20 libros y monografías. 

Los artículos se han clasificado primero de acuerdo a los modelos introducidos en la literatura de series temporales, en lugar de hacerlo respecto a los métodos. Por ejemplo, los métodos bayesianos en general puedn aplicarse a todos los modelos. Los artículos que no conciernen con un modelo particular fueron entonces clasificados atendiendo a los diferentes problemas que abordan (medidas de precisión, combinación). 

Puede haber artículos que no aparezcan o que sean citados por otros autores, aunque esto no significa que no sean importantes. Esta revisión es un breve recorrido histórico de los principales desarrollos. 

\section{Alisado exponencial}
\subsection{Preámbulo}
En 1981, los métodos de alisado exponencial a menudo eran considerados como un conjunto de técnicas especiales para extrapolar varios tipos de series temporales univariantes. Si bien los métodos de alisado exponencial eran ampliamente utilizados en la industria y los negocios, habían recibido poca atención por parte de los estadísticos y no tenían un fundamento estadístico bien desrrollado. Estos métodos se originaron en las décadas 50 y 60 con el trabajo de Brown (1959, 1963) \cite{Brown1959} \cite{Brown1963}, Holt (1957, reimpreso en 2004)\cite{Holt20045}, y Winters (1960) \cite{Winters1960324}. Pegels (1969) \cite{Pegels1969311} proporcionó una clasificación sencilla pero útil de la tendencia y los patrones estacionales dependiendo de si eran aditivos (lineales)  o multiplicativos (no lineales).

Muth (1960) \cite{Muth1960299}  fue el primero en sugerir una base estadística para el alisado exponencial simple (SES) demostrando que proporcionaba la predicción óptima para el camino aleatorio con ruido. Los siguientes pasos para poner en un marco estadístico el alisado exponencial fueron proporcionados por Box y Jenkins (1970), Roberts (1982)\cite{Roberts1982808}, y Abraham y Ledolter (1983, 1986) \cite{Abraham1983}\cite{Abraham198651}, quienes mostraron que algunas predicciones de alisado exponencial lineales resultan como casos especiales de los modelos ARIMA. Sin embargo, estos resultados no se extienden a cualquier método de alisado exponencial no lineal.

Los métodos de alisado exponencial recibieron un impulso debido a dos artículos publicados en 1985, los cuales establecieron la base para gran parte del trabajo posterior en el área. En primer lugar, Gardner (1985) \cite{GardnerES19851} realizó una revisión exhaustiva y una síntesis del trabajo en alisado exponencial hasta la fecha y extendió la clasificación de Pegels para incluir  tendencia amortiguada. Este artículo reunió gran cantidad del trabajo existente lo que estimuló el uso de estos métodos y promovió más trabajos de investigación al respecto. Posteriormente ese mismo año, Snyder (1985) \cite{Snyder1985272} demostró que SES podría ser considerado como una forma resultante de una innovación del modelo de espacio de estados (modelo con una única fuente de error). Aunque esta revelación fue en gran parte inadvertida entonces, en años recientes ha proporcionado la base para una gran cantidad de trabajos en modelos de espacio de estados en los que subyacen  los métodos de alisado exponencial.

La mayor parte del trabajo desde 1980 ha incluido el estudio de las propiedades empíricas de los métodos (Bartolomei \& Sweet, 1989 \cite{Bartolomei1989111};  Makridakis \& Hibon, 1991 \cite{Makridakis1991317}), propuestas para nuevos métodos de estimación o inicialización (McClain, 1988 \cite{McClain1988563}; Sweet \& Wilson, 1988 \cite{Sweet1988573}), o ha estado relacionado con modelos estadísticos en los que se considera que subyacen los métodos (McKenzie, 1984 \cite{McKenzie1984333}). 

Los métodos de alisado multiplicativo de Taylor (2003)\cite{Taylor2003715} proporcionan los únicos métodos de alisado exponencial genuinamente nuevos. durante este periodo. Por supuesto, hay habido numerosos estudios que aplican métodos de alisado exponencial en varios contextos incluyendo componentes de computadores (Gardner, 1993 \cite{GardnerJr1993245}) , pasajeros de vuelo (Grubb \& Masa, 2001 \cite{Grubb200171}), y planificación de producción (Miller \& Liberatore, 1993 \cite{Miller1993509}).

La taxonomía de Hyndman, Koehler, Snyder, y Grose (2002 \cite{Hyndman2002439} extendida por Taylor, 2003 \cite{Taylor2003715}) proporciona una categorización útil para describir la variedad de métodos. Cada método se componen de una de cinco tipos de tendencia (ninguna, aditiva, aditiva alisada, multiplicativa y multiplicativa alisada) y uno de tres tipos de estacionalidad (ninguna, aditiva y multiplicativa). Por tanto, hay 15 métodos diferentes, donde entre los más conocidos está SES (sin tendencia ni estacionalidad), el método lineal de Holt (tendencia aditiva, sin estacionalidad), el método aditivo de Holt-Winters (tendencia aditiva y estacionalidad aditiva), y el método no lineal de Holt-Winters (tendencia aditiva y estacionalidad multiplicativa).

\subsection{Variantes}
Se han propuesto numerosas variaciones de los métodos originales. Por ejemplo, Carreno y Madinaveitia (1990) \cite{Carreno1990479} y Williams y Miller (1999) \cite{Williams1999273} propusieron modificaciones para tratar discontinuidades, y Rosas y Guerrero (1994) \cite{Rosas1994515} consideraron predicciones con alisado exponencial sujetas a una o más restricciones. También hay variantes por cómo y cuándo las componentes estacionales deberían ser normalizadas. Lawton (1998) \cite{Lawton1998393} abogó por una renormalización de los índices estacionales para cada periodo de tiempo, puesto que elimina sesgo en las estimaciones de los componentes estacionales y de nivel. Roberts (1982) \cite{Roberts1982808} y McKenzie (1986) \cite{Mckenzie1986373} dieron esquemas ligeramente diferentes de normalización. Archibald y Koehler (2003) \cite{Archibald2003143} desarrollaron nuevas ecuaciones de normalización que eran más sencillas de usar dando el mismo grado de predicción que los métodos originales.
Una variante útil a mitad de camino entre SES y el método de Holt, es SES con desviación. Es equivalente al método de Holt con el parámetro de tendencia a cero. Hyndman y Billah (2003) \cite{Hyndman2003287} mostraron que este método también era equivalente al método Theta de Assimakopoulos y Nikolopoulos (2000) \cite{Assimakopoulos2000521}, cuando el parámetro de desviación es igual a la mitad de la pendiente de una tendencia lineal ajustada a los datos. El método Theta dio un rendimiento extremadamente bueno en la competición M3, aunque el por qué esta particular elección de modelo y parámetros era buena todavía no se ha determinado.
Ha habido extraordinariamente poco trabajo en desarrollar versiones multivariadas de los métodos de alisado exponencial para predicción. Una excepción notable es Pfeffermann y Allon (1989) \cite{Pfeffermann198983} quienes consideraron datos de turismo israelí. El SES Multivariado se usa para gráficos de control de procesos (Pan, 2005) \cite{Pan2005695}, donde es llamado “medias móviles multivariadas ponderadas exponencialmente”, pero aquí la cuestión no es predicción.

\subsection{Modelos de espacio de estados}
Ord, Koehler y Synder (1997) \cite{Ord19971621} a partir del trabajo de Snyder (1985)\cite{Snyder1985272}  propusieron una clase de modelos de espacio de estado de innovaciones en la que se puede considerar que subyacen algunos métodos de alisado exponencial. Hyndman et al (2002) \cite{Hyndman2002439} y Taylor (2003) \cite{Taylor2003715} amplian esto para incluir los 15 métodos de alisado exponencial. De hecho, Hyndman et al (2002) \cite{Hyndman2002439}  propusieron dos modelos de espacio de estado para cada método, correspondientes a los casos de error aditivo y multiplicativo. Estos modelos no son los únicos, y otros modelos relacionados para los métodos de alisado exponencial se presentaron en Koehler, Snyder y Ord (2001) \cite{Koehler2001269} y Chatfield, Koehler, Ord y Snyder (2001) \cite{Chatfield2001147}. Desde hace tiempo se conoce que algunos modelos ARIMA dan predicciones equivalentes a los métodos lineales de alisado exponencial. La importancia del trabajo reciente en modelos de espacio de estado de innovaciones es que los métodos de alisado exponencial no lineales también pueden derivarse de modelos estadísticos.

\subsection{Selección del modelo}
Gardner y McKenzie (1988) \cite{GardnerJr1988863} proporcionaron algunas reglas sencillas basadas en las varianzas se series temporales diferenciadas para elegir un método de alisado exponencial apropiado. Tashman y Kruk (1996) \cite{Tashman1996235} compararon estos resultados con otros propuestos por Collopy y Armstrong (1992) \cite{Collopy19921394} y un enfoque basado en el criterio de información bayesiano (BIC). Hyndman et al. (2002) \cite{Hyndman2002439} también propuso un enfoque basado en criterio de información, pero usando los modelos de espacio de estado subyacentes.

\subsection{Robustez}
El buen rendimiento de la predicción de los métodos de alisado exponencial ha sido estudiado por varios autores. Satchell y Timmermann (1995) \cite{Satchell1995407} y Chatfield et al. (2001) \cite{Chatfield2001147} mostraron que SES es óptimo para un rango amplio de procesos generadores de datos. En un pequeño estudio de simulación, Hyndman (2001) \cite{Hyndman2001567} mostró que SES rendía mejor los modelos ARIMA de primer orden porque no está tan sujeto a problemas de selección de modelo, especialmente cuando los datos no siguen una distribución normal.

\subsection{Intervalos de predicción}
Una de las críticas a los métodos de alisado exponencial a principios de los 80 era que no había forma de producir intervalos de predicción para las predicciones. La primera aproximación analítica a este problema fue asumir que las series eran generadas mdeiante funciones deterministas del tiempo más ruido blanco (Brown, 1963 \cite{Brown1963}; Gardner, 1985 \cite{GardnerES19851}; McKenzie, 1986 \cite{Mckenzie1986373}; Sweet, 1985 \cite{Sweet1985235}). En tal caso, sería mejor usar un modelo de regresión en lugar de los métodos de alisado exponencial. Newbold y Bos (1989) \cite{Newbold1989523} criticaron fuertemente cualquier enfoque basado en esta hipótesis.
Otros autores buscaron intervalos de predicción mediante la equivalencia entre los métodos de alisado exponencial y los modelos estadísticos. Johnston y Harrison (1986) \cite{Johnston1986303} establecioeron variantes de predicción para los métodos SES, y los métodos de Holt de alisado exponencial para modelos de espacio de estado con múltiples fuentes de error. Yar y Chatfield (1990) \cite{Yar1990127} obtuvieron intervalos de predicción para el método aditivo de Holt-Winters derivando el equivalente método ARIMA subyacente. Chatfield y Yar (1991) \cite{Chatfield199131} discutieron intervalos aproximados de predicción para el método multiplicativo de Holt-Winters, asumiendo que los errores en las predicciones a un paso fueran independientes. Koehler et al. (2001) \cite{Koehler2001269} también derivó una fórmula aproximada para la previsión de varianza para el método multiplicativo de Holt-Winters, distinguiéndose de Chatfield y Yar (1991) \cite{Chatfield199131} solamente en cómo la desviación estándar del error en la predicción a un paso era estimada.
Ord et al (1997) \cite{Ord19971621} y Hyndman et al. (2002) \cite{Hyndman2002439}  usaron el modelo de espacio de innovación subyacente para simular rutas de muestras futuras, y de este modo obtuvieron intervalos de predicción para todos los métodos de alisado exponencial. Hyndman, Koehler, Ord y Snyder (2005) \cite{Hyndman200517} usaron modelos de espacio de estado para derivar intervalos de predicción analíticos para 15 de los 30 métodos, incluyendo todos los métodos usados habitualmente. Proporcionaron el enfoque algebraico más comprensible hasta la fecha para manejar el problema de la distribución de predicción para la mayoría de los métodos de alisado exponencial.

\subsection{Espacio de parámetros y propiedades del modelo}
Una práctica habitual es restringir los parámetros de alisado al rango de 0 a 1. Sin embargo, ahora que los modelos estadísticos subyacentes están disponibles, el espacio natural (invertible) de parámetros se puede usar en su lugar. Archibald (1990) \cite{Archibald1990199} mostró que es posible para parámetros de alisado dentro de los intervalos usuales producir modelos no invertibles. Consecuentemente, al predecir, el impacto del cambio en los valores pasados de la serie no es despreciable. Intuitivamente, tales parámetros producen predicciones pobres y deteriora el rendimiento de la predicción. Lawton (1998) \cite{Lawton1998393} también discutió este problema.

\section{Modelos ARIMA}
\subsection{Preámbulo}
Los primeros intentos de estudiar series temporales, particularmente en el siglo 19, estaban generalmente caracterizados por la idea de un mundo determinista. Fue la mayor contribución de Yule (1927) \cite{Yule1927267} la que impulsó la noción de aleatoriedad en series temporales postulando que cada serie temporal puede ser considerada como la realización de un proceso estocástico. Basado en esa sencilla idea, se desarrolló desde entonces un gran número de métodos de series temporales. Slutsky, Walker, Yaglom y Yule formularon el concepto de los modelos auto-regresivos (AR) y media móvil (MA). El teorema de la descomposición de Wold condujo a la formulación y solución del problema de predicción lineal de Kolmogorov (1941) \cite{Kolmogorov19411}. Desde entonces, una gran cantidad de literatura ha aparecido en el área de series temporales, tratando estimación de parámetros, identificación, comprobación de modelos y predicción, véase Newbold (1983) \cite{Newbold198323} para un estudio anterior.

La publicación \emph{Time Series Analysis: Forecasting and Control} de Box y Jenkins (1970) \cite{Box1976} integró el conocimiento existente. Además, estos autores desarrollaron una ciclo coherente, versátil e iterativo en tres etapas para la identificación, estimación y verificación de series temporales (conocido como el enfoque Box-Jenkins). El libro ha tenido un enorme impacto en la teoría y práctica moderna de análisis y predicción de series temporales. Con la llegada de los ordenadores, se popularizó el uso de los modelos ARIMA y se extendió a muchas áreas de la ciencia. De hecho, la predicción de series temporales discretas mediante modelos ARIMA univariantes, modelos de función de transferencia (regresión dinámica), y modelos ARIMA multivariados han generado bastantes artículos en IJF. A menudo estos estudios son de naturaleza empírica, y usan uno o varios métodos/modelos como punto de referencia para comparar. 

\begin{table}
\caption{Lista de ejemplos de aplicaciones reales}

\resizebox{12cm}{!}{
\begin{tabular}{|L{5cm}|C{3cm}|L{4cm}|c|}\hline %
\bfseries Dataset & \bfseries Horizonte & \bfseries Benchmark & \bfseries Referencia
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{ejemplos.csv}{}%
{\dataset & \horizon & \benchmark & \reference }% 

\end{tabular}
}
\end{table}
\subsection{Univariado}

El éxito de la metodología de Box-Jenkins se basa en el hecho de que los modelos pueden imitar el comportamiento de los diversos tipos de series y sin requerir muchos parámetros a estimar en la elección final del modelo. Sin embargo, a mediados de los años sesenta, la selección de un modelo era en gran medida una cuestión subjetiva del investigador, no había ningún algoritmo para especificar un modelo único. Desde entonces, muchas técnicas y métodos han surgido para añadir rigor matemático en el proceso de búsqueda de un modelo ARMA, incluyendo el criterio de información Akaike (AIC), el error de predicción final de Akaike (FPE) y el criterio de información bayesiano ( BIC). A menudo, estos criterios se reducen a la minimización de errores de predicción a un paso (en la muestra) , con un término de penalización para el sobreajuste. FPE también ha sido generalizado para predicción a varios pasos (véase, por ejemplo , Bhansali , 1996 \cite{Bhansali1996577} y Bhansali , 1999 \cite{Bhansali1999295}) , pero esta generalización no ha sido utilizada en el área aplicada. Este también parece ser el caso de criterios basados en principios de validación cruzada y validación por división muestral (véase, por ejemplo , West , 1996 \cite{West19961084}), haciendo uso de errores de predicción fuera-de-muestra; véase Peña y Sánchez (2005) \cite{Pena2005135} para un enfoque relacionado a considerar.

Existen diferentes métodos para estimar los parámetros de un modelo ARMA (véase Box et al.,1994 \cite{Box1994}). Aunque estos métodos son asintóticamente equivalentes, en el sentido de que las estimaciones tienden a la misma distribución normal, hay grandes diferencias en las propiedades con muestra finitas. En un estudio comparativo de los paquetes de software, Newbold , Agiakloglou y Miller (1994)  \cite{Newbold1994573}mostraron que esta diferencia puede ser muy sustancial y, como consecuencia, puede influir en las predicciones. En consecuencia, recomendaron el uso de máxima verosimilitud completa. El efecto de que los errores de estimación para los parámetros limitan las predicciones también fue algo observado por Zellner (1971) \cite{Zellner1971}. Él utilizó un análisis bayesiano y derivó la distribución predicha de observaciones futuras tratando los parámetros del modelo ARIMA como variables aleatorias. Más recientemente,  Kim (2003) \cite{Kim2003493} consideró una predicción y estimación de los parámetros de modelos AR con pequeñas muestras. Encontró que las estimaciones de parámetros con sesgo corregido producían predicciones más precisas las de mínimos cuadrados. Landsman y Damodaran (1989) \cite{Landsman1989491} presentaron la evidencia de que la estimación de parámetros de ARIMA James-Stein mejoraba la precisión de predicción con respecto a otros métodos, bajo el criterio de MSE.

Si una serie temporal se sabe que sigue un modelo ARIMA univariado, las predicciones que usan observaciones desagregadas son, en términos de MSE, al menos tan buenas como aquellas que usan observaciones agregadas. Sin embargo, en aplicaciones prácticas, existen otros factores a considerar, como valores perdidos en series desagregadas. Ledolter (1989) \cite{Ledolter1989231} y Hotta (1993) \cite{Hotta1993261} analizaron el efecto producido de un outlier aditivo en el intervalo de predicción cuando se estiman los parámetros del modelo ARIMA. Cuando el modelo es estacionario, Hotta y Cardoso Neto (1993) mostraron que la pérdida de eficiencia usando datos desagregados no era alta, incluso si el modelo no es conocido. Por tanto, la predicción podría hacerse tanto con modelos desagregados como agregados.

El problema de añadir información externa (prior) en las predicciones de ARIMA univariado fue considerado por Cholette (1982) \cite{Cholette1982375}, Guerrero (1991) \cite{Guerrero1991339} y de Alba (1993) \cite{deAlba199395}).

Como alternativa a la metodología de ARIMA univariado, Parzen (1982) \cite{Parzen198267} propuso la metodología ARARMA. La idea clave es que una serie temporal es transformada de un filtro AR de memoria larga, a un filtro de memoria corta, evitando así un operador de diferenciación más duro. Además, se usa una fase de identificación diferente al convencional de Box-Jenkins. En la competición M (Makridakis et al., 1982 \cite{Makridakis1982111}), los modelos ARARMA consiguieron el MAPE más bajo para horizontes de predicción mayores. Por lo tanto, es sorprendente encontrar que, a parte de artículo de Meade y Smith (1985) \cite{Meade1985519}, la metodología ARARMA todavía no haya despegado en el área aplicada. Su valor definitivo, quizá pueda ser juzgado mediante el estudio  de Meade (2000), quien comparó la capacidad de predicción de un método ARARMA automático y no automático.

El modelado automático de ARIMA univariado, ha mostrado una capacidad de predicción a un paso tan precisa como la de otros métodos competentes (Hill y Fildes, 1984, Liber, 1984, Poulos et al., 1987 y Texter y Ord, 1989). Varios proveedores de software han implementado métodos de predicción automáticos de series temporales (incluso métodos multivariados); véase Geriner y Ord (1991) \cite{Geriner1991127}, Tashman y Leach (1991) \cite{Tashman1991209}, y Tashman (2000) \cite{Tashman2000437}. A menudo estos métodos actúan como caja negra. La tecnología de sistemas expertos (Mélard \& Pasteels, 2000) se puede usar para evitar este problema. Algunas directrices en la elección de un método de predicción automático se da en Chatfield (1988) \cite{Chatfield198819}.

Mejor que adoptar un modelo AR para todos los horizontes de predicción, Kang (2003) \cite{Kang2003387} investigó empíricamente el caso de seleccionar un modelo AR  de forma diferenciada para cada horizonte en predicciones multi-paso.
El rendimiento de predicción del procedimiento multi-paso parece depender entre otras cosas del criterio de selección de orden óptimo, los periodos de predicción, los horizontes de predicción y la serie a predecir.


\subsection{Función de transferencia}
La identificación de modelos de función de transferencia puede ser difícil cuando hay más de una variable de entrada. Edlund (1984) \cite{Edlund1984297} presentó un método en dos pasos para identificación de una función respuesta impulso cuando varias variables de entrada están correladas. Koreisha (1983) \cite{Koreisha1983151} estableció varias relaciones entre funciones de transferencia, implicaciones causales y especificación de modelo econométrico. Gupta (1987) \cite{Gupta1987195} identificó los principales problemas en las comprobaciones de causalidad. Usando análisis de componentes principales, del Moral y Valderrama (1997) \cite{DelMoral1997237} sugirieron una representación parsimoniosa de  modelo de función de transferencia . Krishnamur, Narayan y Raj (1989) \cite{Krishnamurthi198921} mostraron cómo estimaciones más precisas del impacto de las intervenciones en modelos de función de transferencia se pueden obtener usando una variable de control.

\subsection{Multivariado}
El modelo de vector ARIMA (VARIMA) es una generalización multivariada del modelo univariado ARIMA. Las características de población de procesos VARMA parecen haber sido derivados primero por Quenouille (1957) \cite{Quenouille1957}, aunque el software para su implementación llegó a estar disponible en los 80 y 90. Puesto que los modelos VARIMA pueden acomodar supuestos de exogeneidad y de relaciones de contemporaneidad, ofrecen nuevos retos a los encargados de predecir y elaborar de políticas. Riise y Tjostheim (1984) \cite{Riise1984309} mostraron como los filtros de suavizado pueden construirse dentro de los modelos VARMA. El alisado previene fluctuaciones irregulares en series temporales explicativas al migrar las predicciones de la serie dependiente. Para determinar el máximo horizonte de predicción para procesos VARMA, DeGooijer y Klein (1991) \cite{DeGooijer1992135} establecieron las propiedades teóricas de las predicciones y el error de predicción acumulado a varios pasos. Lütkepohl (1986) \cite{Lutkepohl1986461} estudió los efectos de la agregación temporal y el muestreo sistemático en predicción, suponiendo que la variable desagregada (estacionaria) sigue un proceso VARMA de orden desconocido. Posteriormente, Bidarkota (1998) \cite{Bidarkota1998457} consideró el mismo problema pero con las variables observadas integradas en lugar de estacionarias.

Los vectores de auto-regresión (VAR) constituyen un caso especial de una clase más general de modelos VARMA. En esencia, un modelo VAR es una aproximación flexible a la forma reducida de una variedad amplia de modelos econométricos dinámicos. Los modelos VAR se pueden especificar de varias formas. Funke (1990) \cite{Funke1990363} presentó 5 especificaciones diferentes de VAR y comparó sus rendimientos de predicción usando series mensuales de producción industrial. Dhrymes y Thomakos (1989) \cite{Dhrymes198881} disertó cuestiones relacionadas con la identificación de VARs estructurales. Hafer y Sheedhan (1989) \cite{Hafer1989399} mostraron el efecto de cambios en la estructura del modelo en las predicciones de VAR. Ariño y Franses (2000) \cite{Arino2000111} dieron expresiones explícitas para predicciones VAR por niveles; véase también Wieringa y Horváth (2005) \cite{Wieringa2005279}. Hansson, Jansson y Löf (2005) \cite{Hansson2005377} usaron un modelo de factor dinámico como punto de partida para obtener predicciones a partir de VARs parametrizados parsimoniosamente.

En general, los modelos VAR tienden a sufrir sobreajuste con demasiados parámetros libres y no significativos. Como resultado, estos modelos pueden dar pobres predicciones fuera de muestra, incluso aunque el ajuste en las muestras sea bueno; véase Liu, Gerlow y Irwin (1994) \cite{Liu1994419} y Simkins (1995) \cite{Simkins1995569}. En lugar de restringir algunos parámetros como habitualmente se hace, Litterman (1986) \cite{Litterman198625} y otros impusieron una distribución preferente en los parámetros, expresando la idea de que muchas variables económicas se comportan como un camino aleatorio. Los modelos BVAR se han usado principalmente para predicción macroeconómica (Artis y Zhang, 1990 \cite{Artis1990349}, Ashley, 1988 \cite{Ashley1988363}, Holden \& Broomhead, 1990 \cite{Holden199011} y Kunst \& Neusser, 1986 \cite{Kunst1986447}), para predicción de cuotas de mercado (Ribeiro Ramos, 2003 \cite{RibeiroRamos200395}), predicción del mercado laboral (LeSage y Magura, 1991 \cite{LeSage1991231}), para proyección de negocios (Spencer, 1993 \cite{Spencer1993407}) o para predicción de economía local (LeSage, 1989  \cite{Lesage198937}). Kling y Bessler (1985) \cite{Kling19855} compararon predicciones fuera de muestra de varios métodos para series temporales multivariadas bien conocidos de la época, incluyendo el modelo BVAR de Litterman.

El concepto de cointegración de Engle y Granger (1987) \cite{Engle1987251} ha generado varias cuestiones interesantes en la capacidad de predicción de los modelos de corrección de error (ECMs) frente a VARs y BVARs no restringidos. Shoesmith (1992 \cite{Shoesmith1992187}, 1995 \cite{Shoesmith1995557}), Tegene y Kuchler (1994) \cite{Tegene199465} y Wang y Bessler (2004) \cite{Wang2004683} dieron evidencias empíricas para sugerir que los ECMs mejoran el rendimiento de VARs por niveles, particularmente en horizontes de predicción mayores. Shoesmith (1995) \cite{Shoesmith1995557}, y posteriormente Villani (2001) \cite{Villani2001585}, también mostraron cómo el enfoque bayesiano de Litterman (1986) \cite{Litterman198625} podía mejorar la predicción con VARs cointegrados. Reimers (1997) \cite{Reimers1997369} estudió el rendimiento de predicción de procesos de series temporales vector cointegradas estacionalmente usando un ECM. Poskitt (2003) \cite{Poskitt2003503} disertó sobre la especificación de sistemas VARMA cointegrados. Chevillon y Hendry (2005) \cite{Chevillon2005201} analizó la relación entre estimaciones multi-paso directas de VARs estacionarios frente a no estacionarios y respectivas precisiones de predicción. 

\section{Estacionalidad}
El enfoque más antiguo para controlar la estacionalidad en series temporales es extraerla usando un procedimiento de descomposición como el método X-11. En los últimos 25 años este método y sus variantes (incluyendo versiones más reciente, X-12-ARIMA, Findley, Monsell, Bell, Otto y Chen, 1998 \cite{Findley1998127}, se han estudiado de forma extensa.

Una línea de investigación considera el efecto de usar predicción como parte del método de descomposición estacional. Por ejemplo Dagum (1982) \cite{Dagum1982173} y Huot, Chiu y Higginson (1986) \cite{Huot1986217} observaron el uso de predicción en X-11-ARIMA para reducir el tamaño de las revisiones en el ajuste estacional de los datos, y Pfeffermann, Morry y Wong (1995) \cite{Pfeffermann1995271} exploraron el efecto de las predicciones en la  varianza de los valores ajustados de tendencia y estacionalidad.

Quenneville, Ladiray y Lefrançois (2003) \cite{Quenneville2003727} con otro enfoque observaron las predicciones implícitas mediante los filtros de medias móviles asimétricos del método x-11 y sus variantes.

Un tercer enfoque fue ver la efectividad de predicción usando datos ajustados estacionalmente a partir de un método de descomposición estacional. Milller y Williams, 2003 \cite{Miller2003669} y 2004 \cite{Miller2004529} mostraron que se obtenía la mayor precisión de predicción cuando se encogía la componente estacional hacia cero.  Los comentarios sobre este último artículo dieron sugerencias relacionadas con la implementación de esta idea.

Además de trabajar en el método X-11 y sus variantes, también se han desarrollado varios métodos nuevos para el ajuste estacional, entre los que destaca el enfoque basado en modelo de TRAMO-SEATS (Gómez y Maravall, 2001 \cite{Gomez2001} y Kaiser y Maravall 2005 \cite{Kaiser2005691}) y el método no paramétrico STL (Cleveland, Cleveland, McRae y Terpenning, 1990 \cite{Cleveland19903}). Otra propuesta ha sido el uso de modelos sinusoidales (Simmons, 1990 \cite{Simmons1990485}).

En la predicción de varias series similares, Withycombe (1989) \cite{Withycombe1989547} mostró que puede ser más eficiente estimar una componente estacional combinada para el grupo de series, en lugar de usar sus patrones individuales. Bunn y Vassilopoulos (1993) \cite{Bunn1993517} demostró cómo usar agrupamiento para formar grupos apropiados para esta situación, y Bunn y Vassilopoulus (1999)\cite{Bunn1999431} introdujo algunos estimadores mejorados para los índices de grupo estacional.

A principio de los 80, las pruebas de raíz unitarias acababan de ser inventadas y las pruebas de raíz unitaria estacional estaban por llegar en breve. Posteriormente hubo un trabajo considerable en el uso e implementación de las pruebas de raíz unitaria estacional incluyendo Hylleberg y Pagan (1997) \cite{Hylleberg1997329}, Taylor (1997)\cite{Taylor1997307} y Franses y Koehler (1998) \cite{Franses1998405}. Paap, Franses y Hoek (1997) \cite{Paap1997357} y Clements y Hendry (1997) \cite{Clements1997341} estudió el rendimiento de predicción de modelos con pruebas de raíz especialmente en el contexto de los cambios de nivel.

Algunos autores han advertido contra el uso extendido de modelos de raíz unitaria estacional estándar para series temporales económicas. Osborn (1990) \cite{Osborn1990327} argumentó que las componentes estacionales deterministas son más habituales en series económicas que en estacionalidad estadística. Franses y Romijn (1993) \cite{HansFranses1993467} sugirió que las raíces estacionales en modelos periódicos resultan en mejores predicciones. Los modelos de series temporales periódicas fueron también exploradas por Wells (1997) \cite{Wells1997407}, Herwartz (1997) \cite{Herwartz1997421} y Novales y de Fruto (1997) \cite{Novales1997393}, los cuales encontraron que los modelos periódicos pueden conducir a una mejora en el rendimiento de la predicción comparados con modelos no periódicos bajo algunas condiciones. La predicción de procesos ARMA periódicos multivariados fue considerada por Ullah (1993) \cite{Ula1993645}.

Varios artículos han comparado empíricamente los modelos estacionales. Chen (1997) \cite{Chen1997269} exploró la robustez de un modelo estructural, un modelo de regresión con estacionalidad, un modelo ARIMA, y el método de Holt-Winters, y encontró que los dos últimos daban predicciones relativamente robustas cuando el modelo no estaba completamente especificado. Noakes, McLeod, y Hipel (1985) \cite{Noakes1985179}, Albertson y Aylen (1996) \cite{Albertson1996345}, Kulendran y King (1997) \cite{Kulendran1997319} y Franses y van Dijk (2005) \cite{Franses200587} compararon todos el rendimiento de predicción de varios modelos estacionales aplicados a datos reales. El modelo con mejor predicción variaba a lo largo del estudio, dependiendo de qué modelo fuera usado y la naturaleza de los datos. Parece no haber consenso todavía en qué condiciones se puede preferir un modelo u otro.

\section{Espacio de estado y modelos estructurales y el filtro de Kalman}

A principios de  1980, los modelos de espacio de estado acababan de empezar a usarse por estadísticos para predecir series temporales, aunque las ideas ya habían estado presentes en la literatura de ingeniería desde el trabajo innovador de Kalman (1960) \cite{Kalman196035}. Los modelos de espacio de estado proporcionan un marco unificado en el que se pueda escribir cualquier modelo lineal de serie temporal. La contribución clave de predicción de Kalman (1960) fue dar un  algoritmo recursivo (conocido como el filtro de Kalman) para computar las predicciones. Los estadísticos empezaron interesarse por los modelos de espacio de estado cuando Schweppe (1965) \cite{Schweppe196561} mostró que el filtro de Kalman proporciona un método eficiente para computar los errores de la predicción a un paso y su varianza asociada necesaria para producir la función de probabilidad. Shumway y Stoffer (1982)\cite{Shumway1982253} combinaron el algoritmo EM con el filtro de Kalman para dar un enfoque general para predecir series temporales usando modelos de espacio de estado, incluyendo la posibilidad de observaciones inexistentes.


Una clase particular de modelos de espacio de estado, conocidos como \emph{modelos lineales dinámicos} (DLM) fue introducida por Harrison y Stevens (1976) \cite{Harrison1976205}, quienes también propusieron un enfoque bayesiano para la estimación. Fildes (1983) \cite{Fildes1983137} comparó las predicciones obtenidas usando el método de Harrison y Stevens con métodos más sencillos como el alisado exponencial concluyendo que la complejidad adicional no conducía a una mejora del rendimiento de predicción. El enfoque de modelado y estimación de Harrison y Stevens fue mejorado por (West, Harrison y Migon (1985) \cite{West198573} y West y Harrison (1989) \cite{West1997}. Harvey, 1984 \cite{Harvey1984245} y 1989 \cite{Harvey1989} amplió esta clase de modelos y siguió un enfoque no bayesiano para la estimación. También renombró los modelos como \emph{modelos estructurales}, aunque posteriormente usaría el término \emph{modelos de componentes no observadas}. Harvey (2006) \cite{Harvey2006327} proporciona una revisón comprensible y una introducción para esta clase de modelos incluyendo variaciones no gausianas y en tiempo continuo. 

Estos modelos poseen muchas similaridades con los métodos de alisado exponencial, pero tienen múltiples fuentes de error aleatorio. En particular, el \emph{modelo estructural básico} (BSM) es similar a método de Holt-Winters para datos estacionales e incluye componentes de nivel, tendencia y estacionalidad.

Ray (1989) \cite{Ray1989537} discutió la tasa de convergencia para el crecimiento lineal del modelo estructural y mostró que los estados iniciales (elegidos de forma subjetiva normalmente) tenían un impacto no despreciable en las predicciones. Harvey y Snyder (1990) \cite{Harvey1990187} propusieron algunos modelos estructurales de tiempo continuo para predecir el tiempo de espera demandado para control de inventario. Proietti (2000)\cite{Proietti2000247} discutió variaciones del BSM, comparando sus propiedades y evaluando las predicciones resultantes.

Los modelos estructurales no gausianos, han sido objeto de numerosos artículos, empezando por el modelo de potencia estable de Smith (1979) \cite{Smith1979375} con posterior desarrollo por West et al. (1985) \cite{West198573}. Por ejemplo, estos modelos fueron aplicados a la predicción de series temporales de dimensiones continuas por Grundwald, Raftery y guttorp (1993) \cite{Grunwald1993103} y para recuentos por Harvey y Fernandes (1989)\cite{Harvey1989407}. Sin embargo, Grunwald, Hamza y Hyndman (1997)\cite{Grunwald1997615} mostraron que la mayoría de los modelos habitualmente usados tienen una sustancial imperfección en la trayectoria de muestras convergiendo a una constante cuando el espacio de muestras es menor que la línea real completa, haciéndolas inadecuadas para predicción de intervalos.

Otra clase de modelos de espacio de estado, conocida como \emph{modelos balanceados de espacio de estado}, se ha usado principalmente para predicción de series temporales macro-económicas. Mittnik (1990)\cite{Mittnik1990337} proporcionó un estudio de esta clase de modelos, y Vinod y Basu (1995) \cite{Vinod1995217} obtuvo predicciones para el consumo, ingresos y tasas de interés usando estos modelos. Estos sólo tienen una única fuente de error aleatorio y generalizan otros modelos como ARMAX, ARMA y modelos con retardo racional distribuido. Una clase de espacio de estados relacionada son los modelos de \emph{fuente única de error} en los que subyacen los métodos de alisado exponencial.

A parte de estos desarrollos metodológicos, han habido varios artículos proponiendo modelos innovadores de espacio de estados para resolver problemas prácticos de predicción. Entre ellos, Coomes (1992) \cite{Coomes1992473} quien usó un modelo para predecir empleos según la industria para regiones locales y Patterson (1995) \cite{Patterson1995395} quien usó un enfoque de espacio de estado para predicción de ingresos personales disponibles. 

Los libros de Harvey (1989) \cite{Harvey1989}, West y Harrison (1989) \cite{West1997}y Durbin y Koopman (2001) \cite{Durbin2001} han tenido un impacto sustancial en la literatura de series temporales en cuanto a investigación de modelos de espacio de estado, filtrado de Kalman y modelos estructurales discretos / continuos en tiempo. 


\section{Modelos no lineales}

\subsection{Preámbulo}
Comparado con el estudio de series temporales lineales, el desarrollo de análisis y predicción no lineal de series temporales todavía está en su infancia. El comienzo se atribuye a Volterra (1930) \cite{Volterra1959}. Mostró que cualquier función continua no lineal en \emph{t} podría ser aproximada por una serie finita de Volterra. Wiener (1958) \cite{Wiener1958} se interesó en las ideas de representación funcional de series y desarrolló el material existente. Aunque las propiedades probabilistas de estos modelos se habían estudiado ampliamente, los problemas de estimación de parámetros, ajuste del modelo y predicción fueron descuidados durante un largo tiempo. Esto puede atribuirse a la complejidad del modelo propuesto por Wiener y sus formas simplificadas como el modelo bilineal (Poskitt y Tremayne, 1986 \cite{Poskitt1986101}). En su día, el ajuste de estos modelos conllevaba dificultades de cómputo insuperables.

Aunque la linealidad es una suposición útil y una herramienta potente en muchas áreas, a finales de los 70 y principios de los 80, los modelos lineales eran insuficientes en muchas aplicaciones reales. Por ejemplo, los ciclos prolongados de tamaño de población animal, (los famosos datos del lince Canadiense), ciclos solares (número anual de manchas solares), flujo de energía y relaciones de amplitud-frecuencia, son ejemplos donde no se adecuan los modelos lineales. Para satisfacer esta demanda, se propusieron modelos no lineales de series temporales útiles durante este mismo periodo. De Gooijer y Kumar (1992)  \cite{DeGooijer1992135} ofrecieron un resumen de los desarrollos en el área al principio de los 90. Estos autores argumentaban que el rendimiento superior en predicción de estos modelos no lineales era irregular.

Un factor que probablemente ha retrasado difusión de publicaciones de predicciones no lineales es que hasta entonces no era posible obtener expresiones analíticas cerradas para predicciones multi-paso. Sin embargo, usando la ley de  Chapman-Kolmogorov en principio se pueden obtener mediante complejas integraciones numéricas predicciones de mínimos cuadrados exactas multi-paso. Pemberton (1987)  \cite{Pemberton1987443} y AL-Qassem y Lane (1989)  \cite{Al-Qassam198995}dieron algunos de los primeros ejemplos de este enfoque. Hoy día, las predicciones no lineales se obtienen mediante simulación Monte Carlo o por \emph{bootstrapping} (remuestreo). El último caso se prefiere al no necesitar suposiciones sobre la distribución del proceso de error.

La monografía de Granger y Teräsvirta (1993)  \cite{Granger1993} ha impulsado nuevos desarrollos en la estimación, evaluación y selección entre los diferentes modelos de predicción no lineales para series temporales de economía y finanzas. Un buen resumen del estado del arte en se encuentra en IJF Special Issue 20:2 (2004). En su artículo de introducción, Clements, Franses y Swanson (2004)  \cite{Clements2004169} subrayan varios temas para investigación futura. Concluyen que todavía queda mucho por hacer para que los procedimientos no lineales para predicción, estimación y especificación de modelos estén fácilmente disponibles.

\subsection{Modelos de cambio de régimen}
La clase de modelos no lineales autorregresivos con umbral auto-excitado (SETAR) fue promocionada de forma prominente mediante los libros de Tong, 1983  \cite{Tong1983} y 1990  \cite{Tong1990}. Estos modelos, los cuales son modelos lineales a trozos, en su forma más básica, han atraído algo de atención en IJF. Clements y Smith (1997)  \cite{Clements1997463} compararon varios métodos para obtener predicciones mult-paso para modelos SETAR univariados discretos. Concluyeron que las predicciones mediante simulación Monte Carlo eran satisfactorias en casos donde se sabe que las perturbaciones en el modelo SETAR vienen de una distribución simétrica. En otro caso se prefiere el método de \emph{bootstrapping}. De Gooijer y Vidiella-i-Anguera (2004) \cite{DeGooijer2004237} dieron resultados similares para modelos con umbral VAR. Brockwell y Hyndman (1992) \cite{Brockwell1992157} obtuvieron predicciones de un paso para modelos con umbral continuos (CTAR). Puesto que el cálculo de predicciones multi-paso a partir de los modelos CTAR involucra integraciones complicadas de mayor dimensionalidad, el uso práctico de CTAR está limitado. El rendimiento de predicción fuera de muestra de varias variantes de modelos SETAR con respecto a los modelos lineales ha sido objeto de varios artículos en IJF, incluyendo Astatkiw, Watts y Watt (1997) \cite{Astatkie1997105}, Boero y Marrocu (2004) \cite{Boero2004305}y Enders y Falk (1998) \cite{Enders1998171}.

Una desventaja del modelo SETAR es que la dinámica cambia de forma discontinua de un régimen a otro. En cambio, un modelo de transición suave (STAR) permite una transición más gradual entre los distintos regímenes. Sarantis (2001)\cite{Sarantis2001459} encontró evidencia de que los modelos tipo STAR pueden ser mejores que los modelos lineales autorregresivos y de camino aleatorio para predecir precios de acciones tanto a corto plazo como a medio plazo. El estudio de Bradley y Jansen (2004)\cite{Bradley2004321} parece refutar la conclusión de Sarantis.

Fok, van Dijk y Franses (2005) \cite{Fok2005785} examinaron la siguiente cuestión clave: ¿Puede un modelo STAR multinivel de datos de panel para series desagregadas mejorar las predicciones de agregados macroeconómicos como ingresos totales o desempleo total?. El modelo STAR propuesto parece merecer la pena investigarse en más detalle puesto que permite que los parámetros que gobiernan el cambio de régimen sean distintos según los estados. Basándose en simulaciones y hallazgos empíricos, los autores afirman que de hecho se pueden conseguir mejoras en las predicciones a un paso.

Franses, Paap, y Vroomen (2004) \cite{Franses2004255} propusieron un modelo con umbral AR(1) que permite una inferencia plausible sobre valores específicos de los parámetros. La idea clave es que los valores de los parámetros de AR dependen de una variable indicadora dominante. El modelo resultante mejora el rendimiento respecto a otros modelos no lineales cambiantes en tiempo, incluyendo el modelo de cambio de régimen de Markov, en términos de predicción.

\subsection{Modelos con coeficiente funcional}
Un modelo AR con coeficiente funcional (FCAR o FAR) es un modelo AR en el que los coeficientes AR pueden variar como una función suave de otra variable, como un valor retrasado de la propia serie o una variable exógena. El modelo FCAR incluye los modelos TAR y STAR como casos especiales y es análogo al modelo aditivo generalizado de Hastie y Tibshirani (1991) \cite{Hastie1990}. Chen y Tsay (1993) \cite{Chen1993298} propusieron un procedimiento de modelado usando ideas de estadística paramétrica y no paramétrica. El enfoque supone poca información a priori sobre la estructura del modelo sin sufrir la \emph{maldición de la dimensión}; véase también Cai, Fan y Yao (2000) \cite{Cai2000941}. Harvill y Ray (2005) \cite{Harvill2005717} presentaron resultados de predicción usando modelos univariados y multivariados con coeficiente funcional (V)FCAR. Estos autores limitaron su comparación a tres métodos, el complemento predictivo ingenuo, el predictor bootstrap y el predictor multi-etapa. Tanto los resultados empíricos como las simulaciones indicaban que el método bootstrap daba predicciones ligeramente más precisas. Un área potencialmente útil de investigación futura es saber si la potencia de los modelos VFCAR puede mejorarse usando variables exógenas.

\subsection{Redes neuronales}
Una red neuronal artificial (ANN) puede ser útil para procesos no lineales que tengan relaciones funcionales desconocidas y por tanto difíciles de ajustar (Darbellay y Slama, 2000 \cite{Darbellay200071}). La idea principal con ANN es que las entradas, o variables dependientes, son filtradas mediante una o más capas ocultas cada una de las cuales consta de nodos antes de alcanzar la variable de salida. La salida intermedia se enlaza a la salida final. Otros modelos son versiones específicas de ANN donde se impone cierta estructura (véase JoF Special Issue 17:5/6 (1998).

Una de las mayores áreas de aplicación de ANN es predicción; véase Zhang, Patuwo y Hu (1998) \cite{Zhang199835} y Hippert, Pedreira y Souza (2001) \cite{Hippert200144} para revisiones de la literatura. Existen numerosos estudios que documentan el éxito de ANN en predicción de datos financieros. Sin embargo, en dos publicaciones de IJF, Chatfield, 1993 \cite{Chatfield19931} y 1995 \cite{Chatfield1995501} se cuestionó si las ANNs se habían vendido demasiado como técnicas de predicción milagrosas. Seguidamente algunos artículos documentaron que modelos ingenuos como el de camino aleatorio podían mejorar el rendimiento de ANNs (véase Callen et al., 1996 \cite{Callen1996475}, Church y Curram, 1996 \cite{Church1996255}, Conejo et al., 2005 \cite{Conejo2005435}, Gorr et al., 1994 \cite{Gorr199417} y Tkacz, 2001 \cite{Tkacz200157}). Estas observaciones son consistentes con los resultados de Adya y Collopy (1998) \cite{Adya1998481} donde se evalúa la efectividad de predicción basada en ANN para 48 estudios realizados entre 1988 y 1994.

Gorr (1994) \cite{Gorr19941} y Hill, Marquez, OConnor y Remus (1994) \cite{Hill19945} sugirieron que la investigación a seguir debería tratar de definir mejor los límites en que ANN mejora las técnicas tradicionales y viceversa. Varios autores exploran este tema. Hill et al. (1994) \cite{Hill19945} advirtió que las ANNs probablemente funcionen mejor para datos financieros con alta frecuencia y Balkin and Ord (2000) \cite{Balkin2000509} también subrayaron la importancia del tamaño de la serie para asegurar resultados óptimos del entrenamiento de la ANN. Qi (2001) \cite{Qi2001383} señaló que las ANNs probablemente mejoren otros métodos cuando los datos de entrada se mantienen lo más actualizados posible usando modelado recursivo (Olson y Mossman, 2003 \cite{Olson2003453}).

Un problema general con los modelos no lineales se encuentra en la complejidad de los modelos y su parametrización excesiva. Si se considera realmente importante el principio de parsimonia, es interesante comparar el rendimiento de predicción fuera de muestra de los modelos lineales frente a los no lineales, usando una amplia variedad de criterios de selección de modelo. Esta cuestión fue considerada en bastante profundidad por Swanson y White (1997) \cite{Swanson1997439}. Sus resultados sugirieron que una simple ANN con una única capa oculta \emph{feed-forward}, la cual era muy popular en series temporales econométricas, ofrece una alternativa útil y flexible a modelos lineales de especificación fijada, particularmente para horizontes de predicción mayores de un paso. En contraste con Swanson y White, Heravi, Osborn y Birchenhall (2004) \cite{Heravi2004435} encontraron que los modelos lineales producen predicciones más precisas de producción industrial mensual europea sin ajuste estacional que los modelos de ANN. Ghiassi, Saidane, y Zimbra (2005) \cite{Ghiassi2005341} presentaron una ANN dinámica y compararon su rendimiento de predicción frente a la ANN tradicional y los modelos ARIMA.

Con el tiempo, la importancia del riesgo de la parametrización excesiva y el sobreajuste ha sido reconocida por varios autores; véase Hippert, Bunn y Souza (2005) \cite{Hippert2005425} que usaron una gran ANN (50 entradas, 15 neuronas ocultas y 24 salidas) para predecir perfiles de carga de electricidad diaria. Sin embargo, la cuestión de si la ANN está sobre-parametrizada o no, sigue sin resolverse. Algunas ideas con valor potencial para construir ANNs minimizando el número de parámetros, usando inferencia estadística se sugieren en Teräsvirta, van Dijk y Medeiros (2005) \cite{Terasvirta2005755}.

\subsection{Dinámicas estadísticas frente a deterministas}
La posibilidad de que las no linealidades de alta frecuencia en datos financieros (como rendimientos por hora) sean producidos por procesos caóticos deterministas de baja dimension, ha sido objeto de algunos estudios en IJF. Cecen y Erkal (1996) \cite{Cecen1996465} mostraron que no es posible explotar la dependencia determinista no lineal en los tipos de cambio diario para mejorar la predicción a corto plazo. Lisi y Medio (1997) \cite{Lisi1997255} reconstruyeron el espacio de estado para los tipos de cambio mensual y usando un método lineal local, aproximaron las dinámicas del sistema en ese espacio. La predicción fuera de muestra a un paso mostró que su método mejoraba un modelo de camino aleatorio. Un estudio similar fuer realizado por Cao y Soofi (1999) \cite{Cao1999421}.

\subsection{Otros}
Otros modelos no lineales ni tan conocidos se han usado para predicción. Por ejemplo, Ludlow y Enders (2000) \cite{Ludlow2000333} adoptaron coeficientes de Fourier para aproximar diferentes no linealidades presentes en series temporales. Herwartz (2001) \cite{Herwartz2001231} amplió el vector lineal ECM para permitir asimetrías. Dahl y Hylleberg (2004) \cite{Dahl2004201} compararon el modelo de regresión no lineal flexible de Hamilton (2001) \cite{Hamilton2001537}, ANNs y dos versiones del modelo de regresión de búsqueda de proyección. En un estudio comparativo de Marcellino (2004) \cite{Marcellino2004359} se incluyen modelos AR variantes en tiempo. El método no paramétrico de vecinos más cercanos fue aplicado por  Fernández-Rodríguez, Sosvilla-Rivero y Andrada-Félix (1999) \cite{Fernandez-Rodriguez1999383}

\section{Modelos de memoria larga}
Cuando el parámetro de integración d de un modelo ARIMA es fraccional y mayor que cero, el proceso exhibe memoria en el sentido de que las observaciones de un periodo de tiempo largo, tiene una dependencia no despreciable. Los modelos estacionarios com memoria larga (0 < d < 0.5), también llamados ARMA diferenciados fraccionalmente (FARMA) o modelos ARMA integrados fraccionalmente (ARFIMA), han sido estudiados en muchos campos; véase Granger y Joyeux (1980) \cite{Granger198015} para una introducción. Una motivación para estos estudios es que muchas series temporales empíricas tienen una función de autocorrelación muestral que decae a una tasa menor que en un modelo ARIMA de orden finito y entero d.

El potencial de predicción de modelos ajustados a FARMA/ARFIMA, en contraposición con resultados de predicción obtenidos con otros modelos de series temporales, ha sido tema de varios artículos en IJF y un special issue (2002, 18:2). Ray, 1993a \cite{Ray1993255} y Ray, 1993b \cite{Ray1993511} realizaron un estudio comparativo entre los modelos FARMA/ARFIMA estacionales y los modelos estándar (no fraccionarios) ARIMA estacionales. Los resultados muestran que los modelos AR de mayor orden son capaces de predecir bien a largo plazo comparados con los modelos ARFIMA. Como continuación, Smith y Yadav (1994) \cite{Smith1994507} investigaron el coste de asumir una diferencia unitaria cuando una serie es sólamente fraccinoariamente integrada con d distinto de 1. La sobre-diferenciación de una serie producirá una pérdida en el rendimiento de predicción de un paso, con un pérdida limitada en adelante. En cambio, una serie bajo-diferenciada, es más costosa con mayores potenciales de pérdida a partir del ajuste de un modelo AR poco especificado para cualquier horizonte de predicción. La cuestión es también explorada por Andersson (2000) \cite{Andersson2000121} quien mostró que la mala especificación afecta fuertemente la memoria estimada del modelo ARFIMA, usando una regla que es similar al test de Öller (1985) \cite{Oller1985135}. Man (2003) \cite{Man2003477} argumentó que un modelo ARMA(2,2) adecuadamente adaptado, podía producir predicciones a corto plazo competitivas con los modelos ARFIMA estimados. Las predicciones multi-paso de los modelos de memoria larga fueron desarrollados por Hurvich (2002) \cite{Hurvich2002167} y comparados por Bhansali y Kokoszka (2002) \cite{Bhansali2002181}.

Se han explorado muchas extensiones de modelos ARFIMA y comparaciones de su rendimiento de predicción relativo. Por ejemplo, Franses y Ooms (1997) \cite{Franses1997117} propuso el modelo ARFIMA(0,d,0) periódico donde d puede variar con el parámetro de estacionalidad. Ravishanker y Ray (2002) \cite{Ravishanker2002207} consideraron la estimación y predicción de modelos ARFIMA multivariados. Baillie y Chung (2002) \cite{Baillie2002215} disertaron sobre el uso de modelos ARFIMA con tendencia estacionaria lineales, mientras que Beran, Feng, Ghosh y Sibbertsen (2002) \cite{Beran2002227} ampliaron este modelo para permitir tendencias no lineales. Souza y Smith (2002) investigaron el efecto de tasas de muestreo diferentes, tales como mensual versus trimestral, en estimaciones del parámetro de memoria larga d. De forma similar, Souza y Smith (2004) observaron los efectos de la agregación temporal en estimaciones y predicciones de procesos ARFIMA. Dentro del contexto de la calidad de control estadística, Ramjee, Crato y Ray (2002)\cite{Ramjee2002291} introdujeron un gráfico de control basado en preducción con una media móvil ponderada hiperbólicamente, diseñada específicamente para modelos ARFIMA no estacionarios.


\section{Modelos ARCH/GARCH}
Una característica clave de las series temporales financieras es que las rentabilidades grandes tienden a continuar rentabilidades grandes y lo mismo con rentabilidades pequeñas, es decir, hay periodos que muestran una alta (o baja) volatilidad. Este fenómeno se conoce como el agrupamiento de volatilidad en econometría y finanzas. La clase de modelos AR condicionados a heteroscedasticidad (ARCH), introducidas por Engle (1982) \cite{Engle1982987}, describen los cambios dinámicos de la varianza condicionada como una función determinista (típicamente cuadrática) de rentabilidades pasadas. Puesto que se conoce la varianza en tiempo -1, las predicciones a un paso están disponibles. A continuación, las predicciones multipaso pueden ser computadas de forma recursiva. Un modelo más parsimonioso que ARCH es el llamado modelo ARCH generalizado (GARCH) (Bollerslev et al., 1994\cite{Bollerslev19942959} y Taylor, 1987 \cite{Taylor1987159}) donde se permiten dependencias adicionales de los retardos de la varianza condicional. Un modelo GARCH tiene una representación tipo ARMA, de forma que los modelos comparten muchas propiedades.

La familia de modelos GARCH, y muchas de sus extensiones, son ampliamente revisadas en Bollerslev, Chou y Kroner (1992)  \cite{Bollerslev19925}, Bera y Higgins (1993) \cite{Bera1993305} y Diebold y Lopez (1995) \cite{Diebold1995253}. No es sorprendente que muchos de los trabajos teóricos hayan aparecido en literatura de econometría. Por otra parte, es interesante el hecho de que ni la IJF ni la JoF hayan llegado a ser foros para publicaciones sobre el rendimiento relativo de predicción de los modelos tipo GARCH o de otros varios modelos de volatilidad en general. Como puede verse a continuación, muy pocos trabajos d IJF/JoF han tratado este tema.

Sabbatini y Linton (1998) mostraron que el modelo simple lineal GARCH(1,1) proporciona una buena parametrización para las rentabilidades diarias en el índice de mercado suizo. Sin embargo, la calidad de las predicciones fuera de muestra sugiere que este resultado hay que tomarlo con precaución. Franses y Ghijsels (1999) \cite{Franses19991} remarcaron que esta característica puede deberse al despreciar valores atípicos aditivos (AO). Se dieron cuenta de que los modelos GARCH para las rentabilidades corregidas resultaban en unas predicciones mejoradas de la volatilidad del mercado de valores. Brooks (1998) \cite{Brooks199859} no ve un ganador claro al comparar predicciones de un paso a partir de modelos tipo GARCH estándar (simétricos) con aquellas de otros modelos lineales y ANNs. A nivel de estimación, Brooks, Burke, y Persand (2001) \cite{Brooks200145}, argumentan que los paquetes de software econométrico pueden producir resultados ampliamente variados. Claramente, esto puede tener algún impacto en la precisión de predicción de los modelos GARCH. Esta observación es muy al estilo de la de Newbold et al. (1994) \cite{Newbold1994573} para modelos ARMA univariados. Fuera de la IJF, Karanasos (2001) \cite{Karanasos2001555} consideró los efectos en media de las predicciones multipaso de modelos ARMA con GARCH. Su método puede emplearse en la derivación de predicciones multi-paso a partir de modelos más complicados, incluyendo GARCH multivariado.

Usando dos series de tasas de cambio diarias, Galbraith y Kisinbay (2005) comparó las funciones de contenido de predicción a partir del modelo GARCH estandar y del modelo GARCH fracionariamente integrado (FIGARCH) (Baillie, Bollerslev, y Mikkelsen, 1996) \cite{Baillie19963}. Las predicciones de varianzas condicionales parecen tener un contenido de información de aproximadamente 30 días de mercado. Otra conclusión es que las predicciones por proyección autorregresiva sobre volatilidades pasadas da mejores resultados que las predicciones basadas en GARCH, estimadas por probabilidad cuasi máxima y los modelos FIGARCH. Esto parece confirmar los resultados anteriores de Bollerslev y Wright (2001), por ejemplo. Una crítica que se oye a menudo sobre los modelos FIGARCH y sus generalizaciones es que no razones económicas por las que la predicción financiera de volatilidad tenga memoria larga. Para una crítica más fundamentada del uso de modelos de memoria larga véase Granger (2002) \cite{Granger2002}.

Empíricamente, las rentabilidades y la varianza condicional de las rentabilidades del siguiente periodo están correladas negativamente. Este fenómeno se denomina volatilidad asimétrica en la literatura (véase Engle y Ng (1993) \cite{Engle19931749}). Esto motivó a los investigadores para que desarrollaran varios modelos tipo GARCH asimétricos (incluyendo el GARCH con cambio de régimen); véase Hentschel (1995) \cite{Hentschel199571} y Pagan (1996) \cite{Pagan199615} para resúmenes generales. Awartani y Corradi (2005) investigaron el impacto de la asimetría en la capacidad de predicción fuera de muestra de diferentes modelos GARCH, con varios horizontes.

Paralelamente a GARCH, muchos otros modelos se han propuesto para predicción de volatilidad. Poon y Granger (2003) \cite{Poon2003478}, proporcionan una revisión cuidadosamente realizada de la investigación en esta área durante los últimos 20 años. Compararon los hallazgos de predicción de volatilidad de 93 publicaciones. Dan importantes ideas en cuestiones como evaluación de predicción, el efecto de la frecuencia de los datos en la precisión de la predicción de volatilidad, medidas de \emph{volatilidad actual}, el efecto de confusión de valores extremos, y otros más. El estudio encontró que la volatilidad implícita proporciona mejores predicciones que los modelos de series temporales. Entre los modelos de series temporales (44 estudios) no había ganador claro entre los modelos de histórico de volatilidad (incluyendo camino aleatorio, medias históricas, ARFIMA, y varias formas de alisado exponencial) y los modelos tipo GARCH (incluyendo ARCH y sus varias ampliaciones), pero ambas clases de modelos mejoraban el rendimiento del modelo de volatilidad estadístico; véase Poon y Granger (2005) para una revisión de estos hallazgos.

El estudio de Poon y Granger contiene muchas cuestiones para continuar estudiando. Por ejemplo, los modelos asimétricos GARCH salían bien parados en los concursos de predicción. Sin embargo, no está claro hasta qué punto esto se debe a las asimetrías de la media condicional, de la varianza condicional, y/o de momentos condicionales de  mayor orden. Otra cuestión para investigación futura concierne la combinación de predicciones. Los resultados en dos trabajos (Doidge y Wei, 1998 \cite{Doidge199828} y Kroner et al., 1995 \cite{Kroner199577}) encuentran de ayuda la combinación, pero en otro estudio (Vasilellis y Meade, 1996) \cite{Vasilellis1996125} no. Sería también útil examinar el rendimiento de predicción de volatilidad de modelos tipo GARCH multivariados y no lineales, incorporando a ambos dependencias contemporáneas y temporales.

\section{Predicción de datos de recuento}
Los datos de recuento son frecuentes en la industria y los negocios, especialmente en datos de inventario donde son llamados \emph{datos de demanda intermitente}. Consecuentemente, es sorprendente que haya tan poco trabajo realizado en predicción de estos datos. Hay algo sobre métodos a medida para predicción de datos de recuento, pero pocos artículos aparecen sobre predicción de series temporales de datos de recuento que usen modelos estadísticos.

La mayoría de la investigación en predicción de recuentos está basada en Croston (1972) \cite{Croston1972289} quien propuso usar SES para predecir independientemente los valores no cero de las series y los intervalos de tiempo entre ellos. Willemain, Smart, Shockor y DeSautels (1994) \cite{Willemain1994529} compararon el método de Croston con SES y encontraron que el método de Croston era más robusto, aunque estos resultados se basaban en en MAPEs que a menudo no están definidos para datos de recuento. Las condiciones bajo las cuales el método de Croston mejora al SES fueron discutidas en Johnston y Boylan (1996) \cite{Johnston1996297}. Willemain, Smart y Schwarz (2004) propusieron un procedimiento de remuestreo para datos de demanda intermitente que se encontró que era más preciso que el de SES y el de Croston para las nueve series evaluadas.

La evaluación de predicciones de recuento conlleva dificultades debidas a la presencia de ceros en los datos observados. Syntetos y Boylan (2005) \cite{Syntetos2005303} propusieron usar el error absoluto medio relativo,mientras que Willemain et al. (2004) \cite{Willemain2004375} recomendaron usar el método de transformación integral de probabilidad de Diebold, Gunther y Tay (1998) \cite{Diebold1998863}.

Grunwald, Hyndman, Tedesco y Tweedie (2000) revisaron muchos modelos estadísticos para series temporales de recuento, usando AR de primer orden como marco unificador para los diferentes enfoques. Un posible modelo, explorado por Brännäs (1995) \cite{Brannas200219}, supone que la serie sigue una distribución de Poisson con media que depende de procesos no observados autocorrelados. Un modelo alternativo MA de valor entero fue usado por Brännäs, Hellström y Nordström (2002) para predecir niveles de ocupación de hoteles suizos.

La distribución de predicción se puede obtener por simulación usando cualquiera de estos modelos estadísticos, pero cómo resumir la distribución no es algo obvio. Freeland y McCabe (2004) propusieron usar la mediana de la distribución de predicción, y dieron un método para computar intervalos de confianza para la distribución completa en el caso de modelos autorregresivos de valores enteros (INAR) de orden 1. McCabe y Martin (2005) ampliaron estas ideas presentando metodologías bayesianas para predicción a partir de la clase de modelos INAR.

\section{Evaluación de predicción y medidas de precisión}
Se ha usado un desconcertante conjunto de medidas de precisión para evaluar el rendimiento de los métodos de predicción. Algunos de los cuales se listan en Mahmoud (1984) \cite{Mahmoud1984139}. Primero definimos las medidas más comunes. 

(poner tabla)


\begin{table}
\caption{Medidas normalmente usadas para precisión de predicción}
\begin{center}
%\resizebox{12cm}{!}{
\begin{tabular}{|C{2cm}|L{4cm}|C{5cm}|}\hline %
\bfseries Medida & \bfseries Descripción & \bfseries Expresión
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{medidasError.csv}{}%
{\medida & \descripcion & \expresion }% 

\end{tabular}
%}
\end{center}
\end{table}
La evolución de las medidas de precisión y evaluación de predicción puede verse a través de las medidas usadas para evaluar los métodos en la mayoría de los estudios comparativos realizados.En la competición original M (Makridakis et al., 1982) \cite{Makridakis1982111} las medidas usadas incluían MAPE, MSE, AR, MdAPE y PB. Sin embargo, como apuntaron Chatfield (1988) \cite{Chatfield198819} y Armstrong y Collopy (1992) \cite{Armstrong199269}, el MSE no es apropiado para comparaciones entre series puesto que depende de la escala. MAPE también tiene problemas cuando la serie tiene valores cercanos a cero o cero, como apuntaron Makridakis, Wheelwright y Hyndman (1998, p.45) \cite{Makridakis1983}. En la competición M se evitaron MAPEs excesivamente grandes (o infinitos) incluyendo sólo datos positivos. Sin embargo, es una solución artificial imposible de aplicar en todas las situaciones.

en 1992, aparecieron dos artículos de IJF y varios comentarios sobre las medidas de evaluación. Armstrong y Collopy (1992) \cite{Armstrong199269} recomendaron usar errores absolutos relativos, especialmente GMRAE y MdRAE, a pesar del hecho de que los errores relativos tienen varianza infinita y media no definida. Ellos recomendaron \emph{winsorizar} para recortar valores extremos lo que parcialmente resolvía estos problemas, pero lo cual añade complejidad al cálculo y un nivel de arbitrariedad al tener que especificar la cantidad de recorte. Fildes (1992) \cite{Fildes199281} también prefirió GMRAE aunque lo expresó en una forma equivalente como la raíz cuadrada de la media geométrica de los errores relativos cuadráticos. Esta equivalencia no parece ser advertida por ninguno de los ponentes en los comentarios de Ahlburg et al. (1992) \cite{Chatfield1992100}. 

El estudio de Fildes, Hibon, Makridakis y Meade (1998), el cual se enfocaba en predicción de datos de telecomunicaciones, usó MAPE, MdAPE, PB, AR, GMRAE y MdRAE teniendo en cuenta algunas de las críticas de los métodos usados para la competición M.

La competición M3 (Makridakis y Hibon, 2000) \cite{Makridakis2000451} usó tres medidas de precisión: MdRAE, sMAPE y sMdAPE. Las medidas \emph{simétricas} fueron propuestas por Makridakis (1993) \cite{Makridakis1993527} en respuesta a la observación de que MAPE y MdAPE tienen la desventaja de que penalizan más errores positivos que negativos. Sin embargo, estas medidas no son tan simétricas como su nombre sugiere. Para el mismo valor de Yt, el valor 2*Abs(Yt-Ft)/(Yt+Ft) tiene más penalización cuando las predicciones son altas en comparación con cuando son bajas. Véase Goodwin y Lawton (1999) \cite{Goodwin1999405} y Koehler (2001) \cite{Koehler2001269} para más disertación sobre este punto.

Es notable que ninguno de los estudios comparativos ha usado medidas relativas (a diferencia de medidas de errores relativos) tales como RelMAE o LMR. Esta última fue propuesta por Thompson (1990) \cite{Thompson1990219}quien abogó por su uso basado en sus buenas propiedades estadísticas. Fue aplicada a los datos de la competición M en Thompson (1991) \cite{Thompson1991331}.

A parte de Thompson (1990) \cite{Thompson1990219}, ha habido poco trabajo teórico en las propiedades estadísticas de estas medidas. Una excepción es Wun y Pearn (1991) quienes estudiaron las propiedades estadísticas de MAE.

Una novedosa medida alternativa de precisión es la \emph{distancia temporal}, que fue considerada por Granger y Jeon, 2003a \cite{Granger2003199} y Granger y Jeon 2003b \cite{Granger2003339}. En esta medida se capturan también  las propiedades de dirección y desfase de la predicción. De nuevo, esta medida no ha sido usada en ningún estudio comparativo importante.

Una línea de investigación paralela ha examinado tests estadísticos para comparar métodos de predicción. Una contribución precoz fue Flores (1989) \cite{Flores1989529}. El mejor enfoque conocido para comprobar diferencias entre la precisión de los métodos de predicción es el test de Diebold y Mariano (1995) \cite{Diebold1995253}. Harvey, Leybourne y Newbold (1997) \cite{Harvey1997281} propusieron una modificación de este test con tamaño corregido. McCracken (2004) \cite{McCracken2004503} examinó el efecto de estimación de parámetros en estos tests y proporcionó un nuevo método de ajuste para error de estimación de parámetros.

Otro problema en evaluación de predicción, y más serio que el error de estimación de parámetros, es el \emph{intercambio de datos}, el uso de los mismo datos para diferentes métodos de predicción. Sullivan, Timmermann y White (2003) \cite{Sullivan2003217} propusieron un procedimiento de remuestreo diseñado para superar la distorsión resultante de inferencia estadística.

Una línea de investigación independiente ha examinado las propiedades teóricas de predicción de modelos de series temporales. Una contribución importante fue Clements y Hendry (1993) \cite{Clements1993617} quienes mostraron que el MSE teórico de un modelo de predicción no era invariante a transformaciones lineales con preservación de escala tales como diferenciación de los datos. En su lugar, propusieron el criterio de \emph{momento segundo de error de predicción generalizado} (GFESM), el cual no tiene esa propiedad no deseable. Sin embargo, es difícil aplicar tales medidas empíricamente y la idea parece no haberse extendido ampliamente.

\section{Combinación}
Durante las últimas 3 décadas se ha estudiado la combinación, mezcla y puesta en común de predicciones obtenidas a partir de diferentes métodos de series temporales y diferentes fuentes de información. Contribuciones prematuras en este área son Bates y Granger (1969) \cite{BATESJM1969451}, Newbold y Granger (1974) \cite{Newbold1974131} y Winkler y Makridakis (1983) \cite{Winkler1983150}. En una revisión bibliográfica exhaustiva, Clemen (1989) \cite{Clemen1989559} resumió una evidencia irresistible de la eficiencia relativa de predicciones combinadas, normalmente en términos de varianzas de error de predicción.

Se han propuesto numerosos métodos para seleccionar los pesos de combinación. La simple media es el método de combinación más ampliamente usado (véase la revisión de Clemen y Bunn, 1985 \cite{Bunn1985151}), pero el método no usa información pasada respecto a la precisión de las predicciones o la dependencia entre las predicciones. Otro simple método es la mezcla lineal de las predicciones individuales con pesos de combinación determinados por OLS (asumiendo insesgamiento) a partir de la matriz de predicciones pasadas y el vector de observaciones pasadas (Granger y Ramanathan, 1984) \cite{Granger1984197}. Sin embargo, las estimaciones OLS de los pesos son insuficientes debido a la posible presencia de correlación en serie en los errores de predicción combinados. Aksu y Gunter (1992) \cite{Aksu199227} y Gunter (1992) \cite{Gunter199245} investigaron este problema con algo más de detalle. Ellos recomendaron el uso de predicciones combinadas OLS con la restricción de que los pesos sumen uno. Granger (1989) \cite{Granger1989167} dio varias ampliaciones de la idea original de Bates y Granger (1969) \cite{BATESJM1969451} incluyendo combinación de predicciones con horizontes mayores que un período.

Mejor que usar pesos fijos, Deutsch, Granger y Teräsvirta (1994) \cite{Deutsch199447} permitieron que estos cambiaran a lo largo del tiempo usando modelos STAR y de cambio de régimen. Otro esquema de ponderación variante en el tiempo fue propuesto por Fiordaliso (1998) \cite{Fiordaliso1998367}, quien usó un sistema difuso para combinar un conjunto de predicciones individuales de forma no lineal. Diebold y Pauly (1990) \cite{Diebold1990503} usaron técnicas de contracción bayesiana para permitir la incorporación a priori de información en la estimación de los pesos de combinación. Zou y Yang (2004) \cite{Zou200469} consideraron la combinación de predicciones de modelos muy similares con actualización secuencial de pesos.

La combinación de pesos determinada a partir de los métodos invariantes en tiempo puede llevar a predicciones relativamente pobres si ocurre no estacionariedad entre las diferentes componentes de predicción. Miller, Clemen y Winkler (1992) \cite{Miller1992515} examinaron el efecto del desplazamiento de ubicación de la no estacionariedad en un conjunto de  métodos de combinación. Concluyeron que la simple media batía otros artefactos de combinación complejos, véase también Hendry y Clements (2002) \cite{Hendry20021} para resultados más recientes. El tema relacionado de combinar predicciones a partir de modelos lineales y otros no lineales, con pesos OLS  así como determinados por un método variante en el tiempo fue abordado por Terui y van Dijk (2002)\cite{Terui2002421}.

La forma de la distribución del error de predicciones combinadas y el correspondiente comportamiento estadístico fue estudiado por de Menezes y Bunn (1998) \cite{DeMenezes1998415} y Taylor y Bunn (1999) \cite{Taylor1999325}. Para distribuciones de error de predicción  no normales, la oblicuidad surge como un criterio relevante para especificar el método de combinación. Fang (2003) \cite{Fang200387} dio algunas ideas de por qué las predicciones en competición pueden ser fructuosamente combinadas para producir una predicción superior a las predicciones individuales abarcando tests de predicción. Hibon y Evgeniou (2005) \cite{Hibon200515} propusieron un criterio para seleccionar entre predicciones y sus combinaciones.

\section{Intervalos de predicción y densidades}
El uso de intervalos de predicción, y más recientemente densidades de predicción, ha ido siendo más usual a lo largo los últimos 25 años conforme los profesionales han sido más conscientes de las limitaciones de las predicciones puntuales. Una revisión importante y completa de los intervalos de predicción se da en Chatfield (1993) \cite{Chatfield1993121}, resumiendo la literatura hasta el momento.

Desafortunadamente, hay algo de confusión en la terminología y algunos autores usan \emph{intervalo de confianza} en lugar de \emph{ intervalo de predicción}. El intervalo de confianza se usa para un parámetro del modelo, mientras que el intervalo de predicción se usa para una variable aleatoria. Casi siempre, los pronosticadores, van a querer intervalos de predicción, intervalos que contengan los valores verdaderos de observaciones futuras con su probabilidad específica.

La mayoría de los intervalos de predicción están basados en un modelo estadístico subyacente. Por tanto, ha habido bastantes trabajos para formular modelos estadísticos apropiados para generalizar procedimientos de predicción comunes.

El vínculo entre las fórmulas de intervalos de predicción y el modelo a partir del que se derivan no siempre se ha examinado correctamente. Por ejemplo, el intervalo de predicción apropiado para un modelo de camino aleatorio fue aplicado por Makridakis y Hibon (1987) \cite{Makridakis1987489} y Lefrançois (1989) \cite{Lefrancois1989553} para predicciones obtenidas a partir de muchos otros métodos. Este problema fue advertido por Koehler (1990) \cite{Koehler1990557} y Chatfield y Koehler (1991) \cite{Chatfield1991239}.

La incertidumbre asociada a la selección del modelo y la estimación de parámetros no se tiene en cuenta con la mayoría de intervalos de predicción basados en modelo. Por tanto, los intervalos son demasiado estrechos. Ha habido investigación considerable en cómo hacer que los intervalos de predicción basados en modelo tengan una cobertura más realista. Una serie de artículos apareció sobre el uso de remuestreo para crear intervalos de predicción basados en modelo para el modelo AR, empezando por Masarotto (1990) \cite{Masarotto1990229} e incluyendo a McCullough, 1994 \cite{McCullough199451}, McCullough, 1996 \cite{Mccullough1996293}, Grigoletto (1998) \cite{Grigoletto1998447}, Clements y Taylor (2001) \cite{Clements2001247} y Kim (2004b) \cite{Kim200485}. Procedimientos similares para otros modelos también se han considerado incluyendo modelos ARIMA (Pascual et al., 2001\cite{Pascual200183}, Pascual et al., 2004 \cite{Pascual2004449}, Pascual et al., 2005 \cite{Pascual2005219} y Wall y Stoffer, 2002\cite{Wall2002733}), VAR (Kim, 1999 \cite{Kim1999393} y Kim, 2004a \cite{Kim200485}), ARCH (Reeves, 2005 \cite{Reeves2005237}) y regresión (Lam \& Veal, 2002 \cite{Lam2002125}). Parece probable que estos métodos de remuestreo se utilicen más conforme las velocidades de cómputo aumente por sus mejores propiedades de cobertura.

Cuando el error de predicción no sigue una distribución normal, hallar la densidad de predicción completa es útil puesto que un intervalo único puede no proporcionar un resumen adecuado del futuro esperado. Un resumen sobre densidad de predicción se da en Tay y Wallis (2000) \cite{Tay2000235} junto con varios otros artículos del mismo número especial de la JoF. Resumiendo, una densidad de predicción ha sido objeto de interesantes propuestas incluyendo los \emph{gráficos de ventilador}  (Wallis, 1999) \cite{Wallis1999106} y las \emph{regiones de mayor densidad} (Hyndman, 1995 \cite{Hyndman1995431}). El uso de estos resúmenes gráficos ha crecido rápidamente en los años recientes y las densidades de predicción se utilizan de forma relativamente amplia.

Conforme los intervalos de predicción y las densidades van siendo más usadas, la atención se centra en su evaluación y testeo. Diebold, Gunther y Tay (1998) \cite{Diebold1998863} introdujeron el método notablemente sencillo de \emph{transformación integral de probabilidad}, el cual puede usarse para evaluar una densidad univariada. Este enfoque ha llegado a usarse ampliamente en un periodo de tiempo muy corto y ha sido un avance clave en este área. La idea se extiende para densidades de predicción multivariada en Diebold, Hahn y Tay (1999) \cite{Diebold1999661}.

Otros enfoques para evaluar la densidad y los intervalos de predicción se da en Wallis (2003) \cite{Wallis2003165} quien propuso tests chi cuadrado para ambos intervalos y densidades, y Clements y Smith (2002) quienes disertaron sobre algunos tests sencillos pero potentes para evaluar densidades de predicción multivariadas.

\section{Una mirada al futuro}
En las secciones previas, se ha examinado la historia de las series temporales de la IJF, con la esperanza de arrojar luz al presente. De cara al futuro, es interesante reflejar las propuestas de investigación identificadas. 

Chatfield (1988) \cite{Chatfield198819} remarcó la necesidad de investigar métodos multivariados enfatizando en hacerlos más prácticos. Ord (1988) \cite{Ord1988389} también advirtió que no se había realizado suficiente trabajo en modelos de múltiples series temporales, incluyendo alisado exponencial multivariado. Hasta la fecha la predicción de series temporales multivariadas todavía no es aplicada ampliamente, a pesar  de los considerables avances en este área. Se sospecha que existen dos razones para ello: una falta de investigación empírica sobre algoritmos de predicción robustos para modelos multivariados, y la escasez de software fácil de usar. Algunos de los métodos sugeridos (modelos VARIMA) son difíciles de estimar por el gran número de parámetros que incluyen. Otros como el alisado exponencial multivariado, no han adquirido suficiente atención teórica para su aplicación rutinaria. Un enfoque para predicción de series temporales multivariadas consiste en usar modelos de factor dinámico. Estos modelos promete tanto teóricamente (Forni et al., 2005 \cite{Forni2005830} y Stock y Watson, 2002 \cite{Stock20021167}) como para su aplicación (Peña y Poncela, 2004 \cite{Pena2004291}) y se cree que llegarán a usarse más ampliamente en el futuro.

Ord (1988) \cite{Ord1988389} también indicó la necesidad de investigar más profundamente en los métodos de predicción basados en modelos no lineales. Si bien muchos aspectos de los modelos no lineales se han investigado en la IJF, se debe seguir investigando de forma continuada. Por ejemplo, no existe un claro consenso de que las predicciones de los modelos no lineales superen categóricamente las de los modelos lineales (véase Stock y Watson, 1999 \cite{Stock19991}). 

Otros temas que sugiere Ord (1988) \cite{Ord1988389} incluye la necesidad de desarrollar procedimientos de selección de modelo que hagan un uso efectivo tanto de los datos como del conocimiento a priori, a la necesidad de especificar objetivos para las predicciones y desarrollar sistemas de predicción dirigidos hacia estos objetivos. Estas áreas todavía necesitan atención y se espera que aparezcan herramientas para solventar estos problemas.

Dado el frecuente mal uso de métodos basados en modelos lineales con distribuciones de error gaussianas independientes e idénticas, Cogger (1988) \cite{Cogger1988403} abogó por que los nuevos desarrollos del área de métodos estadísticos \emph{robustos} recibieran más atención dentro de la comunidad de la predicción de series temporales. Un procedimiento robusto se espera que funcione bien cuando existan valores atípicos o desplazamientos de ubicación en los datos que sean difíciles de detectar. La estadística robusta puede estar basada tanto en métodos paramétricos como no paramétricos. Un ejemplo de este último, es el concepto de cuantiles de regresión de Kkoenker y Bassett (1978) \cite{Koenker197833} investigado por Cogger. En predicción, estos puede aplicarse como cuantiles condicionados univariados y multivariados. Un área de aplicación importante es al estimar herramientas de manejo de riesgo como el \emph{valor en riesgo}. Recientemente, Engle y Manganelli (2004) \cite{Engle2004367} realizaron un comienzo en esta dirección, proponiendo un modelo de valor en riesgo condicional. Se espera que haya muchas más investigación en este área.

Un tema relacionado en donde ha habido bastante actividad investigadora es la densidad de predicción, donde el foco está en la densidad de probabilidad de observaciones futuras más allá de la media o la varianza. Por ejemplo, Yao y Tong (1995) \cite{Yao1995395} propusieron el concepto de intervalo de predicción de percentil condicional. Su anchura ya no es una constante, como en el caso de los modelos lineales, y puede variar respecto a la posición del espacio de estado a partir del que se realizan las predicciones; véase también De Gooijer y Gannoun (2000) \cite{DeGooijer2004237} y Polonik y Yao (2000) \cite{Polonik2000509}.

Claramente, el área de los intervalos de predicción mejorada requiere mayor investigación. Esto está en concordancia con Armstrong (2001) \cite{Armstrong2001} quen listó 23 principios de gran necesidad de interés incluyendo el elemento 14:13: \emph{Para intervalos de predicción, incorporar la incertidumbre asociada con la predicción de las variables explicativas}.


En los años recientes, las series temporales no gaussianas, han comenzado a recibir una atención considerable y los métodos de predicción van lentamente siendo desarrollados. En particular las series no gaussiana con valores positivos tiene importantes aplicaciones. Dos áreas importantes son la volatilidad y la duración entre transacciones. Algunas contribuciones importantes hasta la fecha han sido el modelo de \emph{duración condicional autorregresivo} de Engle y Russell (1998) \cite{Engle19981127} y Andersen, Bollerslev, Diebold y Labys (2003) \cite{Andersen2003579}. Dada la importancia de estas aplicaciones, se espera mucho más trabajo en este área.

Si bien, las series temporales no gaussianas con espacio de muestreo continuo han comenzado a recibir mayor atención de investigación, especialmente en el contexto de las  finanzas, la predicción de series temporales con un espacio muestral discreto (tales como las series de recuentos) todavía está en su infancia. Tales datos son muy frecuentes en la industria y los negocios y existen muchos problemas asociados a la predicción de recuentos sin resolver teóricamente o de forma práctica; por tanto, se espera una mayor investigación productiva en este área en el futuro cercano.

Otros autores han intentado identificar temas de investigación importantes. Tanto De Gooijer (1990) \cite{deGooijer1990449} como Clements (2003) \cite{Clements20031} en dos editoriales, y Ord como parte de un artículo de disertación de Dawes, Fildes, Lawrence y Ord (1994) \cite{Dawes1994151} sugirieron más trabajo en predicciones combinadas. Aunque el tema ha recibido una cantidad de atención considerable, todavía hay algunas cuestiones abiertas. Por ejemplo, cuál es el mejor método de combinación para modelos no lineales y lineales y qué intervalo de predicción puede establecerse en torno a la predicción combinada. Un buen punto de partida para mayor investigación en este área es Teräsvirta (2006) \cite{Terasvirta2006}; véase también Armstrong (2001, 12.5-12.7) \cite{Armstrong2001}. Recientemente, Stock y Watson (2004) \cite{Stock2004405} discutieron el \emph{puzle de combinación de predicción}, a saber, el hallazgo empírico repetido de que combinaciones simples como las medias superan combinaciones más sofisticadas en las que la teoría sugiere mejor funcionamiento. Esta importante cuestión práctica sin duda recibirá mayor atención de investigación.

Los cambios en el almacenamiento y recolección de datos también conducirá a nuevas direcciones de investigación. Por ejemplo, en el pasado, los paneles de datos disponibles (llamados datos longitudinales en bioestadística) tenían la dimensión temporal de la serie \emph{t} pequeña mientras que la dimensión de sección cruzada \emph{n} era grande. Sin embargo, ahora en muchas áreas aplicadas como marketing, se pueden obtener fácilmente grandes conjuntos de datos con ambos \emph{n} y \emph{t} grandes. La extracción de características a partir de lo paneles de datos es el objeto del \emph{análisis de datos funcionales}; véase por ejemplo  Ramsay y Silverman (1997) \cite{Ramsay1997}. Sin embargo, el problema de realizar predicciones multi-paso basadas en datos funcionales sigue abierto tanto a investigación teórica como práctica. Dada el predominio creciente de este tipo de datos, se espera que este área sea fructífera en el futuro.

Los conjuntos de datos grandes, también se prestan a métodos de computación intensiva. Mientras las redes neuronales se han usado en predicción durante más de dos décadas, existen muchas cuestiones excepcionales asociadas con su uso e implementación, incluyendo el cuándo van a mejorar probablemente otros métodos. Otros métodos que implican computación pesada (como bagging y boosting) son incluso menos entendidos en el contexto de predicción. Con la disponibilidad de conjuntos de datos muy grandes y potencia alta de computación se espera que ésta sea un área de investigación importante.


\bibliographystyle{splncs}

\bibliography{bibliography}


\end{document}

%%Está chulo el trabajo, el STATE OF ART es de premio :)
