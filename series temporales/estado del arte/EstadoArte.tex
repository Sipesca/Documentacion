\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{csvsimple}
\usepackage[spanish,es-noshorthands,es-ucroman,es-tabla]{babel}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{amsmath}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Estado del arte en predicción de series temporales}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{J.J. Asensio, P.A. Castillo}
\authorrunning{J.J. Asensio et al.}

\institute{
Departamento de Arquitectura y Tecnología de Computadores \\
ETSIIT, CITIC-UGR \\
Universidad de Granada, España \\
\email{\{asensio, pacv\}@ugr.es}
}

\maketitle
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 
bonito abstract test
\end{abstract}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introducción}
\label{sec:intro}


\section{Predicción de series temporales}
\subsection{Preámbulo}
En esta sección se resumen algunos trabajos relevantes en el área de predicción de series temporales.Los métodos que se comentan a continuación se clasifican según los modelos utilizados. 

\subsection{Alisado exponencial}

Los métodos de alisado exponencial habitualmente se utilizan en la estimación de la demanda de un producto para un periodo dado. Una forma sencilla de realizar la predicción consiste en tomar la media ponderada de los valores previos dando mayor peso a los valores más cercanos en el tiempo. El error en la predicción actual puede ser tenida en cuenta para la siguiente predicción.

Estos métodos tienen su origen en las décadas 50 y 60 con el trabajo de Brown (1959, 1963) \cite{Brown1959} \cite{Brown1963}, Holt (1957, reimpreso en 2004)\cite{Holt20045}, y Winters (1960) \cite{Winters1960324}. Pegels (1969) \cite{Pegels1969311} proporcionó una clasificación sencilla pero útil de la tendencia y los patrones estacionales dependiendo de si eran aditivos (lineales)  o multiplicativos (no lineales).

Los métodos de alisado exponencial fueron ampliamente utilizados en la industria y los negocios, sin embargo no habían recibido mucha atención por parte de los estadísticos y no tenían un fundamento estadístico bien desarrollado. 

A partir de Muth (1960) \cite{Muth1960299}  surge la primera base estadística para el alisado exponencial simple (SES) demostrando que proporcionaba una predicción óptima para caminos aleatorios con ruido. 

Posteriormente Box y Jenkins (1970), Roberts (1982)\cite{Roberts1982808}, y Abraham y Ledolter (1983, 1986) \cite{Abraham1983}\cite{Abraham198651}, mostraron que algunas predicciones de alisado exponencial resultan ser casos especiales de modelos ARIMA. Snyder (1985) \cite{Snyder1985272} demostró que SES podría ser también una forma de modelo de espacio de estados de innovaciones (con una única fuente de error). 

Taylor (2003)\cite{Taylor2003715} proporcionó los únicos métodos de alisado exponencial multiplicativos genuinamente nuevos. Entre los trabajos de aplicación se destacan varios contextos incluyendo componentes de computadores (Gardner, 1993 \cite{GardnerJr1993245}), pasajeros de vuelo (Grubb \& Masa, 2001 \cite{Grubb200171}), y planificación de producción (Miller \& Liberatore, 1993 \cite{Miller1993509}).

En Hyndman, Koehler, Snyder, y Grose (2002) \cite{Hyndman2002439} y en la ampliación de Taylor (2003) \cite{Taylor2003715} se da una clasificación de este tipo de métodos. Cada método se componen de una de cinco tipos de tendencia (ninguna, aditiva, aditiva alisada, multiplicativa y multiplicativa alisada) y uno de tres tipos de estacionalidad (ninguna, aditiva y multiplicativa). Por tanto, hay 15 métodos diferentes, donde entre los más conocidos está SES (sin tendencia ni estacionalidad), el método lineal de Holt (tendencia aditiva, sin estacionalidad), el método aditivo de Holt-Winters (tendencia aditiva y estacionalidad aditiva), y el método no lineal de Holt-Winters (tendencia aditiva y estacionalidad multiplicativa).


\subsection{Modelos ARIMA}

Los primeros intentos de estudiar series temporales, particularmente en el siglo 19, estaban generalmente caracterizados por la idea de un mundo determinista. Fue la mayor contribución de Yule (1927) \cite{Yule1927267} la que impulsó la noción de aleatoriedad en series temporales postulando que cada serie temporal puede ser considerada como la realización de un proceso estocástico. Basado en esa sencilla idea, se desarrolló desde entonces un gran número de métodos de series temporales. Slutsky, Walker, Yaglom y Yule formularon el concepto de los modelos auto-regresivos (AR) y media móvil (MA). El teorema de la descomposición de Wold condujo a la formulación y solución del problema de predicción lineal de Kolmogorov (1941) \cite{Kolmogorov19411}. Desde entonces, una gran cantidad de literatura ha aparecido en el área de series temporales, tratando estimación de parámetros, identificación, comprobación de modelos y predicción, véase Newbold (1983) \cite{Newbold198323} para un estudio anterior.

La publicación \emph{Time Series Analysis: Forecasting and Control} de Box y Jenkins (1970) \cite{Box1976} integró el conocimiento existente. Además, estos autores desarrollaron una ciclo coherente, versátil e iterativo en tres etapas para la identificación, estimación y verificación de series temporales (conocido como el enfoque Box-Jenkins). El libro ha tenido un enorme impacto en la teoría y práctica moderna de análisis y predicción de series temporales. Con la llegada de los ordenadores, se popularizó el uso de los modelos ARIMA y se extendió a muchas áreas de la ciencia. De hecho, la predicción de series temporales discretas mediante modelos ARIMA univariados, modelos de función de transferencia (regresión dinámica), y modelos ARIMA multivariados han generado bastantes artículos en IJF. A menudo estos estudios son de naturaleza empírica, y usan uno o varios métodos/modelos como punto de referencia para comparar. 

\begin{table}
\caption{Lista de ejemplos de aplicaciones reales}

\resizebox{12cm}{!}{
\begin{tabular}{|L{5cm}|C{3cm}|L{4cm}|c|}\hline %
\bfseries Dataset & \bfseries Horizonte & \bfseries Benchmark & \bfseries Referencia
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{ejemplos.csv}{}%
{\dataset & \horizon & \benchmark & \reference }% 

\end{tabular}
}
\end{table}
\subsubsection{Univariado}

El éxito de la metodología de Box-Jenkins se basa en el hecho de que los modelos pueden imitar el comportamiento de los diversos tipos de series y sin requerir muchos parámetros a estimar en la elección final del modelo. Sin embargo, a mediados de los años sesenta, la selección de un modelo era en gran medida una cuestión subjetiva del investigador, no había ningún algoritmo para especificar un modelo único. Desde entonces, muchas técnicas y métodos han surgido para añadir rigor matemático en el proceso de búsqueda de un modelo ARMA, incluyendo el criterio de información Akaike (AIC), el error de predicción final de Akaike (FPE) y el criterio de información bayesiano (BIC). A menudo, estos criterios se reducen a la minimización de errores de predicción a un paso (en la muestra) , con un término de penalización para el sobreajuste. FPE también ha sido generalizado para predicción a varios pasos (véase por ejemplo, Bhansali, 1996 \cite{Bhansali1996577} y Bhansali, 1999 \cite{Bhansali1999295}), pero esta generalización no ha sido utilizada en el área aplicada. Este también parece ser el caso de criterios basados en principios de validación cruzada y validación por división muestral (véase por ejemplo, West , 1996 \cite{West19961084}), haciendo uso de errores de predicción fuera-de-muestra; véase Peña y Sánchez (2005) \cite{Pena2005135} para un enfoque relacionado a considerar.

El problema de añadir información externa (prior) en las predicciones de ARIMA univariado fue considerado por Cholette (1982) \cite{Cholette1982375}, Guerrero (1991) \cite{Guerrero1991339} y de Alba (1993) \cite{deAlba199395}).

Como alternativa a la metodología de ARIMA univariado, Parzen (1982) \cite{Parzen198267} propuso la metodología ARARMA. La idea clave es que una serie temporal es transformada de un filtro AR de memoria larga, a un filtro de memoria corta, evitando así un operador de diferenciación más duro. Además, se usa una fase de identificación diferente al convencional de Box-Jenkins. En la competición M (Makridakis et al., 1982 \cite{Makridakis1982111}), los modelos ARARMA consiguieron el MAPE más bajo para horizontes de predicción mayores. 

El modelado automático de ARIMA univariado, ha mostrado una capacidad de predicción a un paso tan precisa como la de otros métodos competentes (Hill y Fildes, 1984, Liber, 1984, Poulos et al., 1987 y Texter y Ord, 1989). Varios proveedores de software han implementado métodos de predicción automáticos de series temporales (incluso métodos multivariados); véase Geriner y Ord (1991) \cite{Geriner1991127}, Tashman y Leach (1991) \cite{Tashman1991209}, y Tashman (2000) \cite{Tashman2000437}. A menudo estos métodos actúan como caja negra. La tecnología de sistemas expertos (Mélard \& Pasteels, 2000) se puede usar para evitar este problema. Algunas directrices en la elección de un método de predicción automático se da en Chatfield (1988) \cite{Chatfield198819}.

Mejor que adoptar un modelo AR para todos los horizontes de predicción, Kang (2003) \cite{Kang2003387} investigó empíricamente el caso de seleccionar un modelo AR  de forma diferenciada para cada horizonte en predicciones multi-paso.

\subsubsection{Función de transferencia}
La identificación de modelos de función de transferencia puede ser difícil cuando hay más de una variable de entrada. Edlund (1984) \cite{Edlund1984297} presentó un método en dos pasos para identificación de una función respuesta impulso cuando varias variables de entrada están correladas. Koreisha (1983) \cite{Koreisha1983151} estableció varias relaciones entre funciones de transferencia, implicaciones causales y especificación del modelo econométrico. Gupta (1987) \cite{Gupta1987195} identificó los principales problemas en las comprobaciones de causalidad. Usando análisis de componentes principales, del Moral y Valderrama (1997) \cite{DelMoral1997237} sugirieron una representación parsimoniosa de  modelo de función de transferencia . Krishnamur, Narayan y Raj (1989) \cite{Krishnamurthi198921} mostraron cómo se pueden obtener estimaciones más precisas del impacto de las intervenciones en modelos de función de transferencia usando una variable de control.


\subsubsection{Multivariado}
El modelo de vector ARIMA (VARIMA) es una generalización multivariada del modelo univariado ARIMA. Las características de población de procesos VARMA parecen haber sido derivados primero por Quenouille (1957) \cite{Quenouille1957}, aunque el software para su implementación comenzó estar disponible en los 80 y 90. Riise y Tjostheim (1984) \cite{Riise1984309} mostraron como los filtros de suavizado pueden construirse dentro de los modelos VARMA. El alisado previene fluctuaciones irregulares en series temporales explicativas al migrar las predicciones de la serie dependiente. Para determinar el máximo horizonte de predicción para procesos VARMA, DeGooijer y Klein (1991) \cite{DeGooijer1992135} establecieron las propiedades teóricas de las predicciones y el error de predicción acumulado a varios pasos. Lütkepohl (1986) \cite{Lutkepohl1986461} estudió los efectos de la agregación temporal y el muestreo sistemático en predicción, suponiendo que la variable desagregada (estacionaria) sigue un proceso VARMA de orden desconocido. Posteriormente, Bidarkota (1998) \cite{Bidarkota1998457} consideró el mismo problema pero con las variables observadas integradas en lugar de estacionarias.

Los vectores de auto-regresión (VAR) constituyen un caso especial de una clase más general de modelos VARMA. En esencia, un modelo VAR es una aproximación flexible a la forma reducida de una variedad amplia de modelos econométricos dinámicos. Los modelos VAR se pueden especificar de varias formas. Funke (1990) \cite{Funke1990363} presentó 5 especificaciones diferentes de VAR y comparó sus rendimientos de predicción usando series mensuales de producción industrial.

En general, los modelos VAR tienden a sufrir sobreajuste con demasiados parámetros libres y no significativos. Como resultado, estos modelos pueden dar pobres predicciones fuera de muestra, incluso aunque el ajuste en las muestras sea bueno; véase Liu, Gerlow y Irwin (1994) \cite{Liu1994419} y Simkins (1995) \cite{Simkins1995569}. En lugar de restringir algunos parámetros como habitualmente se hace, Litterman (1986) \cite{Litterman198625} y otros impusieron una distribución preferente en los parámetros, expresando la idea de que muchas variables económicas se comportan como un camino aleatorio. 

\subsection{Estacionalidad}

El enfoque más antiguo para controlar la estacionalidad en series temporales es extraerla usando un procedimiento iterativo de descomposición como el método X-11. Este método y sus variantes (incluyendo versiones más reciente, X-12-ARIMA, Findley, Monsell, Bell, Otto y Chen, 1998 \cite{Findley1998127}, se han estudiado extensivamente durante los últimos años derivando en varios enfoques.

Uno de ellos estudia el uso predicción como parte del método de descomposición estacional. Quenneville, Ladiray y Lefrançois (2003) \cite{Quenneville2003727} con otro enfoque observaron las predicciones implícitas mediante los filtros de medias móviles asimétricos del método x-11 y sus variantes. Un tercer enfoque fue ver la efectividad de predicción usando datos ajustados estacionalmente a partir de un método de descomposición estacional. 

Además de trabajar en el método X-11 y sus variantes, también se han desarrollado varios métodos nuevos para el ajuste estacional, entre los que destaca el enfoque basado en modelo de TRAMO-SEATS (Gómez y Maravall, 2001 \cite{Gomez2001} y Kaiser y Maravall 2005 \cite{Kaiser2005691}) y el método no paramétrico STL (Cleveland, Cleveland, McRae y Terpenning, 1990 \cite{Cleveland19903}). Otra propuesta ha sido el uso de modelos sinusoidales (Simmons, 1990 \cite{Simmons1990485}).

En la predicción de varias series similares, Withycombe (1989) \cite{Withycombe1989547} mostró que puede ser más eficiente estimar una componente estacional combinada para el grupo de series, en lugar de usar sus patrones individuales. Bunn y Vassilopoulos (1993) \cite{Bunn1993517} demostró cómo usar agrupamiento para formar grupos apropiados para esta situación, y Bunn y Vassilopoulus (1999)\cite{Bunn1999431} introdujo algunos estimadores mejorados para los índices de grupo estacional.

A principio de los 80, las pruebas de raíz unitarias acababan de ser inventadas y las pruebas de raíz unitaria estacional estaban a punto de llegar. Posteriormente hubo un trabajo considerable en el uso e implementación de las pruebas de raíz unitaria estacional incluyendo Hylleberg y Pagan (1997) \cite{Hylleberg1997329}, Taylor (1997)\cite{Taylor1997307} y Franses y Koehler (1998) \cite{Franses1998405}. 

Los modelos de series temporales periódicas fueron también exploradas por Wells (1997) \cite{Wells1997407}, Herwartz (1997) \cite{Herwartz1997421} y Novales y de Fruto (1997) \cite{Novales1997393}, los cuales encontraron que los modelos periódicos pueden conducir a una mejora en el rendimiento de la predicción comparados con modelos no periódicos bajo algunas condiciones. La predicción de procesos ARMA periódicos multivariados fue considerada por Ullah (1993) \cite{Ula1993645}.

Varios artículos han comparado empíricamente los modelos estacionales. Chen (1997) \cite{Chen1997269} exploró la robustez de un modelo estructural, un modelo de regresión con estacionalidad, un modelo ARIMA, y el método de Holt-Winters, y encontró que los dos últimos daban predicciones relativamente robustas cuando el modelo no estaba completamente especificado. Noakes, McLeod, y Hipel (1985) \cite{Noakes1985179}, Albertson y Aylen (1996) \cite{Albertson1996345}, Kulendran y King (1997) \cite{Kulendran1997319} y Franses y van Dijk (2005) \cite{Franses200587} compararon el rendimiento de predicción de varios modelos estacionales aplicados a datos reales. El modelo con mejor predicción variaba a lo largo del estudio, dependiendo de qué modelo se usara y la naturaleza de los datos. Parece no haber consenso todavía en qué condiciones se puede preferir un modelo u otro.

\subsection{Espacio de estado, modelos estructurales y el filtro de Kalman}

A principios de  1980, los modelos de espacio de estado acababan de empezar a ser usados por los estadísticos para predecir series temporales, aunque las ideas ya habían estado presentes en la literatura de ingeniería desde el innovador trabajo de Kalman (1960) \cite{Kalman196035}. Los modelos de espacio de estado proporcionan un marco unificado en el que se puede escribir cualquier modelo lineal de serie temporal. La contribución clave de predicción de Kalman (1960) fue dar un  algoritmo recursivo (conocido como el filtro de Kalman) para computar las predicciones. Los estadísticos empezaron interesarse por los modelos de espacio de estado cuando Schweppe (1965) \cite{Schweppe196561} mostró que el filtro de Kalman proporciona un método eficiente para computar los errores de la predicción a un paso y su varianza asociada necesaria para producir la función de probabilidad. Shumway y Stoffer (1982)\cite{Shumway1982253} combinaron el algoritmo EM con el filtro de Kalman para dar un enfoque general para predecir series temporales usando modelos de espacio de estado, incluyendo la posibilidad de tener observaciones inexistentes.

Una clase particular de modelos de espacio de estado, conocidos como \emph{modelos lineales dinámicos} (DLM) fue introducida por Harrison y Stevens (1976) \cite{Harrison1976205}, quienes también propusieron un enfoque bayesiano para la estimación. Fildes (1983) \cite{Fildes1983137} comparó las predicciones obtenidas usando el método de Harrison y Stevens con métodos más sencillos como el alisado exponencial concluyendo que la complejidad adicional no conducía a una mejora del rendimiento de predicción. Harvey, 1984 \cite{Harvey1984245} y 1989 \cite{Harvey1989} amplió esta clase de modelos y siguió un enfoque no bayesiano para la estimación.  Harvey (2006) \cite{Harvey2006327} proporciona una revisón comprensible y una introducción para esta clase de modelos incluyendo variaciones no gausianas y en tiempo continuo. 

Estos modelos poseen muchas similaridades con los métodos de alisado exponencial, pero tienen múltiples fuentes de error aleatorio. En particular, el \emph{modelo estructural básico} (BSM) es similar a método de Holt-Winters para datos estacionales e incluye componentes de nivel, tendencia y estacionalidad.

Otra clase de modelos de espacio de estado, conocida como \emph{modelos balanceados de espacio de estado}, se ha usado principalmente para predicción de series temporales macro-económicas. Mittnik (1990)\cite{Mittnik1990337} proporcionó un estudio de esta clase de modelos, y Vinod y Basu (1995) \cite{Vinod1995217} obtuvo predicciones para el consumo, ingresos y tasas de interés usando estos modelos. Estos sólo tienen una única fuente de error aleatorio y generalizan otros modelos como ARMAX, ARMA y modelos con retardo racional distribuido. Una clase de espacio de estados relacionada son los modelos de \emph{fuente única de error} en los que subyacen los métodos de alisado exponencial.

A parte de estos desarrollos metodológicos, han habido varios artículos proponiendo modelos innovadores de espacio de estados para resolver problemas prácticos de predicción. Entre ellos, Coomes (1992) \cite{Coomes1992473} quien usó un modelo para predecir empleos según la industria para regiones locales y Patterson (1995) \cite{Patterson1995395} quien usó un enfoque de espacio de estado para predicción de ingresos personales disponibles. 

Los libros de Harvey (1989) \cite{Harvey1989}, West y Harrison (1989) \cite{West1997}y Durbin y Koopman (2001) \cite{Durbin2001} han tenido un impacto sustancial en la literatura de series temporales en cuanto a investigación de modelos de espacio de estado, filtrado de Kalman y modelos estructurales discretos/continuos en tiempo. 


\subsection{Modelos no lineales}

Aunque la linealidad es un supuesto útil y una herramienta potente  en muchas áreas, a finales de los 70 y principios de los 80, los modelos lineales eran insuficientes en muchas aplicaciones reales. Por ejemplo, los ciclos prolongados de tamaño de población animal, (los famosos datos del lince Canadiense), ciclos solares (número anual de manchas solares), flujo de energía y relaciones de amplitud-frecuencia, son ejemplos donde no se adecuan los modelos lineales. Para satisfacer esta demanda, se propusieron modelos no lineales de series temporales útiles durante ese periodo. De Gooijer y Kumar (1992)  \cite{DeGooijer1992135} ofrecieron un resumen de los desarrollos en el área al principio de los 90. Estos autores argumentaban que el rendimiento superior en predicción de estos modelos no lineales era irregular.

Un factor que probablemente ha retrasado la difusión de publicaciones de predicciones no lineales es que hasta entonces no era posible obtener expresiones analíticas cerradas para predicciones multi-paso. Hoy día, las predicciones no lineales se obtienen mediante simulación Monte Carlo o por \emph{bootstrapping} (remuestreo). El último caso se prefiere al no necesitar suposiciones sobre la distribución del proceso de error.


\subsubsection{Modelos de cambio de régimen}
La clase de modelos no lineales autorregresivos con umbral auto-excitado (SETAR) fue promocionada de forma prominente mediante los libros de Tong, 1983  \cite{Tong1983} y 1990  \cite{Tong1990}. Clements y Smith (1997)  \cite{Clements1997463} compararon varios métodos para obtener predicciones mult-paso para modelos SETAR univariados discretos. Concluyeron que las predicciones mediante simulación Monte Carlo eran satisfactorias en casos donde se sabe que las perturbaciones en el modelo SETAR vienen de una distribución simétrica. En otro caso se prefiere el método de \emph{bootstrapping}. 

Una desventaja del modelo SETAR es que la dinámica cambia de forma discontinua de un régimen a otro. En cambio, un modelo de transición suave (STAR) permite una transición más gradual entre los distintos regímenes. 

Fok, van Dijk y Franses (2005) \cite{Fok2005785} examinaron la siguiente cuestión clave: ¿Puede un modelo STAR multinivel de datos de panel para series desagregadas mejorar las predicciones de agregados macroeconómicos como ingresos totales o desempleo total?. El modelo STAR propuesto parece merecer la pena investigarse en más detalle puesto que permite que los parámetros que gobiernan el cambio de régimen sean distintos según los estados. Basándose en simulaciones y hallazgos empíricos, los autores afirman que de hecho se pueden conseguir mejoras en las predicciones a un paso.

Franses, Paap, y Vroomen (2004) \cite{Franses2004255} propusieron un modelo con umbral AR(1) que permite una inferencia plausible sobre valores específicos de los parámetros. La idea clave es que los valores de los parámetros de AR dependen de una variable indicadora dominante. El modelo resultante mejora el rendimiento respecto a otros modelos no lineales cambiantes en tiempo, incluyendo el modelo de cambio de régimen de Markov, en términos de predicción.

\subsubsection{Modelos con coeficiente funcional}
Un modelo AR con coeficiente funcional (FCAR o FAR) es un modelo AR en el que los coeficientes AR pueden variar como una función suave de otra variable, como un valor retrasado de la propia serie o una variable exógena. El modelo FCAR incluye los modelos TAR y STAR como casos especiales y es análogo al modelo aditivo generalizado de Hastie y Tibshirani (1991) \cite{Hastie1990}. 

\subsubsection{Redes neuronales}
Una red neuronal artificial (ANN) puede ser útil para procesos no lineales que tengan relaciones funcionales desconocidas y por tanto difíciles de ajustar (Darbellay y Slama, 2000 \cite{Darbellay200071}). La idea principal con ANN es que las entradas, o variables dependientes, son filtradas mediante una o más capas ocultas cada una de las cuales consta de nodos antes de alcanzar la variable de salida. La salida intermedia se enlaza a la salida final. Otros modelos son versiones específicas de ANN donde se impone cierta estructura (véase JoF Special Issue 17:5/6 (1998).

Una de las mayores áreas de aplicación de ANN es predicción; véase Zhang, Patuwo y Hu (1998) \cite{Zhang199835} y Hippert, Pedreira y Souza (2001) \cite{Hippert200144} para revisiones de la literatura. Sin embargo no hay un claro consenso sobre cuándo es mejor utilizar ANNs para predicción.

Gorr (1994) \cite{Gorr19941} y Hill, Marquez, OConnor y Remus (1994) \cite{Hill19945} sugirieron que la investigación a seguir debería tratar de definir mejor los límites en que ANN mejora las técnicas tradicionales y viceversa. 

Un problema general con los modelos no lineales se encuentra en la complejidad de los modelos y su parametrización excesiva. Si se considera realmente importante el principio de parsimonia, es interesante comparar el rendimiento de predicción fuera de muestra de los modelos lineales frente a los no lineales, usando una amplia variedad de criterios de selección de modelo. Esta cuestión fue considerada en bastante profundidad por Swanson y White (1997) \cite{Swanson1997439}. Sus resultados sugirieron que una simple ANN con una única capa oculta \emph{feed-forward}, la cual era muy popular en series temporales econométricas, ofrece una alternativa útil y flexible a modelos lineales de especificación fijada, particularmente para horizontes de predicción mayores de un paso. En contraste con Swanson y White, Heravi, Osborn y Birchenhall (2004) \cite{Heravi2004435} encontraron que los modelos lineales producen predicciones más precisas de producción industrial mensual europea sin ajuste estacional que los modelos de ANN. Ghiassi, Saidane, y Zimbra (2005) \cite{Ghiassi2005341} presentaron una ANN dinámica y compararon su rendimiento de predicción frente a la ANN tradicional y los modelos ARIMA.

Con el tiempo, la importancia del riesgo de la parametrización excesiva y el sobreajuste ha sido reconocida por varios autores; véase Hippert, Bunn y Souza (2005) \cite{Hippert2005425} que usaron una gran ANN (50 entradas, 15 neuronas ocultas y 24 salidas) para predecir perfiles de carga de electricidad diaria. Sin embargo, la cuestión de si la ANN está sobre-parametrizada o no, sigue sin resolverse. Algunas ideas con valor potencial para construir ANNs minimizando el número de parámetros, usando inferencia estadística se sugieren en Teräsvirta, van Dijk y Medeiros (2005) \cite{Terasvirta2005755}.


\subsubsection{Otros}
(aquí va todo lo de machine learning)



\subsection{Modelos de memoria larga}
Cuando el parámetro de integración d de un modelo ARIMA es fraccional y mayor que cero, el proceso exhibe memoria en el sentido de que las observaciones de un periodo de tiempo largo, tiene una dependencia no despreciable. Los modelos estacionarios com memoria larga (0 < d < 0.5), también llamados ARMA diferenciados fraccionalmente (FARMA) o modelos ARMA integrados fraccionalmente (ARFIMA), han sido estudiados en muchos campos; véase Granger y Joyeux (1980) \cite{Granger198015} para una introducción. Una motivación para estos estudios es que muchas series temporales empíricas tienen una función de autocorrelación muestral que decae a una tasa menor que en un modelo ARIMA de orden finito y entero d.

El potencial de predicción de modelos ajustados a FARMA/ARFIMA, en contraposición con resultados de predicción obtenidos con otros modelos de series temporales, ha sido tema de varios artículos en IJF y un special issue (2002, 18:2). Ray, 1993a \cite{Ray1993255} y Ray, 1993b \cite{Ray1993511} realizaron un estudio comparativo entre los modelos FARMA/ARFIMA estacionales y los modelos estándar (no fraccionarios) ARIMA estacionales. Los resultados muestran que los modelos AR de mayor orden son capaces de predecir bien a largo plazo comparados con los modelos ARFIMA. Como continuación, Smith y Yadav (1994) \cite{Smith1994507} investigaron el coste de asumir una diferencia unitaria cuando una serie es sólamente fraccinoariamente integrada con d distinto de 1. La sobre-diferenciación de una serie producirá una pérdida en el rendimiento de predicción de un paso, con un pérdida limitada en adelante. En cambio, una serie bajo-diferenciada, es más costosa con mayores potenciales de pérdida a partir del ajuste de un modelo AR poco especificado para cualquier horizonte de predicción. La cuestión es también explorada por Andersson (2000) \cite{Andersson2000121} quien mostró que la mala especificación afecta fuertemente la memoria estimada del modelo ARFIMA, usando una regla que es similar al test de Öller (1985) \cite{Oller1985135}. Man (2003) \cite{Man2003477} argumentó que un modelo ARMA(2,2) adecuadamente adaptado, podía producir predicciones a corto plazo competitivas con los modelos ARFIMA estimados. Las predicciones multi-paso de los modelos de memoria larga fueron desarrollados por Hurvich (2002) \cite{Hurvich2002167} y comparados por Bhansali y Kokoszka (2002) \cite{Bhansali2002181}.

Se han explorado muchas extensiones de modelos ARFIMA y comparaciones de su rendimiento de predicción relativo. Por ejemplo, Franses y Ooms (1997) \cite{Franses1997117} propuso el modelo ARFIMA(0,d,0) periódico donde d puede variar con el parámetro de estacionalidad. Ravishanker y Ray (2002) \cite{Ravishanker2002207} consideraron la estimación y predicción de modelos ARFIMA multivariados. Baillie y Chung (2002) \cite{Baillie2002215} disertaron sobre el uso de modelos ARFIMA con tendencia estacionaria lineales, mientras que Beran, Feng, Ghosh y Sibbertsen (2002) \cite{Beran2002227} ampliaron este modelo para permitir tendencias no lineales. Souza y Smith (2002) investigaron el efecto de tasas de muestreo diferentes, tales como mensual versus trimestral, en estimaciones del parámetro de memoria larga d. De forma similar, Souza y Smith (2004) observaron los efectos de la agregación temporal en estimaciones y predicciones de procesos ARFIMA. Dentro del contexto de la calidad de control estadística, Ramjee, Crato y Ray (2002)\cite{Ramjee2002291} introdujeron un gráfico de control basado en preducción con una media móvil ponderada hiperbólicamente, diseñada específicamente para modelos ARFIMA no estacionarios.


\subsection{Modelos ARCH/GARCH}
Una característica clave de las series temporales financieras es que las rentabilidades grandes tienden a continuar rentabilidades grandes y lo mismo con rentabilidades pequeñas, es decir, hay periodos que muestran una alta (o baja) volatilidad. Este fenómeno se conoce como el agrupamiento de volatilidad en econometría y finanzas. La clase de modelos AR condicionados a heteroscedasticidad (ARCH), introducidas por Engle (1982) \cite{Engle1982987}, describen los cambios dinámicos de la varianza condicionada como una función determinista (típicamente cuadrática) de rentabilidades pasadas. Puesto que se conoce la varianza en tiempo -1, las predicciones a un paso están disponibles. A continuación, las predicciones multipaso pueden ser computadas de forma recursiva. Un modelo más parsimonioso que ARCH es el llamado modelo ARCH generalizado (GARCH) (Bollerslev et al., 1994\cite{Bollerslev19942959} y Taylor, 1987 \cite{Taylor1987159}) donde se permiten dependencias adicionales de los retardos de la varianza condicional. Un modelo GARCH tiene una representación tipo ARMA, de forma que los modelos comparten muchas propiedades.

La familia de modelos GARCH, y muchas de sus extensiones, son ampliamente revisadas en Bollerslev, Chou y Kroner (1992)  \cite{Bollerslev19925}, Bera y Higgins (1993) \cite{Bera1993305} y Diebold y Lopez (1995) \cite{Diebold1995253}. No es sorprendente que muchos de los trabajos teóricos hayan aparecido en literatura de econometría. Por otra parte, es interesante el hecho de que ni la IJF ni la JoF hayan llegado a ser foros para publicaciones sobre el rendimiento relativo de predicción de los modelos tipo GARCH o de otros varios modelos de volatilidad en general. Como puede verse a continuación, muy pocos trabajos d IJF/JoF han tratado este tema.

Sabbatini y Linton (1998) mostraron que el modelo simple lineal GARCH(1,1) proporciona una buena parametrización para las rentabilidades diarias en el índice de mercado suizo. Sin embargo, la calidad de las predicciones fuera de muestra sugiere que este resultado hay que tomarlo con precaución. Franses y Ghijsels (1999) \cite{Franses19991} remarcaron que esta característica puede deberse al despreciar valores atípicos aditivos (AO). Se dieron cuenta de que los modelos GARCH para las rentabilidades corregidas resultaban en unas predicciones mejoradas de la volatilidad del mercado de valores. Brooks (1998) \cite{Brooks199859} no ve un ganador claro al comparar predicciones de un paso a partir de modelos tipo GARCH estándar (simétricos) con aquellas de otros modelos lineales y ANNs. A nivel de estimación, Brooks, Burke, y Persand (2001) \cite{Brooks200145}, argumentan que los paquetes de software econométrico pueden producir resultados ampliamente variados. Claramente, esto puede tener algún impacto en la precisión de predicción de los modelos GARCH. Esta observación es muy al estilo de la de Newbold et al. (1994) \cite{Newbold1994573} para modelos ARMA univariados. Fuera de la IJF, Karanasos (2001) \cite{Karanasos2001555} consideró los efectos en media de las predicciones multipaso de modelos ARMA con GARCH. Su método puede emplearse en la derivación de predicciones multi-paso a partir de modelos más complicados, incluyendo GARCH multivariado.

Usando dos series de tasas de cambio diarias, Galbraith y Kisinbay (2005) comparó las funciones de contenido de predicción a partir del modelo GARCH estandar y del modelo GARCH fracionariamente integrado (FIGARCH) (Baillie, Bollerslev, y Mikkelsen, 1996) \cite{Baillie19963}. Las predicciones de varianzas condicionales parecen tener un contenido de información de aproximadamente 30 días de mercado. Otra conclusión es que las predicciones por proyección autorregresiva sobre volatilidades pasadas da mejores resultados que las predicciones basadas en GARCH, estimadas por probabilidad cuasi máxima y los modelos FIGARCH. Esto parece confirmar los resultados anteriores de Bollerslev y Wright (2001), por ejemplo. Una crítica que se oye a menudo sobre los modelos FIGARCH y sus generalizaciones es que no razones económicas por las que la predicción financiera de volatilidad tenga memoria larga. Para una crítica más fundamentada del uso de modelos de memoria larga véase Granger (2002) \cite{Granger2002}.

Empíricamente, las rentabilidades y la varianza condicional de las rentabilidades del siguiente periodo están correladas negativamente. Este fenómeno se denomina volatilidad asimétrica en la literatura (véase Engle y Ng (1993) \cite{Engle19931749}). Esto motivó a los investigadores para que desarrollaran varios modelos tipo GARCH asimétricos (incluyendo el GARCH con cambio de régimen); véase Hentschel (1995) \cite{Hentschel199571} y Pagan (1996) \cite{Pagan199615} para resúmenes generales. Awartani y Corradi (2005) investigaron el impacto de la asimetría en la capacidad de predicción fuera de muestra de diferentes modelos GARCH, con varios horizontes.

Paralelamente a GARCH, muchos otros modelos se han propuesto para predicción de volatilidad. Poon y Granger (2003) \cite{Poon2003478}, proporcionan una revisión cuidadosamente realizada de la investigación en esta área durante los últimos 20 años. Compararon los hallazgos de predicción de volatilidad de 93 publicaciones. Dan importantes ideas en cuestiones como evaluación de predicción, el efecto de la frecuencia de los datos en la precisión de la predicción de volatilidad, medidas de \emph{volatilidad actual}, el efecto de confusión de valores extremos, y otros más. El estudio encontró que la volatilidad implícita proporciona mejores predicciones que los modelos de series temporales. Entre los modelos de series temporales (44 estudios) no había ganador claro entre los modelos de histórico de volatilidad (incluyendo camino aleatorio, medias históricas, ARFIMA, y varias formas de alisado exponencial) y los modelos tipo GARCH (incluyendo ARCH y sus varias ampliaciones), pero ambas clases de modelos mejoraban el rendimiento del modelo de volatilidad estadístico; véase Poon y Granger (2005) para una revisión de estos hallazgos.

El estudio de Poon y Granger contiene muchas cuestiones para continuar estudiando. Por ejemplo, los modelos asimétricos GARCH salían bien parados en los concursos de predicción. Sin embargo, no está claro hasta qué punto esto se debe a las asimetrías de la media condicional, de la varianza condicional, y/o de momentos condicionales de  mayor orden. Otra cuestión para investigación futura concierne la combinación de predicciones. Los resultados en dos trabajos (Doidge y Wei, 1998 \cite{Doidge199828} y Kroner et al., 1995 \cite{Kroner199577}) encuentran de ayuda la combinación, pero en otro estudio (Vasilellis y Meade, 1996) \cite{Vasilellis1996125} no. Sería también útil examinar el rendimiento de predicción de volatilidad de modelos tipo GARCH multivariados y no lineales, incorporando a ambos dependencias contemporáneas y temporales.

\subsection{Predicción de datos de recuento}
Los datos de recuento son frecuentes en la industria y los negocios, especialmente en datos de inventario donde son llamados \emph{datos de demanda intermitente}. Consecuentemente, es sorprendente que haya tan poco trabajo realizado en predicción de estos datos. Hay algo sobre métodos a medida para predicción de datos de recuento, pero pocos artículos aparecen sobre predicción de series temporales de datos de recuento que usen modelos estadísticos.

La mayoría de la investigación en predicción de recuentos está basada en Croston (1972) \cite{Croston1972289} quien propuso usar SES para predecir independientemente los valores no cero de las series y los intervalos de tiempo entre ellos. Willemain, Smart, Shockor y DeSautels (1994) \cite{Willemain1994529} compararon el método de Croston con SES y encontraron que el método de Croston era más robusto, aunque estos resultados se basaban en en MAPEs que a menudo no están definidos para datos de recuento. Las condiciones bajo las cuales el método de Croston mejora al SES fueron discutidas en Johnston y Boylan (1996) \cite{Johnston1996297}. Willemain, Smart y Schwarz (2004) propusieron un procedimiento de remuestreo para datos de demanda intermitente que se encontró que era más preciso que el de SES y el de Croston para las nueve series evaluadas.

La evaluación de predicciones de recuento conlleva dificultades debidas a la presencia de ceros en los datos observados. Syntetos y Boylan (2005) \cite{Syntetos2005303} propusieron usar el error absoluto medio relativo,mientras que Willemain et al. (2004) \cite{Willemain2004375} recomendaron usar el método de transformación integral de probabilidad de Diebold, Gunther y Tay (1998) \cite{Diebold1998863}.

Grunwald, Hyndman, Tedesco y Tweedie (2000) revisaron muchos modelos estadísticos para series temporales de recuento, usando AR de primer orden como marco unificador para los diferentes enfoques. Un posible modelo, explorado por Brännäs (1995) \cite{Brannas200219}, supone que la serie sigue una distribución de Poisson con media que depende de procesos no observados autocorrelados. Un modelo alternativo MA de valor entero fue usado por Brännäs, Hellström y Nordström (2002) para predecir niveles de ocupación de hoteles suizos.

La distribución de predicción se puede obtener por simulación usando cualquiera de estos modelos estadísticos, pero cómo resumir la distribución no es algo obvio. Freeland y McCabe (2004) propusieron usar la mediana de la distribución de predicción, y dieron un método para computar intervalos de confianza para la distribución completa en el caso de modelos autorregresivos de valores enteros (INAR) de orden 1. McCabe y Martin (2005) ampliaron estas ideas presentando metodologías bayesianas para predicción a partir de la clase de modelos INAR.

\subsection{Evaluación de predicción y medidas de precisión}
Se ha usado un desconcertante conjunto de medidas de precisión para evaluar el rendimiento de los métodos de predicción. Algunos de los cuales se listan en Mahmoud (1984) \cite{Mahmoud1984139}. Primero definimos las medidas más comunes. 

(poner tabla)


\begin{table}
\caption{Medidas normalmente usadas para precisión de predicción}
\begin{center}
%\resizebox{12cm}{!}{
\begin{tabular}{|C{2cm}|L{4cm}|C{5cm}|}\hline %
\bfseries Medida & \bfseries Descripción & \bfseries Expresión
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{medidasError.csv}{}%
{\medida & \descripcion & \expresion }% 

\end{tabular}
%}
\end{center}
\end{table}
La evolución de las medidas de precisión y evaluación de predicción puede verse a través de las medidas usadas para evaluar los métodos en la mayoría de los estudios comparativos realizados.En la competición original M (Makridakis et al., 1982) \cite{Makridakis1982111} las medidas usadas incluían MAPE, MSE, AR, MdAPE y PB. Sin embargo, como apuntaron Chatfield (1988) \cite{Chatfield198819} y Armstrong y Collopy (1992) \cite{Armstrong199269}, el MSE no es apropiado para comparaciones entre series puesto que depende de la escala. MAPE también tiene problemas cuando la serie tiene valores cercanos a cero o cero, como apuntaron Makridakis, Wheelwright y Hyndman (1998, p.45) \cite{Makridakis1983}. En la competición M se evitaron MAPEs excesivamente grandes (o infinitos) incluyendo sólo datos positivos. Sin embargo, es una solución artificial imposible de aplicar en todas las situaciones.

en 1992, aparecieron dos artículos de IJF y varios comentarios sobre las medidas de evaluación. Armstrong y Collopy (1992) \cite{Armstrong199269} recomendaron usar errores absolutos relativos, especialmente GMRAE y MdRAE, a pesar del hecho de que los errores relativos tienen varianza infinita y media no definida. Ellos recomendaron \emph{winsorizar} para recortar valores extremos lo que parcialmente resolvía estos problemas, pero lo cual añade complejidad al cálculo y un nivel de arbitrariedad al tener que especificar la cantidad de recorte. Fildes (1992) \cite{Fildes199281} también prefirió GMRAE aunque lo expresó en una forma equivalente como la raíz cuadrada de la media geométrica de los errores relativos cuadráticos. Esta equivalencia no parece ser advertida por ninguno de los ponentes en los comentarios de Ahlburg et al. (1992) \cite{Chatfield1992100}. 

El estudio de Fildes, Hibon, Makridakis y Meade (1998), el cual se enfocaba en predicción de datos de telecomunicaciones, usó MAPE, MdAPE, PB, AR, GMRAE y MdRAE teniendo en cuenta algunas de las críticas de los métodos usados para la competición M.

La competición M3 (Makridakis y Hibon, 2000) \cite{Makridakis2000451} usó tres medidas de precisión: MdRAE, sMAPE y sMdAPE. Las medidas \emph{simétricas} fueron propuestas por Makridakis (1993) \cite{Makridakis1993527} en respuesta a la observación de que MAPE y MdAPE tienen la desventaja de que penalizan más errores positivos que negativos. Sin embargo, estas medidas no son tan simétricas como su nombre sugiere. Para el mismo valor de Yt, el valor 2*Abs(Yt-Ft)/(Yt+Ft) tiene más penalización cuando las predicciones son altas en comparación con cuando son bajas. Véase Goodwin y Lawton (1999) \cite{Goodwin1999405} y Koehler (2001) \cite{Koehler2001269} para más disertación sobre este punto.

Es notable que ninguno de los estudios comparativos ha usado medidas relativas (a diferencia de medidas de errores relativos) tales como RelMAE o LMR. Esta última fue propuesta por Thompson (1990) \cite{Thompson1990219}quien abogó por su uso basado en sus buenas propiedades estadísticas. Fue aplicada a los datos de la competición M en Thompson (1991) \cite{Thompson1991331}.

A parte de Thompson (1990) \cite{Thompson1990219}, ha habido poco trabajo teórico en las propiedades estadísticas de estas medidas. Una excepción es Wun y Pearn (1991) quienes estudiaron las propiedades estadísticas de MAE.

Una novedosa medida alternativa de precisión es la \emph{distancia temporal}, que fue considerada por Granger y Jeon, 2003a \cite{Granger2003199} y Granger y Jeon 2003b \cite{Granger2003339}. En esta medida se capturan también  las propiedades de dirección y desfase de la predicción. De nuevo, esta medida no ha sido usada en ningún estudio comparativo importante.

Una línea de investigación paralela ha examinado tests estadísticos para comparar métodos de predicción. Una contribución precoz fue Flores (1989) \cite{Flores1989529}. El mejor enfoque conocido para comprobar diferencias entre la precisión de los métodos de predicción es el test de Diebold y Mariano (1995) \cite{Diebold1995253}. Harvey, Leybourne y Newbold (1997) \cite{Harvey1997281} propusieron una modificación de este test con tamaño corregido. McCracken (2004) \cite{McCracken2004503} examinó el efecto de estimación de parámetros en estos tests y proporcionó un nuevo método de ajuste para error de estimación de parámetros.

Otro problema en evaluación de predicción, y más serio que el error de estimación de parámetros, es el \emph{intercambio de datos}, el uso de los mismo datos para diferentes métodos de predicción. Sullivan, Timmermann y White (2003) \cite{Sullivan2003217} propusieron un procedimiento de remuestreo diseñado para superar la distorsión resultante de inferencia estadística.

Una línea de investigación independiente ha examinado las propiedades teóricas de predicción de modelos de series temporales. Una contribución importante fue Clements y Hendry (1993) \cite{Clements1993617} quienes mostraron que el MSE teórico de un modelo de predicción no era invariante a transformaciones lineales con preservación de escala tales como diferenciación de los datos. En su lugar, propusieron el criterio de \emph{momento segundo de error de predicción generalizado} (GFESM), el cual no tiene esa propiedad no deseable. Sin embargo, es difícil aplicar tales medidas empíricamente y la idea parece no haberse extendido ampliamente.

\subsection{Combinación}
Durante las últimas 3 décadas se ha estudiado la combinación, mezcla y puesta en común de predicciones obtenidas a partir de diferentes métodos de series temporales y diferentes fuentes de información. Contribuciones prematuras en este área son Bates y Granger (1969) \cite{BATESJM1969451}, Newbold y Granger (1974) \cite{Newbold1974131} y Winkler y Makridakis (1983) \cite{Winkler1983150}. En una revisión bibliográfica exhaustiva, Clemen (1989) \cite{Clemen1989559} resumió una evidencia irresistible de la eficiencia relativa de predicciones combinadas, normalmente en términos de varianzas de error de predicción.

Se han propuesto numerosos métodos para seleccionar los pesos de combinación. La simple media es el método de combinación más ampliamente usado (véase la revisión de Clemen y Bunn, 1985 \cite{Bunn1985151}), pero el método no usa información pasada respecto a la precisión de las predicciones o la dependencia entre las predicciones. Otro simple método es la mezcla lineal de las predicciones individuales con pesos de combinación determinados por OLS (asumiendo insesgamiento) a partir de la matriz de predicciones pasadas y el vector de observaciones pasadas (Granger y Ramanathan, 1984) \cite{Granger1984197}. Sin embargo, las estimaciones OLS de los pesos son insuficientes debido a la posible presencia de correlación en serie en los errores de predicción combinados. Aksu y Gunter (1992) \cite{Aksu199227} y Gunter (1992) \cite{Gunter199245} investigaron este problema con algo más de detalle. Ellos recomendaron el uso de predicciones combinadas OLS con la restricción de que los pesos sumen uno. Granger (1989) \cite{Granger1989167} dio varias ampliaciones de la idea original de Bates y Granger (1969) \cite{BATESJM1969451} incluyendo combinación de predicciones con horizontes mayores que un período.

Mejor que usar pesos fijos, Deutsch, Granger y Teräsvirta (1994) \cite{Deutsch199447} permitieron que estos cambiaran a lo largo del tiempo usando modelos STAR y de cambio de régimen. Otro esquema de ponderación variante en el tiempo fue propuesto por Fiordaliso (1998) \cite{Fiordaliso1998367}, quien usó un sistema difuso para combinar un conjunto de predicciones individuales de forma no lineal. Diebold y Pauly (1990) \cite{Diebold1990503} usaron técnicas de contracción bayesiana para permitir la incorporación a priori de información en la estimación de los pesos de combinación. Zou y Yang (2004) \cite{Zou200469} consideraron la combinación de predicciones de modelos muy similares con actualización secuencial de pesos.

La combinación de pesos determinada a partir de los métodos invariantes en tiempo puede llevar a predicciones relativamente pobres si ocurre no estacionariedad entre las diferentes componentes de predicción. Miller, Clemen y Winkler (1992) \cite{Miller1992515} examinaron el efecto del desplazamiento de ubicación de la no estacionariedad en un conjunto de  métodos de combinación. Concluyeron que la simple media batía otros artefactos de combinación complejos, véase también Hendry y Clements (2002) \cite{Hendry20021} para resultados más recientes. El tema relacionado de combinar predicciones a partir de modelos lineales y otros no lineales, con pesos OLS  así como determinados por un método variante en el tiempo fue abordado por Terui y van Dijk (2002)\cite{Terui2002421}.

La forma de la distribución del error de predicciones combinadas y el correspondiente comportamiento estadístico fue estudiado por de Menezes y Bunn (1998) \cite{DeMenezes1998415} y Taylor y Bunn (1999) \cite{Taylor1999325}. Para distribuciones de error de predicción  no normales, la oblicuidad surge como un criterio relevante para especificar el método de combinación. Fang (2003) \cite{Fang200387} dio algunas ideas de por qué las predicciones en competición pueden ser fructuosamente combinadas para producir una predicción superior a las predicciones individuales abarcando tests de predicción. Hibon y Evgeniou (2005) \cite{Hibon200515} propusieron un criterio para seleccionar entre predicciones y sus combinaciones.

\subsection{Intervalos de predicción y densidades}
El uso de intervalos de predicción, y más recientemente densidades de predicción, ha ido siendo más usual a lo largo los últimos 25 años conforme los profesionales han sido más conscientes de las limitaciones de las predicciones puntuales. Una revisión importante y completa de los intervalos de predicción se da en Chatfield (1993) \cite{Chatfield1993121}, resumiendo la literatura hasta el momento.

Desafortunadamente, hay algo de confusión en la terminología y algunos autores usan \emph{intervalo de confianza} en lugar de \emph{ intervalo de predicción}. El intervalo de confianza se usa para un parámetro del modelo, mientras que el intervalo de predicción se usa para una variable aleatoria. Casi siempre, los pronosticadores, van a querer intervalos de predicción, intervalos que contengan los valores verdaderos de observaciones futuras con su probabilidad específica.

La mayoría de los intervalos de predicción están basados en un modelo estadístico subyacente. Por tanto, ha habido bastantes trabajos para formular modelos estadísticos apropiados para generalizar procedimientos de predicción comunes.

El vínculo entre las fórmulas de intervalos de predicción y el modelo a partir del que se derivan no siempre se ha examinado correctamente. Por ejemplo, el intervalo de predicción apropiado para un modelo de camino aleatorio fue aplicado por Makridakis y Hibon (1987) \cite{Makridakis1987489} y Lefrançois (1989) \cite{Lefrancois1989553} para predicciones obtenidas a partir de muchos otros métodos. Este problema fue advertido por Koehler (1990) \cite{Koehler1990557} y Chatfield y Koehler (1991) \cite{Chatfield1991239}.

La incertidumbre asociada a la selección del modelo y la estimación de parámetros no se tiene en cuenta con la mayoría de intervalos de predicción basados en modelo. Por tanto, los intervalos son demasiado estrechos. Ha habido investigación considerable en cómo hacer que los intervalos de predicción basados en modelo tengan una cobertura más realista. Una serie de artículos apareció sobre el uso de remuestreo para crear intervalos de predicción basados en modelo para el modelo AR, empezando por Masarotto (1990) \cite{Masarotto1990229} e incluyendo a McCullough, 1994 \cite{McCullough199451}, McCullough, 1996 \cite{Mccullough1996293}, Grigoletto (1998) \cite{Grigoletto1998447}, Clements y Taylor (2001) \cite{Clements2001247} y Kim (2004b) \cite{Kim200485}. Procedimientos similares para otros modelos también se han considerado incluyendo modelos ARIMA (Pascual et al., 2001\cite{Pascual200183}, Pascual et al., 2004 \cite{Pascual2004449}, Pascual et al., 2005 \cite{Pascual2005219} y Wall y Stoffer, 2002\cite{Wall2002733}), VAR (Kim, 1999 \cite{Kim1999393} y Kim, 2004a \cite{Kim200485}), ARCH (Reeves, 2005 \cite{Reeves2005237}) y regresión (Lam \& Veal, 2002 \cite{Lam2002125}). Parece probable que estos métodos de remuestreo se utilicen más conforme las velocidades de cómputo aumente por sus mejores propiedades de cobertura.

Cuando el error de predicción no sigue una distribución normal, hallar la densidad de predicción completa es útil puesto que un intervalo único puede no proporcionar un resumen adecuado del futuro esperado. Un resumen sobre densidad de predicción se da en Tay y Wallis (2000) \cite{Tay2000235} junto con varios otros artículos del mismo número especial de la JoF. Resumiendo, una densidad de predicción ha sido objeto de interesantes propuestas incluyendo los \emph{gráficos de ventilador}  (Wallis, 1999) \cite{Wallis1999106} y las \emph{regiones de mayor densidad} (Hyndman, 1995 \cite{Hyndman1995431}). El uso de estos resúmenes gráficos ha crecido rápidamente en los años recientes y las densidades de predicción se utilizan de forma relativamente amplia.

Conforme los intervalos de predicción y las densidades van siendo más usadas, la atención se centra en su evaluación y testeo. Diebold, Gunther y Tay (1998) \cite{Diebold1998863} introdujeron el método notablemente sencillo de \emph{transformación integral de probabilidad}, el cual puede usarse para evaluar una densidad univariada. Este enfoque ha llegado a usarse ampliamente en un periodo de tiempo muy corto y ha sido un avance clave en este área. La idea se extiende para densidades de predicción multivariada en Diebold, Hahn y Tay (1999) \cite{Diebold1999661}.

Otros enfoques para evaluar la densidad y los intervalos de predicción se da en Wallis (2003) \cite{Wallis2003165} quien propuso tests chi cuadrado para ambos intervalos y densidades, y Clements y Smith (2002) quienes disertaron sobre algunos tests sencillos pero potentes para evaluar densidades de predicción multivariadas.

\section{Una mirada al futuro}
En las secciones previas, se ha examinado la historia de las series temporales de la IJF, con la esperanza de arrojar luz al presente. De cara al futuro, es interesante reflejar las propuestas de investigación identificadas. 

Chatfield (1988) \cite{Chatfield198819} remarcó la necesidad de investigar métodos multivariados enfatizando en hacerlos más prácticos. Ord (1988) \cite{Ord1988389} también advirtió que no se había realizado suficiente trabajo en modelos de múltiples series temporales, incluyendo alisado exponencial multivariado. Hasta la fecha la predicción de series temporales multivariadas todavía no es aplicada ampliamente, a pesar  de los considerables avances en este área. Se sospecha que existen dos razones para ello: una falta de investigación empírica sobre algoritmos de predicción robustos para modelos multivariados, y la escasez de software fácil de usar. Algunos de los métodos sugeridos (modelos VARIMA) son difíciles de estimar por el gran número de parámetros que incluyen. Otros como el alisado exponencial multivariado, no han adquirido suficiente atención teórica para su aplicación rutinaria. Un enfoque para predicción de series temporales multivariadas consiste en usar modelos de factor dinámico. Estos modelos promete tanto teóricamente (Forni et al., 2005 \cite{Forni2005830} y Stock y Watson, 2002 \cite{Stock20021167}) como para su aplicación (Peña y Poncela, 2004 \cite{Pena2004291}) y se cree que llegarán a usarse más ampliamente en el futuro.

Ord (1988) \cite{Ord1988389} también indicó la necesidad de investigar más profundamente en los métodos de predicción basados en modelos no lineales. Si bien muchos aspectos de los modelos no lineales se han investigado en la IJF, se debe seguir investigando de forma continuada. Por ejemplo, no existe un claro consenso de que las predicciones de los modelos no lineales superen categóricamente las de los modelos lineales (véase Stock y Watson, 1999 \cite{Stock19991}). 

Otros temas que sugiere Ord (1988) \cite{Ord1988389} incluye la necesidad de desarrollar procedimientos de selección de modelo que hagan un uso efectivo tanto de los datos como del conocimiento a priori, a la necesidad de especificar objetivos para las predicciones y desarrollar sistemas de predicción dirigidos hacia estos objetivos. Estas áreas todavía necesitan atención y se espera que aparezcan herramientas para solventar estos problemas.

Dado el frecuente mal uso de métodos basados en modelos lineales con distribuciones de error gaussianas independientes e idénticas, Cogger (1988) \cite{Cogger1988403} abogó por que los nuevos desarrollos del área de métodos estadísticos \emph{robustos} recibieran más atención dentro de la comunidad de la predicción de series temporales. Un procedimiento robusto se espera que funcione bien cuando existan valores atípicos o desplazamientos de ubicación en los datos que sean difíciles de detectar. La estadística robusta puede estar basada tanto en métodos paramétricos como no paramétricos. Un ejemplo de este último, es el concepto de cuantiles de regresión de Kkoenker y Bassett (1978) \cite{Koenker197833} investigado por Cogger. En predicción, estos puede aplicarse como cuantiles condicionados univariados y multivariados. Un área de aplicación importante es al estimar herramientas de manejo de riesgo como el \emph{valor en riesgo}. Recientemente, Engle y Manganelli (2004) \cite{Engle2004367} realizaron un comienzo en esta dirección, proponiendo un modelo de valor en riesgo condicional. Se espera que haya muchas más investigación en este área.

Un tema relacionado en donde ha habido bastante actividad investigadora es la densidad de predicción, donde el foco está en la densidad de probabilidad de observaciones futuras más allá de la media o la varianza. Por ejemplo, Yao y Tong (1995) \cite{Yao1995395} propusieron el concepto de intervalo de predicción de percentil condicional. Su anchura ya no es una constante, como en el caso de los modelos lineales, y puede variar respecto a la posición del espacio de estado a partir del que se realizan las predicciones; véase también De Gooijer y Gannoun (2000) \cite{DeGooijer2004237} y Polonik y Yao (2000) \cite{Polonik2000509}.

Claramente, el área de los intervalos de predicción mejorada requiere mayor investigación. Esto está en concordancia con Armstrong (2001) \cite{Armstrong2001} quen listó 23 principios de gran necesidad de interés incluyendo el elemento 14:13: \emph{Para intervalos de predicción, incorporar la incertidumbre asociada con la predicción de las variables explicativas}.


En los años recientes, las series temporales no gaussianas, han comenzado a recibir una atención considerable y los métodos de predicción van lentamente siendo desarrollados. En particular las series no gaussiana con valores positivos tiene importantes aplicaciones. Dos áreas importantes son la volatilidad y la duración entre transacciones. Algunas contribuciones importantes hasta la fecha han sido el modelo de \emph{duración condicional autorregresivo} de Engle y Russell (1998) \cite{Engle19981127} y Andersen, Bollerslev, Diebold y Labys (2003) \cite{Andersen2003579}. Dada la importancia de estas aplicaciones, se espera mucho más trabajo en este área.

Si bien, las series temporales no gaussianas con espacio de muestreo continuo han comenzado a recibir mayor atención de investigación, especialmente en el contexto de las  finanzas, la predicción de series temporales con un espacio muestral discreto (tales como las series de recuentos) todavía está en su infancia. Tales datos son muy frecuentes en la industria y los negocios y existen muchos problemas asociados a la predicción de recuentos sin resolver teóricamente o de forma práctica; por tanto, se espera una mayor investigación productiva en este área en el futuro cercano.

Otros autores han intentado identificar temas de investigación importantes. Tanto De Gooijer (1990) \cite{deGooijer1990449} como Clements (2003) \cite{Clements20031} en dos editoriales, y Ord como parte de un artículo de disertación de Dawes, Fildes, Lawrence y Ord (1994) \cite{Dawes1994151} sugirieron más trabajo en predicciones combinadas. Aunque el tema ha recibido una cantidad de atención considerable, todavía hay algunas cuestiones abiertas. Por ejemplo, cuál es el mejor método de combinación para modelos no lineales y lineales y qué intervalo de predicción puede establecerse en torno a la predicción combinada. Un buen punto de partida para mayor investigación en este área es Teräsvirta (2006) \cite{Terasvirta2006}; véase también Armstrong (2001, 12.5-12.7) \cite{Armstrong2001}. Recientemente, Stock y Watson (2004) \cite{Stock2004405} discutieron el \emph{puzle de combinación de predicción}, a saber, el hallazgo empírico repetido de que combinaciones simples como las medias superan combinaciones más sofisticadas en las que la teoría sugiere mejor funcionamiento. Esta importante cuestión práctica sin duda recibirá mayor atención de investigación.

Los cambios en el almacenamiento y recolección de datos también conducirá a nuevas direcciones de investigación. Por ejemplo, en el pasado, los paneles de datos disponibles (llamados datos longitudinales en bioestadística) tenían la dimensión temporal de la serie \emph{t} pequeña mientras que la dimensión de sección cruzada \emph{n} era grande. Sin embargo, ahora en muchas áreas aplicadas como marketing, se pueden obtener fácilmente grandes conjuntos de datos con ambos \emph{n} y \emph{t} grandes. La extracción de características a partir de lo paneles de datos es el objeto del \emph{análisis de datos funcionales}; véase por ejemplo  Ramsay y Silverman (1997) \cite{Ramsay1997}. Sin embargo, el problema de realizar predicciones multi-paso basadas en datos funcionales sigue abierto tanto a investigación teórica como práctica. Dada el predominio creciente de este tipo de datos, se espera que este área sea fructífera en el futuro.

Los conjuntos de datos grandes, también se prestan a métodos de computación intensiva. Mientras las redes neuronales se han usado en predicción durante más de dos décadas, existen muchas cuestiones excepcionales asociadas con su uso e implementación, incluyendo el cuándo van a mejorar probablemente otros métodos. Otros métodos que implican computación pesada (como bagging y boosting) son incluso menos entendidos en el contexto de predicción. Con la disponibilidad de conjuntos de datos muy grandes y potencia alta de computación se espera que ésta sea un área de investigación importante.


\bibliographystyle{splncs}

\bibliography{bibliography}


\end{document}

%%Está chulo el trabajo, el STATE OF ART es de premio :)
