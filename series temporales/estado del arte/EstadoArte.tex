\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage[breaklinks]{hyperref}
\usepackage{breakcites}
\usepackage{csvsimple}
\usepackage[spanish,es-noshorthands,es-ucroman,es-tabla]{babel}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{amsmath}
\usepackage{ifthen}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Series temporales, aprendizaje automático y tráfico de vehículos}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{J.J. Asensio, P.A. Castillo etc}
\authorrunning{J.J. Asensio et al.}

\institute{
Departamento de Arquitectura y Tecnología de Computadores \\
ETSIIT, CITIC-UGR \\
Universidad de Granada, España \\
\email{\{asensio, pacv\}@ugr.es}
}

\maketitle
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 
En este trabajo se presenta una breve revisión histórica de las técnicas convencionales de predicción de series temporales, el estado del arte actualmente dominado por los métodos de aprendizaje automático y algunos ejemplos de aplicación en el ámbito de predicción de tráfico de vehículos. 
\end{abstract}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introducción}
\label{sec:intro}

Una serie temporal es una secuencia de datos puntuales observados a intervalos sucesivos de tiempo equidistantes. Este tipo de información a menudo se encuentra disponible en una gran amplitud de contextos como por ejemplo en economía y finanzas, datos atmosféricos, geología, sociología, etc. 

Su presencia es una constante en prácticamente cualquier rama de la ciencia debido a la frecuencia con que los datos pueden ser considerados en forma de serie temporal. Las series temporales se estudian en estadística, procesamiento de señales, econometría y muchas otras áreas. Su análisis es habitualmente empleado para predicción. 

La investigación en el área de predicción ha ido descubriendo y consolidando numerosas herramientas valiosas desde su inicio. Sin embargo, otras técnicas están abriendo camino a nuevos enfoques relativamente recientes, como es el campo del aprendizaje automático.  

En este trabajo se resume a modo introductorio el trabajo de \cite{DeGooijer2006443} sobre la evolución histórica de las técnicas convencionales para predicción. A continuación se describe un subconjunto de métodos de aprendizaje automático relativamente representativo del estado del arte y finalmente se enumeran algunos trabajos de aplicación en el tratamiento de datos de tráfico de vehículos.

El objetivo de este documento es ofrecer una visión generalista tanto de la investigación acumulada como de su aplicación a la predicción de tráfico que se está realizando actualmente.

\section{Métodos convencionales}

En esta sección se resumen algunos trabajos relevantes en el área de predicción de series temporales. Los métodos que se comentan a continuación se clasifican según los modelos utilizados. 

\subsection{Alisado exponencial}

Los métodos de alisado exponencial habitualmente se utilizan en la estimación de la demanda de un producto para un periodo dado. Una forma sencilla de realizar la predicción consiste en tomar la media ponderada de los valores previos dando mayor peso a los valores más cercanos en el tiempo. El error en la predicción cometido puede ser tenido en cuenta para la siguiente predicción.

Estos métodos tienen su origen en las décadas 50 y 60 con el trabajo de \cite{Brown1959}, \cite{Holt20045} y \cite{Winters1960324}. \cite{Pegels1969311} proporcionó una clasificación sencilla pero útil de la tendencia y los patrones estacionales dependiendo de si eran aditivos (lineales)  o multiplicativos (no lineales).

Los métodos de alisado exponencial fueron ampliamente utilizados en la industria y los negocios, sin embargo no habían recibido mucha atención por parte de los estadísticos y no tenían un fundamento estadístico bien desarrollado. 

A partir de \cite{Muth1960299} surge la primera base estadística para el alisado exponencial simple (SES) demostrando que proporcionaba una predicción óptima para caminos aleatorios con ruido. 

Posteriormente \cite{Box1976} mostraron que algunas predicciones de alisado exponencial resultan ser casos especiales de modelos ARIMA.

\cite{Taylor2003715} proporcionó los únicos métodos de alisado exponencial multiplicativos genuinamente nuevos. Entre las áreas de aplicación se destacan varios entornos que incluyen componentes de computadores (\cite{GardnerJr1993245}), pasajeros de vuelo (\cite{Grubb200171}), y planificación de producción (\cite{Miller1993509}).

En \cite{Hyndman2002439} y en la ampliación de \cite{Taylor2003715} se da una clasificación de este tipo de métodos. Cada método se componen de una de cinco tipos de tendencia (ninguna, aditiva, aditiva alisada, multiplicativa y multiplicativa alisada) y uno de tres tipos de estacionalidad (ninguna, aditiva y multiplicativa). Por tanto, hay 15 métodos diferentes, donde entre los más conocidos está SES (sin tendencia ni estacionalidad), el método lineal de Holt (tendencia aditiva, sin estacionalidad), el método aditivo de Holt-Winters (tendencia aditiva y estacionalidad aditiva), y el método no lineal de Holt-Winters (tendencia aditiva y estacionalidad multiplicativa).


\subsection{Modelos ARIMA}

Los primeros intentos de estudiar series temporales estaban generalmente caracterizados por la idea de un mundo determinista. La mayor contribución de \cite{Yule1927267} fue la que impulsó la noción de aleatoriedad en series temporales postulando que cada serie temporal puede ser considerada como la realización de un proceso estocástico. Basado en esa sencilla idea, se desarrolló desde entonces una gran variedad de métodos de series temporales. Slutsky, Walker, Yaglom y Yule formularon el concepto de los modelos auto-regresivos (AR) y media móvil (MA). El teorema de la descomposición de Wold condujo a la formulación y solución del problema de predicción lineal de \cite{Kolmogorov19411}. Desde entonces, ha aparecido una gran cantidad de literatura tratando la estimación e identificación de parámetros, comprobación de modelos y predicción. En \cite{Newbold198323} podemos ver un estudio anterior.

La publicación \emph{Time Series Analysis: Forecasting and Control} de \cite{Box1976} reunió el conocimiento existente hasta la fecha. Además, estos autores desarrollaron una ciclo coherente, versátil e iterativo en tres etapas para la identificación, estimación y verificación de series temporales, conocido como el enfoque Box-Jenkins. El libro ha tenido un enorme impacto en la teoría y práctica moderna de análisis y predicción de series temporales. Con la llegada de los ordenadores, se popularizó el uso de los modelos ARIMA y se extendió a muchas áreas de la ciencia. Prueba de ellos son los numerosos estudios de naturaleza empírica que usan estos modelos como punto de referencia para comparar.

\begin{table}
\caption{Lista de ejemplos de aplicaciones reales}
\resizebox{12cm}{!}{
\begin{tabular}{|L{5cm}|C{3cm}|L{4cm}|c|}\hline %
\bfseries \makebox[5cm][c]{Dataset} & \bfseries Horizonte & \bfseries \makebox[4cm][c]{Benchmark} & \bfseries Referencia
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{ejemplos.csv}{}%
{\dataset & \horizon & \benchmark & \reference}% 
\end{tabular}
}
\end{table}
\subsubsection{Univariado}

El éxito de la metodología de Box-Jenkins se basa en el hecho de que los modelos pueden imitar el comportamiento de los diversos tipos de series y sin requerir muchos parámetros a estimar en el modelo finalmente elegido. Sin embargo, a mediados de los años sesenta, la selección de un modelo era en gran medida una cuestión subjetiva del investigador, no había ningún algoritmo para encontrar el modelo apropiado. Desde entonces han surgido muchas técnicas y métodos matemáticos para la selección de un modelo ARMA, como el criterio de información Akaike (AIC), el error de predicción final de Akaike (FPE) y el criterio de información bayesiano (BIC). A menudo, estos criterios se reducen a la minimización de errores de predicción a un paso (en la muestra), con un término de penalización para el sobreajuste. FPE también ha sido generalizado para predicción a varios pasos como en \cite{Bhansali1999295}, pero sin embargo no ha sido utilizada en el área aplicada. Este también parece ser el caso de criterios basados en principios de validación cruzada y validación por división muestral (véase por ejemplo, \cite{West19961084}), haciendo uso de errores de predicción fuera-de-muestra.

El problema de añadir información externa a priori en las predicciones de ARIMA univariado fue considerado por  \cite{Cholette1982375}, \cite{Guerrero1991339} y \cite{deAlba199395}).

Como alternativa a la metodología de ARIMA univariado, \cite{Parzen198267} propuso la metodología ARARMA. La idea clave es que una serie temporal es transformada de un filtro AR de memoria larga, a un filtro de memoria corta, evitando así un operador de diferenciación más duro. Además, se usa una fase de identificación diferente al convencional de Box-Jenkins. En la competición M (\cite{Makridakis1982111}), los modelos ARARMA consiguieron el MAPE más bajo para horizontes de predicción mayores. 

El modelado automático de ARIMA univariado, ha demostrado tener una capacidad de predicción a un paso tan precisa como la de otros métodos competentes (\cite{Hill1984319}, \cite{Libert1984325}, \cite{Poulos1987261} y \cite{Texter1989209}). Varios proveedores de software han implementado métodos de predicción automáticos de series temporales (incluso métodos multivariados) que a menudo actúan como caja negra (\cite{Geriner1991127}, \cite{Tashman2000437}). Algunas directrices en la elección de un método de predicción automático se da en \cite{Chatfield198819}.

En luegar de adoptar un modelo AR para cualquier horizonte de predicción, \cite{Kang2003387} investigó empíricamente el caso de seleccionar un modelo AR de forma diferenciada para cada horizonte en predicciones multi-paso.

\subsubsection{Función de transferencia}
La identificación de modelos de función de transferencia suele ser difícil cuando hay más de una variable de entrada. \cite{Edlund1984297} presentó un método en dos pasos para identificación de una función respuesta impulso cuando varias variables de entrada están correladas. \cite{Koreisha1983151} estableció varias relaciones entre funciones de transferencia, implicaciones causales y especificación del modelo econométrico. \cite{Gupta1987195} identificó los principales problemas en las comprobaciones de causalidad. Mediante análisis de componentes principales, \cite{DelMoral1997237} sugirieron una representación cuidadosa de modelo de función de transferencia. \cite{Krishnamurthi198921} mostraron cómo se pueden obtener estimaciones más precisas del impacto de las intervenciones en modelos de función de transferencia usando una variable de control.


\subsubsection{Multivariado}
El modelo de vector ARIMA (VARIMA) es una generalización multivariada del modelo univariado ARIMA. \cite{Riise1984309} mostraron como los filtros de suavizado pueden construirse dentro de los modelos VARMA. El alisado previene fluctuaciones irregulares en series temporales explicativas al migrar las predicciones de la serie dependiente. Para determinar el máximo horizonte de predicción para procesos VARMA, \cite{DeGooijer1992135} establecieron propiedades teóricas de las predicciones y del error de predicción acumulado a varios pasos. \cite{Lutkepohl1986461} estudió los efectos de la agregación temporal y el muestreo sistemático en predicción, suponiendo que la variable desagregada (estacionaria) sigue un proceso VARMA de orden desconocido. Posteriormente, \cite{Bidarkota1998457} consideró el mismo problema pero con las variables observadas integradas en lugar de estacionarias.

Los vectores de auto-regresión (VAR) constituyen un caso especial de una clase más general de modelos VARMA. En esencia, un modelo VAR es una aproximación flexible y reducida de una amplia variedad de modelos dinámicos de econometría. Los modelos VAR se pueden especificar de varias formas. \cite{Funke1990363} presentó 5 especificaciones diferentes de VAR y comparó sus rendimientos de predicción usando series mensuales de producción industrial.

En general, los modelos VAR tienden a sufrir sobreajuste con demasiados parámetros libres y no significativos. Como resultado, estos modelos pueden dar pobres predicciones fuera de muestra, incluso aunque el ajuste en las muestras sea bueno. En lugar de restringir algunos parámetros como habitualmente se hace, \cite{Litterman198625} y otros impusieron una distribución preferente en los parámetros, expresando la idea de que muchas variables económicas se comportan como un camino aleatorio. 

\subsection{Estacionalidad}
El enfoque más antiguo para controlar la estacionalidad en series temporales es extraerla usando un procedimiento iterativo de descomposición como el método X-11. Este método y sus variantes (incluyendo versiones más reciente como X-12-ARIMA, \cite{Findley1998127}), han sido estudiados de forma extensiva durante los últimos años derivando en varios enfoques.

Uno de ellos trata el uso de predicción como parte del método de descomposición estacional. \cite{Quenneville2003727} con otro enfoque observaron las predicciones implícitas mediante los filtros de medias móviles asimétricos del método x-11 y sus variantes. 

Además de trabajar en el método X-11 y sus variantes, también se han desarrollado varios métodos nuevos para el ajuste estacional, entre los que destaca el enfoque basado en modelo de TRAMO-SEATS (\cite{Gomez2001} y \cite{Kaiser2005691}) y el método no paramétrico STL (\cite{Cleveland19903}). Otra propuesta ha sido el uso de modelos sinusoidales (\cite{Simmons1990485}).

Los modelos de series temporales periódicas fueron también exploradas por \cite{Wells1997407}, \cite{Herwartz1997421} y \cite{Novales1997393}, los cuales encontraron que los modelos periódicos pueden conducir a una mejora en el rendimiento de la predicción comparados con modelos no periódicos bajo algunas condiciones. La predicción de procesos ARMA periódicos multivariados fue considerada por \cite{Ula1993645}.

Varios artículos han comparado empíricamente los modelos estacionales. \cite{Chen1997269} exploró la robustez de un modelo estructural, un modelo de regresión con estacionalidad, un modelo ARIMA, y el método de Holt-Winters, y encontró que los dos últimos daban predicciones relativamente robustas cuando el modelo no estaba completamente especificado. \cite{Noakes1985179}, \cite{Albertson1996345}, \cite{Kulendran1997319} y \cite{Franses200587} compararon el rendimiento de predicción de varios modelos estacionales aplicados a datos reales. El modelo con mejor predicción variaba a lo largo del estudio, dependiendo de qué modelo se usara y la naturaleza de los datos. Parece no haber consenso todavía en qué condiciones se puede preferir un modelo u otro.

\subsection{Espacio de estado, modelos estructurales y el filtro de Kalman}

Los estadísticos empezaron a usar los modelos de espacio de estado para predecir series temporales a principios de los 80, aunque las ideas ya estaban presentes en la literatura de ingeniería desde el innovador trabajo de \cite{Kalman196035}. Los modelos de espacio de estado proporcionan un marco unificado en el que se puede escribir cualquier modelo lineal de serie temporal. La contribución clave fue el algoritmo recursivo conocido como el filtro de Kalman para calcular las predicciones. Los estadísticos empezaron a interesarse por los modelos de espacio de estado cuando \cite{Schweppe196561} mostró que el filtro de Kalman era un método eficiente para computar los errores de predicción a un paso y su varianza asociada necesaria para producir la función de probabilidad. \cite{Shumway1982253} combinaron el algoritmo EM con el filtro de Kalman para dar un enfoque general para predecir series temporales usando modelos de espacio de estado, incluyendo la posibilidad de tener observaciones inexistentes.

Una clase particular de modelos de espacio de estado, conocidos como \emph{modelos lineales dinámicos} (DLM) fue introducida por \cite{Harrison1976205}, quienes también propusieron un enfoque bayesiano para la estimación. \cite{Fildes1983137} comparó las predicciones obtenidas usando el método de Harrison y Stevens con métodos más sencillos como el alisado exponencial concluyendo que la complejidad adicional no conducía a una mejora del rendimiento de predicción. \cite{Harvey1989} amplió esta clase de modelos y siguió un enfoque no bayesiano para la estimación En \cite{Harvey2006327} se proporciona una revisón y una introducción para esta clase de modelos incluyendo variaciones no gausianas y en tiempo continuo. 

Estos modelos poseen muchas similaridades con los métodos de alisado exponencial, pero tienen múltiples fuentes de error aleatorio. En particular, el \emph{modelo estructural básico} (BSM) es similar a método de Holt-Winters para datos estacionales e incluye componentes de nivel, tendencia y estacionalidad.

Otra clase de modelos de espacio de estado, conocida como \emph{modelos balanceados de espacio de estado}, se ha usado principalmente para predicción de series temporales macro-económicas. \cite{Mittnik1990337} proporcionó un estudio de esta clase de modelos, y \cite{Vinod1995217} obtuvo predicciones para el consumo, ingresos y tasas de interés usando estos modelos. Estos sólo tienen una única fuente de error aleatorio y generalizan otros modelos como ARMAX, ARMA y modelos con retardo racional distribuido. Una clase de espacio de estados relacionada son los modelos de \emph{fuente única de error} en los que subyacen los métodos de alisado exponencial.

A parte de estos desarrollos metodológicos, existen varios artículos proponiendo modelos innovadores de espacio de estados para resolver problemas prácticos de predicción. Entre ellos, \cite{Coomes1992473} usó un modelo para predecir el empleo según la industria para regiones locales y \cite{Patterson1995395} usó un enfoque de espacio de estado para predicción de ingresos personales disponibles. 

\subsection{Modelos no lineales}

La suposición de linealidad es útil y una herramienta potente en muchas áreas. Sin embargo, a finales de los 70 y principios de los 80, los modelos lineales eran insuficientes en muchas aplicaciones reales. Por ejemplo, los ciclos prolongados de tamaño de población animal, los ciclos solares (número anual de manchas solares), flujo de energía y relaciones de amplitud-frecuencia, son ejemplos donde no se adecuan los modelos lineales. Para satisfacer esta demanda, se propusieron modelos no lineales de series temporales útiles durante ese periodo.

Un factor que probablemente ha retrasado la difusión de publicaciones de predicciones no lineales es que hasta entonces no era posible obtener expresiones analíticas cerradas para predicciones multi-paso. Hoy día, las predicciones no lineales se obtienen mediante simulación Monte Carlo o por \emph{bootstrapping} (remuestreo). El último caso se prefiere al no necesitar suposiciones sobre la distribución del proceso de error.

\subsubsection{Modelos de cambio de régimen}
Los modelos no lineales autorregresivos con umbral auto-excitado (SETAR) fueron difundidos por \cite{Tong1990}.  \cite{Clements1997463} compararon varios métodos para obtener predicciones mult-paso para modelos SETAR univariados discretos. Concluyeron que las predicciones mediante simulación Monte Carlo eran satisfactorias en casos donde se sabe que las perturbaciones del modelo SETAR vienen de una distribución simétrica. En otro caso se prefiere el método de \emph{bootstrapping}. 

Una desventaja del modelo SETAR es que la dinámica cambia de forma discontinua de un régimen a otro. En cambio, un modelo de transición suave (STAR) permite una transición más gradual entre los distintos regímenes. 

\cite{Fok2005785} se preguntaron lo siguiente: ¿Puede un modelo STAR multinivel de datos de panel para series desagregadas mejorar las predicciones de agregados macroeconómicos como ingresos totales o desempleo total?. El modelo STAR propuesto parece merecer la pena investigarse en más detalle puesto que permite que los parámetros que gobiernan el cambio de régimen sean distintos según el estado. Basándose en simulaciones y hallazgos empíricos, los autores afirman que de hecho se pueden conseguir mejoras en las predicciones a un paso.

\cite{Franses2004255} propusieron un modelo con umbral AR(1) que permite una inferencia plausible sobre valores específicos de los parámetros. La idea clave es que los valores de los parámetros de AR dependen de una variable indicadora dominante. El modelo resultante mejora el rendimiento respecto a otros modelos no lineales cambiantes en tiempo, incluyendo el modelo de cambio de régimen de Markov, en términos de predicción.

\subsubsection{Modelos con coeficiente funcional}
Un modelo AR con coeficiente funcional (FCAR o FAR) es un modelo AR en el que los coeficientes AR pueden variar como una función suave de otra variable, como un valor retrasado de la propia serie o una variable exógena. El modelo FCAR incluye los modelos TAR y STAR como casos especiales y es análogo al modelo aditivo generalizado de \cite{Hastie1990}. 

\subsection{Modelos de memoria larga}
Cuando el parámetro de integración \emph{d} de un modelo ARIMA es fraccional y mayor que cero, el proceso exhibe memoria en el sentido de que las observaciones de un periodo de tiempo largo, tiene una dependencia apreciable. Los modelos estacionarios con memoria larga ($0 < d < 0.5$), también llamados ARMA diferenciados fraccionalmente (FARMA) o modelos ARMA integrados fraccionalmente (ARFIMA), han sido estudiados en muchos campos; véase \cite{Granger198015} para una introducción. Una motivación para estos estudios es que muchas series temporales empíricas tienen una función de autocorrelación muestral que decae a una tasa menor que en un modelo ARIMA de orden finito y entero \emph{d}.

\cite{Ray1993255} y \cite{Ray1993511} realizaron un estudio comparativo entre los modelos FARMA/ARFIMA estacionales y los modelos estándar (no fraccionarios) ARIMA estacionales. Los resultados muestran que los modelos AR de mayor orden son capaces de predecir bien a largo plazo comparados con los modelos ARFIMA. 

\subsection{Modelos ARCH/GARCH}
Una característica clave de las series temporales financieras es que las rentabilidades grandes tienden a continuar rentabilidades grandes y lo mismo con rentabilidades pequeñas, es decir, hay periodos que muestran una alta (o baja) volatilidad. Este fenómeno se conoce como el agrupamiento de volatilidad en econometría y finanzas. La clase de modelos AR condicionados a heteroscedasticidad (ARCH), introducidas por \cite{Engle1982987}, describen los cambios dinámicos de la varianza condicionada como una función determinista (típicamente cuadrática) de rentabilidades pasadas. Puesto que se conoce la varianza en tiempo -1, las predicciones a un paso están disponibles. A continuación, las predicciones multipaso pueden ser computadas de forma recursiva. El modelo ARCH generalizado (GARCH) (\cite{Bollerslev19942959} y \cite{Taylor1987159}) permite dependencias adicionales de los retardos de la varianza condicional. Un modelo GARCH tiene una representación tipo ARMA, de forma que los modelos comparten muchas propiedades.

Paralelamente a GARCH, muchos otros modelos se han propuesto para predicción de volatilidad.  \cite{Poon2003478}, proporcionan una revisión cuidadosamente realizada de la investigación en este área durante los últimos 20 años. 

El estudio de Poon y Granger contiene muchas cuestiones para continuar estudiando. Por ejemplo, los modelos asimétricos GARCH consiguieron buenas puntuaciones en los concursos de predicción. Sin embargo, no está claro hasta qué punto esto se debe a las asimetrías de la media condicional, de la varianza condicional, y/o de momentos condicionales de mayor orden. Otra cuestión para investigación futura concierne la combinación de predicciones. Sería también útil examinar el rendimiento de predicción de volatilidad de modelos tipo GARCH multivariados y no lineales, incorporando a ambos dependencias contemporáneas y temporales.

\subsection{Predicción de datos de recuento}
Los datos de recuento son frecuentes especialmente en datos de inventario donde son llamados \emph{datos de demanda intermitente}. Consecuentemente, es sorprendente que haya tan poco trabajo realizado en predicción de estos datos. Hay algo sobre métodos a medida para predicción de datos de recuento, pero pocos artículos tratan sobre predicción de series temporales de datos de recuento usando modelos estadísticos.

La mayoría de la investigación en predicción de recuentos está basada en \cite{Croston1972289} quien propuso usar SES para predecir independientemente los valores no nulos de las series y los intervalos de tiempo entre ellos. \cite{Willemain1994529} compararon el método de Croston con SES y encontraron que el método de Croston era más robusto, aunque estos resultados se basaban en en MAPEs que a menudo no están definidos para datos de recuento. Las condiciones bajo las cuales el método de Croston mejora al SES fueron discutidas en \cite{Johnston1996297} \cite{Willemain2004375} propusieron un procedimiento de remuestreo para datos de demanda intermitente que se encontró que era más preciso que el de SES y el de Croston para las nueve series evaluadas.

La evaluación de predicciones de recuento conlleva dificultades debido a la presencia de ceros en los datos observados. \cite{Syntetos2005303} propusieron usar el error absoluto medio relativo, mientras que \cite{Willemain2004375} recomendaron usar el método de transformación integral de probabilidad de \cite{Diebold1998863}.


\subsection{Evaluación de predicción y medidas de precisión}
Se ha usado un variado conjunto de medidas de precisión para evaluar el rendimiento de los métodos de predicción. Algunos de los cuales se listan en \cite{Mahmoud1984139}. A continuación definimos las medidas más comunes. 

\begin{table}
\caption{Medidas normalmente usadas para precisión de predicción}
\begin{center}
%\resizebox{12cm}{!}{
\begin{tabular}{|C{2cm}|L{4cm}|C{5cm}|}\hline %
\bfseries Medida & \bfseries Descripción & \bfseries Expresión
\\\hline
\csvreader[separator=semicolon,head to column names, late after line=\\\hline]{medidasError.csv}{}%
{\medida & \descripcion & \expresion }% 

\end{tabular}
%}
\end{center}
\end{table}

La evolución de las medidas de precisión y evaluación de predicción puede verse a través de las medidas usadas para evaluar los métodos en la mayoría de los estudios comparativos realizados. En la competición original M (\cite{Makridakis1982111}) las medidas usadas incluían MAPE, MSE, AR, MdAPE y PB. Sin embargo, como apuntaron \cite{Chatfield198819} y \cite{Armstrong199269}, el MSE no es apropiado para comparaciones entre series puesto que depende de la escala. MAPE también tiene problemas cuando la serie tiene valores cercanos a cero o cero, como apuntaron \cite{Makridakis1983}. En la competición M se evitaron MAPEs excesivamente grandes (o infinitos) incluyendo sólo datos positivos. Sin embargo, es una solución artificial imposible de aplicar en todas las situaciones.

La competición M3 (\cite{Makridakis2000451}) usó tres medidas de precisión: MdRAE, sMAPE y sMdAPE. Las medidas \emph{simétricas} fueron propuestas por \cite{Makridakis1993527} en respuesta a la observación de que MAPE y MdAPE tienen la desventaja de que penalizan más los errores positivos que los negativos. Sin embargo, estas medidas no son tan simétricas como su nombre sugiere. Para el mismo valor de Yt, el valor 2*Abs(Yt-Ft)/(Yt+Ft) tiene más penalización cuando las predicciones son altas en comparación a cuando son bajas. 

Es notable que ninguno de los estudios comparativos haya usado medidas relativas (a diferencia de medidas de errores relativos) tales como RelMAE o LMR. Esta última fue propuesta por \cite{Thompson1990219} quien abogó por su uso dadas sus buenas propiedades estadísticas. Fue aplicada a los datos de la competición M en \cite{Thompson1991331}.

Una novedosa medida alternativa de precisión es la \emph{distancia temporal}, que fue considerada por \cite{Granger2003199} y \cite{Granger2003339}. En esta medida se capturan también  las propiedades de dirección y desfase de la predicción. De nuevo, esta medida no ha sido usada en ningún estudio comparativo importante.

Una línea de investigación paralela ha examinado tests estadísticos para comparar métodos de predicción. Una contribución prontía fue \cite{Flores1989529}. El mejor enfoque conocido para comprobar diferencias entre la precisión de los métodos de predicción es el test de \cite{Diebold1995253}. \cite{Harvey1997281} propusieron una modificación de este test con tamaño corregido. \cite{McCracken2004503} examinó el efecto de estimación de parámetros en estos tests y proporcionó un nuevo método de ajuste para error de estimación de parámetros.


\subsection{Combinación de métodos}
Durante las últimas 3 décadas se ha estudiado la combinación, mezcla y puesta en común de predicciones obtenidas a partir de diferentes métodos de series temporales y diferentes fuentes de información. En una revisión bibliográfica exhaustiva, \cite{Clemen1989559} resumió una evidencia de la eficiencia relativa de predicciones combinadas, normalmente en términos de varianzas de error de predicción.

Se han propuesto numerosos métodos para seleccionar los pesos de combinación. La simple media es el método de combinación más utilizado (\cite{Bunn1985151}), pero el método no usa información pasada respecto a la precisión de las predicciones o la dependencia entre las predicciones. Otro simple método es la mezcla lineal de las predicciones individuales con pesos de combinación determinados por OLS (asumiendo que no hay sesgo) a partir de la matriz de predicciones pasadas y el vector de observaciones pasadas (\cite{Granger1984197}). Sin embargo, las estimaciones OLS de los pesos son insuficientes debido a la posible presencia de correlación en los errores de predicción combinados. \cite{Aksu199227} y \cite{Gunter199245} investigaron este problema y recomendaron el uso de predicciones combinadas OLS con la restricción de que los pesos sumen uno.

En lugar de usar pesos fijos, \cite{Deutsch199447} permitieron que estos cambiaran a lo largo del tiempo usando modelos STAR y otros modelos de cambio de régimen. Otro esquema de ponderación variante en el tiempo fue propuesto por \cite{Fiordaliso1998367}, quien usó un sistema difuso para combinar un conjunto de predicciones individuales de forma no lineal. 

La combinación de pesos determinada a partir de los métodos invariantes en tiempo puede llevar a predicciones relativamente pobres si ocurre no estacionariedad entre las diferentes componentes de predicción. \cite{Miller1992515} examinaron el efecto del desplazamiento de ubicación de la no estacionariedad en un conjunto de métodos de combinación. Concluyeron que la simple media batía otros artefactos de combinación complejos. El tema relacionado de combinar predicciones a partir de modelos lineales y otros no lineales, con pesos OLS  así como determinados por un método variante en el tiempo fue abordado por \cite{Terui2002421}.

La forma de la distribución del error de predicciones combinadas y el correspondiente comportamiento estadístico fue estudiado por de \cite{DeMenezes1998415} y \cite{Taylor1999325}. Para distribuciones de error de predicción  no normales, la oblicuidad surge como un criterio relevante para especificar el método de combinación. \cite{Fang200387} dio algunas ideas de por qué las predicciones en competición pueden ser fructuosamente combinadas para producir una predicción superior a las predicciones individuales abarcando tests de predicción. \cite{Hibon200515} propusieron un criterio para seleccionar entre diferentes predicciones y sus posibles combinaciones.

\subsection{Intervalos de predicción y densidades}
El uso de intervalos de predicción, y más recientemente densidades de predicción, ha ido siendo más usual a lo largo los últimos 25 años conforme los profesionales han sido más conscientes de las limitaciones de las predicciones puntuales. Una revisión importante y completa de los intervalos de predicción se da en \cite{Chatfield1993121}, resumiendo la literatura hasta el momento.

La mayoría de los intervalos de predicción están basados en un modelo estadístico subyacente. Por tanto, ha habido bastantes trabajos para formular modelos estadísticos apropiados para generalizar procedimientos de predicción comunes.

La incertidumbre asociada a la selección del modelo y la estimación de parámetros no se tiene en cuenta con la mayoría de intervalos de predicción basados en modelo. Por tanto, los intervalos son demasiado estrechos. Ha habido investigación considerable en cómo hacer que los intervalos de predicción basados en modelo tengan una cobertura más realista. Una serie de artículos apareció sobre el uso de remuestreo para crear intervalos de predicción basados en modelo para el modelo AR, empezando por incluyendo \cite{Mccullough1996293}, \cite{Grigoletto1998447}, \cite{Clements2001247} y \cite{Kim200485}. Procedimientos similares para otros modelos también han sido considerados, incluyendo modelos ARIMA (\cite{Pascual2005219} y \cite{Wall2002733}), VAR (\cite{Kim200485}), ARCH (\cite{Reeves2005237}) y regresión (\cite{Lam2002125}). Es probable que estos métodos de remuestreo se vayan a utilizar más a medida que las velocidades de cómputo aumenten por sus mejores propiedades de cobertura.

Cuando el error de predicción no sigue una distribución normal, hallar la densidad de predicción completa es útil puesto que un intervalo único puede no proporcionar un resumen adecuado del futuro esperado. Un resumen sobre densidad de predicción se da en  \cite{Tay2000235}. Resumiendo, una densidad de predicción ha sido objeto de interesantes propuestas incluyendo los \emph{gráficos de ventilador}  (\cite{Wallis1999106}) y las \emph{regiones de mayor densidad} (\cite{Hyndman1995431}). El uso de estos resúmenes gráficos ha crecido rápidamente en los años recientes y las densidades de predicción se utilizan de forma relativamente amplia.

Conforme los intervalos de predicción y las densidades van siendo mayormente usadas, la atención se centra en su evaluación y testeo. \cite{Diebold1998863} introdujeron el método notablemente sencillo de \emph{transformación integral de probabilidad}, el cual puede usarse para evaluar una densidad univariada. Este enfoque ha llegado a usarse ampliamente en un periodo de tiempo muy corto y ha sido un avance clave en este área. La idea se extiende para densidades de predicción multivariada en \cite{Diebold1999661}.

Otros enfoques para evaluar la densidad y los intervalos de predicción se da en \cite{Wallis2003165} quien propuso tests chi cuadrado para ambos intervalos y densidades y \cite{Clements2002397} que disertaron sobre algunos tests sencillos pero potentes para evaluar las densidades de predicción multivariadas.


\section{Métodos de Aprendizaje automático}
En aprendizaje automático (ML del inglés \emph{Machine Learning}) el área de aprendizaje supervisado consiste en el modelado, bajo un conjunto de observaciones finitas, de la relación entre un conjunto de variables de entrada y una o más variables de salida, las cuales se consideran de alguna manera dependientes de la entrada. Una vez que el modelo de esta relación se obtiene, es posible usarlo para predicción a un paso. En predicciones a un paso, los \emph{n} valores previos de la serie son la entrada para formular un problema genérico de regresión. 

En las últimas dos décadas los modelos de ML se han establecido como serios competidores a los modelos estadísticos convencionales en la comunidad de predicción (\cite{Ahmed2010594},\cite{Palit2005} y \cite{Zhang199835}). Estos modelos, también llamados de caja negra o modelos orientados a datos \cite{Mitchell1997}, son ejemplos de modelos lineales no paramétricos en los que sólo se usan datos históricos para aprender la dependencia estadística entre valores pasados y futuros. Werbos encontró que ANN mejoraba los métodos estadísticos clásicos tales como regresión lineal o el enfoque Box-Jenkins (\cite{Werbos1974} y \cite{Werbos1988339}). En un estudio similar realizado por \cite{Lapedes1987} se concluye que las ANNs pueden ser usadas satisfactoriamente para modelado y predicción de series temporales no lineales. Posteriormente, otros modelos han aparecido tales como árboles de decisión, máquinas de vectores soporte y regresión por vecinos más cercanos (\cite{Hastie2001} y \cite{Alpaydin2004}). Además se ha examinado en varias competiciones de predicción la precisión empírica de varios modelos de ML bajo diferentes condiciones de datos (NN3, NN5 y las competiciones ESTSP \cite{Crone2009456}\cite{Crone}\cite{Lendasse2007}) creando interesantes debates en el área de minería de datos y predicción (\cite{Hand2008}, \cite{Price2009452} y \cite{Crone2009456}). Un análisis empírico comparativo de algunos de los métodos de ML más utilizados es dado en \cite{ahmed2010empirical}. 


\subsection{Técnicas de aprendizaje local\\}
El papel de las técnicas de ML se revisa aquí bajo los siguientes aspectos: la formalización de problemas de predicción de un paso como tareas de aprendizaje supervisado, el uso de técnicas de aprendizaje local como herramienta para tratar datos temporales y el papel de la estrategia de predicción al pasar de predicciones de un paso a predicciones multi-paso. 

Si bien existen multitud de técnicas de aprendizaje supervisado para predicción, las técnicas de aprendizaje local están motivadas por las siguientes razones:
\begin{itemize}

\item No requieren demasiadas suposiciones: el aprendizaje local no supone un conocimiento a priori del proceso subyacente a los datos. La única información disponible se representa por un conjunto de pares entrada/salida. Esta característica es especialmente importante en conjuntos de datos reales donde puede haber valores perdidos y las relaciones no son estacionarias. 

\item Capacidad de aprendizaje en línea: el número de muestras de entrenamiento puede incrementarse en el tiempo. En este caso, el aprendizaje local simplemente añade los nuevos puntos al conjunto y no necesita re-entrenamiento al aparecer nuevos datos.

\item Modelado de no estacionariedad: los métodos de aprendizaje local pueden tratar configuraciones variables en el tiempo, donde el proceso estadístico subyacente de los datos no es estacionario. En este caso, es suficiente con interpretar la noción de vecindad en un sentido espacio-temporal. Para cada punto, los vecinos son muestras similares obtenidas recientemente. Por tanto la variable temporal es tenida en cuenta para mejorar la precisión de las predicciones.
\end{itemize}
La técnica de vecinos más cercanos y la técnica de aprendizaje vago son descritas a continuación:

\begin{itemize}
\item{\textbf{Vecinos más cercanos}}: El método de vecinos más cercanos es el ejemplo más sencillo de enfoque local aplicado al problema de predicción. Este método consiste en buscar en el conjunto de datos los vecinos más cercanos al estado actual y predecir que evolucionará de igual manera que lo hizo su vecindad. Este enfoque fue por primera vez propuesto por Lorenz \cite{LORENZEN1969636}. Algunas extensiones consideran más vecinos (aproximaciones de mayor orden, \cite{Ikeguchi1995}). \cite{Tong1980245} introdujeron análisis de series temporales por aproximación lineal por partes. \cite{Priestley1988} sugirió la importancia de aproximaciones de mayor orden. \cite{Farmer1987845} y \cite{Farmer1988} estudiaron el enfoque local y demostraron su efectividad en series temporales sobre varios experimentos numéricos. 

\item{\textbf{Aprendizaje vago}}: Esta técnica adapta automáticamente el tamaño de la vecindad según un criterio de validación cruzada. El atractivo de esta técnica es su naturaleza tipo \emph{divide y vencerás}, ya que reduce un problema complejo y no lineal a una secuencia de problemas locales manejables. Esto permite explotar el rango completo de técnicas de validación e identificación las cuales son rápidas, fiables y poseen una justificación teórica profunda.
El procedimiento consiste básicamente en los siguientes pasos: 
\begin{enumerate}
\item{} Ordenar de forma ascendiente el conjunto de vectores según la distancia al vector sobre el que se quiere predecir el siguiente valor.
\item{} Determinar el número óptimo de vecinos.
\item{} Calcular, dado el número de vecinos, la predicción mediante un modelo local (constante o lineal).
\end{enumerate}
Este método ha sido aplicado con éxito a varios problemas de regresión y predicción \cite{Bontempi1999}. En \cite{Birattari1999375} \cite{Bontempi199932} se dan más detalles sobre la técnica de aprendizaje vago y sus aplicaciones.
\end{itemize}

\subsubsection{Predicción para horizontes mayores de un paso\\}
A continuación se consideran tres técnicas para abordar el problema de predicción multi-paso: 
\begin{enumerate}
\item{Mediante recursividad}. Primero se entrena un modelo de un paso y se utiliza para una predicción multi-paso. Una desventaja conocida de este método es su sensibilidad al error estimado pues se acumula en predicciones sucesivas.
\item{De forma directa}. Esta estrategia entrena tantos modelos como pasos del horizonte, y concatena las soluciones de cada uno formando la predicción del horizonte. No está sujeta a acumulación de errores pero requiere mayor complejidad funcional \cite{Tong1983} para modelar la dependencia entre valores de la serie distantes \cite{Guo1999559}. También tiene la desventaja de ser más costosa computacionalmente al tener que aprender tantos modelos como pasos tenga el horizonte. Entre los modelos usados para implementar este método destacan las redes neuronales \cite{Kline2004226} y árboles de decisión \cite{Tran20099378}.
\item{Predicción híbrida}. Aquí se combinan las dos arquitecturas anteriores \cite{Sorjamaa2006143}. Para cada horizonte se calcula un modelo pero se añaden los puntos de entrada predichos.
\end{enumerate}

\subsubsection{Estrategia de múltiples salidas\\}
El problema de los enfoques mencionados anteriormente es que obvian la existencia de dependencias estadísticas entre los valores de predicción consecutivos. Para remediar esto, se utilizan modelos para aprender la relación entre el vector de valores pasados y otro vector de valores futuros. La estrategía MIMO (\emph{Multi-Input Multi-Output}) (\cite{Bontempi2008145} y \cite{Bontempi2011689}) evita obviar la dependencia condicional entre los valores futuros aprendiendo un modelo con múltiples salidas. Este método ha sido aplicado a varios problemas reales (\cite{Bontempi2008145},  \cite{Bontempi2011689}, \cite{BenTaieb20093054} y \cite{BenTaieb20101950}), sin embargo presenta la desventaja de tener que usar la misma estructura del modelo para predecir cualquier horizonte (\cite{BenTaieb20093054}). Una posible variante para superar esta falta de flexibilidad consiste en particionar el horizonte de tamaño \emph{H} en \emph{m} bloques de tamaño \emph{s} y usar MIMO para predecir en cada bloque, lo que se conoce como DIRMO (mezcla entre DIRecta y MIMO). Esto permite calibrar la dependencia de las salidas (sin dependencia en caso de \emph{s = 1} y máxima dependencia con \emph{s = H}

\subsubsection{Aprendizaje local para predicción multipaso\\}
A continuación se discuten algunos trabajos que usan técnicas de aprendizaje local para tratar específicamente el problema de predicción a largo plazo.  En \cite{McNames1998112} y \cite{Bontempi199932} se modifica el aprendizaje local para tener en cuenta el comportamiento temporal de la predicción multi-paso y mejorar así la estrategia recursiva. Otra mejora reciente de la estrategia recursiva es RECNOISY (\cite{BenTaieb2011}), la cual introduce perturbaciones en el conjunto de entrenamiento para cada paso del proceso de predicción de forma que pueda manejar de manera más adecuada los valores aproximados. 

Dos mejoras de aprendizaje vago fueron presentadas en \cite{Sorjamaa2005509}. La primera se basa en una poda iterativa de las entradas, la segunda realiza búsqueda por fuerza bruta en el conjunto de posible entradas mediante una aproximador de \emph{k} vecinos más cercanos (K-NN).

En \cite{Bontempi2008145} se propone aprendizaje local para predicción MIMO donde se extiende el algoritmo básico de aprendizaje vago para múltiples salidas así como una estrategia de ponderación de los diferentes predictores a largo plazo para mejorar la precisión resultante.

En  \cite{BenTaieb2009}  \cite{BenTaieb20101950} se presenta la estrategia DIRMO con aprendizaje local. Este método ha sido aplicado con éxito en las competiciones de predicción: ESTSP'07 (\cite{BenTaieb2009}) y NN3 (\cite{BenTaieb20101950}).
Una revisión más detallada y comparativa de las estrategias para predicción de series temporales multi-paso basadas en el algoritmo de aprendizaje local se puede encontrar en \cite{Roberts1982808}.


\subsection{Redes neuronales}

Una red neuronal artificial (ANN) puede ser útil para procesos no lineales que tengan relaciones funcionales desconocidas y por tanto difíciles de ajustar (Darbellay y Slama, 2000 \cite{Darbellay200071}). La idea principal con ANN es que las entradas, o variables dependientes, son filtradas mediante una o más capas ocultas cada una de las cuales consta de nodos antes de alcanzar la variable de salida. La salida intermedia se enlaza a la salida final. Otros modelos son versiones específicas de ANN donde se impone cierta estructura (véase JoF Special Issue 17:5/6 (1998).

Una de las mayores áreas de aplicación de ANN es predicción; véase \cite{Zhang199835} y \cite{Hippert200144} para revisiones de la literatura. Sin embargo no hay un claro consenso sobre cuándo es mejor utilizar ANNs para predicción.

\cite{Gorr19941} y \cite{Hill19945} sugirieron que la investigación a seguir debería tratar de definir mejor los límites en que ANN mejora las técnicas tradicionales y viceversa. 

Un problema general con los modelos no lineales se encuentra en la complejidad de los modelos y su excesiva parametrización. Si se considera realmente importante el principio de parsimonia, es interesante comparar el rendimiento de predicción fuera de muestra de los modelos lineales frente a los no lineales, usando una amplia variedad de criterios de selección de modelo. Esta cuestión fue considerada en bastante profundidad por \cite{Swanson1997439}. Sus resultados sugirieron que una simple ANN con una única capa oculta \emph{feed-forward}, la cual era muy popular en series temporales econométricas, ofrece una alternativa útil y flexible a modelos lineales de especificación fija, particularmente para horizontes de predicción mayores de un paso. En contraste con Swanson \cite{Heravi2004435} encontraron que los modelos lineales producen predicciones más precisas de producción industrial mensual europea sin ajuste estacional que los modelos de ANN. \cite{Ghiassi2005341} presentaron una ANN dinámica y compararon su rendimiento de predicción frente a la ANN tradicional y los modelos ARIMA.

Con el tiempo, la importancia del riesgo de la parametrización excesiva y el sobreajuste ha sido reconocida por varios autores; véase \cite{Hippert2005425} que usaron una ANN grande (50 entradas, 15 neuronas ocultas y 24 salidas) para predecir perfiles de carga de electricidad diaria. Sin embargo, la cuestión de si la ANN está sobre-parametrizada o no, sigue sin resolverse. Algunas ideas con valor potencial para construir ANNs minimizando el número de parámetros, usando inferencia estadística son las sugeridas en \cite{Terasvirta2005755}.

\subsubsection{Selección de retardos}

La propia selección de variables de entrada para construir el modelo es en sí mismo un problema de minería de datos. En este caso es necesario encontrar la dimensión de entrada mínima capaz de representar la relación entre los datos históricos. El teorema de Takens \cite{Takens1981366} establece que una dimensión alta permite reconstruir correctamente un espacio de estados que garantiza una dinámica topológicamente idéntica a las dinámicas de los espacios de estado del sistema real. Esta elección de la dimensión se conoce como la dimensión activa o dimensión intrínseca.

En \cite{ferreira2008new} se presenta un método evolutivo inspirado en el teorema de Takens para un modelo iterativo híbrido compuesto por una ANN con un algoritmo genético. El método primero itera sobre el algoritmo genético para obtener una solución con mínimo \emph{fitness} y determinar así la dimensión activa. Posteriormente la ANN es ajustada. 

Lukoseviciute y Ragulskis \cite{lukoseviciute2010evolutionary} dividen la selección de retardos en dos etapas, encontrando primero la dimensión óptima del espacio de fases mediante el algoritmo de falsos vecinos más cercanos y a continuación buscando un conjunto óptimo de retardos más cercanos mediante un algoritmo evolutivo para un sistema de inferencia difuso.

Otros métodos utilizados para determinar la dimensión intrínseca son el análisis de componentes principales (PCA) el método de Grassberger-Procaccia \cite{grassberger2004measuring} y  el método de Box-Counting. 


\subsection{El perceptrón multicapa (MLP)}

El perceptrón multicapa, a veces simplementa llamado red neuronal, es quizá la arquitectura de red más ampliamente conocida y usada tanto para clasificación como para regresión \cite{bishop1995neural}. Su arquitectura consiste en múltiples capas de neuronas que permiten resolver problemas no linealmente separables. El algoritmo de retro-propagación mediante descenso de gradiente para su entrenamiento fue un importante hallazgo gracias a su eficiencia. Otro método que se sabe que mejora la eficiencia es Levenberg-Marquard. Sin embargo presenta algunas limitaciones a la hora de extrapolar y también es sensible a los mínimos locales. En cualquier caso este tipo de red ha sido ampliamente utilizado para predicción de series temporales. Una revisión de su uso en este campo es dada por \cite{hippert2001neural}, \cite{azoff1994neural} y \cite{hoptroff1993principles}.

\subsection{Red neuronal bayesiana (BNN)}
Una red neuronal bayesiana es una red neuronal diseñada sobre la base de la formulación probabilista bayesiana (\cite{mackay1992bayesian}). La idea es tratar los parámetros (pesos) de la red como variables aleatorias que obedecen una distribución a priori. Esta distribución se diseña de forma que favorezca modelos con baja complejidad (y por tanto se obtenga un ajuste suave). Las BNN están relacionadas con el concepto de regularización y la estimación bayesiana de parámetros. Una vez observados los datos, la distribución posterior de los pesos se evalúa y se calcula la predicción de la red. Las predicciones reflejan tanto el aspecto de fluidez impuesto por la distribución a priori, como el ajuste preciso impuesto por los datos observados (véase \cite{dan1997gauss} para los detalles de un algoritmo concreto de entrenamiento).

\subsection{Red de funciones de base radial (RBFN)}
Este tipo de red neuronal se caracteriza por el uso de una función de base radial como función de activación de las neuronas. La salida de la red es una combinación lineal de las funciones de base radial aplicadas a la entrada. Aparecen por primera vez en el trabajo de \cite{broomhead1988radial}. 

\cite{chen1991orthogonal} es de los primeros en introducir su uso para predicción de series temporales. A partir de entonces numerosos trabajos han aparecido para tratar las cuestiones relacionadas con su uso predicción de series temporales. Su principal ventaja es que los pesos de la red puede ser calculados de forma eficiente una vez estimados los pesos y los radios de las neuronas. A cambio, existen otros parámetros a determinar, como el número de neuronas, los centros y los pesos.

Los centros de la red pueden calcularse con diferentes técnicas de agrupamiento, los radios en cambio, son más difíciles de calcular debido a la existencia de numerosos mínimos locales. Lo que se suele hacer es fijarlos a una cantidad fija, o dependiente de alguna heurística. En \cite{whitehead1996cooperative} se propuso usar algoritmos evolutivos, en concreto se utiliza un algoritmo genético cooperativo-competitivo. En los problemas de predicción un tema importante es la no estacionariedad, en \cite{chng1996gradient} abordaron este problema modificando la estructura de la red para que responda al gradiente de la serie. El problema del número de neuronas adecuado es tratado en \cite{leung2001prediction} donde proponen un algortimo basado en validación cruzada para encontrar el óptimo.

\subsection{Sistemas difusos}
La lógica difusa es una forma de lógica multi-valuada que trata el razonamiento como algo aproximado en lugar de exacto donde los valores de verdad están entre 0 y 1. Este valor indica la vaguedad del hecho dado mediante una función de pertenencia mientras que en probabilidad indica la incertidumbre sobre tal hecho. Ambos conceptos son sutilmente diferentes lo que generó una polémica en tanto que las funciones de pertenencia en realidad no representaban incertidumbre. Como consecuencia se añadió una tercera dimensión a la función de pertenencia para añadir el concepto de incertidumbre. Cuando ésta se representa mediante un intervalo las operaciones aritméticas resultan más fáciles de tratar \cite{liang2000interval}. En general los sistemas difusos de mayor orden están todavía en su infancia.

Entre aquellos que usan sistemas difusos para predicción, son notables los siguientes trabajos:

\begin{itemize}

\item En \cite{jang1993anfis} se presentó la arquitectura y los métodos de aprendizaje del sistema de inferencia difusa basado en red adaptativa (ANFIS), los cuales fueron comparados con otras técnicas de modelos difusos anteriores y redes neuronales obteniendo resultados que mejoraban el rendimiento.

\item También fue destacable el trabajo \cite{gao2005narmax}  donde se aborda la predicción mediante un modelo NARMAX (modelo de media móvil autoregresivo no lineal con variables exógenas) ajustado mediante una metodología de redes neuronales difusas (FNN) tanto hacia adelante como recurrentes.

\item En \cite{kasabov2002denfis} se introdujo un nuevo tipo de sistemas de inferencia difusa denominado DENFIS (sistema de inferencia neuro-difuso dinámico evolutivo), el cual puede trabajar tanto en línea como fuera de línea.

\end{itemize}

\subsection{Máquinas de vectores soporte (SVM)}
Las SVM son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT\&T.

Intuitivamente una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificación o regresión. Una buena separación entre las clases permitirá una mejor clasificación.

Las máquinas de vectores soporte para predicción han sido estudiadas en numerosos trabajos. Algunos trabajos relevantes son:
\begin{itemize}

\item En \cite{cao2003support}, \cite{Tay2001309} y \cite{kim2003financial} se demuestra que pueden obtener un mejor rendimiento que el algoritmo de retro-propagación para predecir series temporales en finanzas. 
\item \cite{Cao2003321} sugiere utilizar varias redes neuronales mediante SVM (expertos) para entrenar sobre las diferentes regiones disjuntas del espacio de entrada obtenidas mediante mapas auto-organizativos de características y aplica esta técnica con éxito sobre varias series de referencia en un estudio comparativo.
\item También se ha intentado hibridar la capacidad de SVM con los métodos estadísticos convencionales de tipo ARIMA en \cite{pai2005hybrid}.
 
\end{itemize}


\subsection{Árboles de regresión y clasificación (CART)}
Este modelo se basa en una partición del espacio de entrada en forma jerárquica con forma de árbol (\cite{breiman1984classification}). El árbol se compone de nodos internos de decisión y hojas. Dado un dato se determina la ruta hasta el nodo hoja cuyo modelo local asociado es usado para la predicción.  Para construir el árbol mediante los datos de entrenamiento básicamente se procede de la siguiente forma: 
Partiendo del nodo raíz, se selecciona la variable (y el umbral de partición) que minimiza el error cuadrático medio, este proceso se repite recursivamente hasta alcanzar un error cuadrático medio aceptable. Una vez que se obtiene el árbol se suele realizar algún tipo de poda para eliminar nodos inefectivos y disminuir la complejidad del modelo.



\section{Modelos aplicados a predicción del tráfico}

En general las prácticas actuales en la administración y control de estrategias de tráfico están dominadas por el uso emergente de sistemas de transporte inteligente (STI), el rápido desarrollo de los computadores y la existencia de métodos algorítmicos flexibles. En los últimos años un gran número de trabajos han aparecido describiendo diversas especificaciones matemáticas para modelar las características del tráfico y producir predicciones bajo diferentes condiciones, no existiendo un claro consenso en los diferentes requerimientos involucrados durante el modelado.  Como ejemplo en \cite{Treiber2010983} se discute el término de fase de tráfico en la reproducción de los patrones espacio-temporales de congestión y la necesidad de una definición más precisa de los términos científicos empleados mientras que en \cite{boyce2002sequential} se revisa el paradigma de predicción actual sugiriendo que los métodos actuales pueden llegar a ser contraproductivos.

En cuanto a los STI, a pesar de las muchas iniciativas que han surgido como respuesta a los retos de una rápida urbanización y una creciente congestión del tráfico, todavía existe una falta de comprensión acerca de cuál es su definición apropiada. En \cite{Debnath201447} se proporciona un marco metodológico para evaluar este concepto a través de diferentes indicadores aplicables a una ciudad. De esta forma evalúan varias ciudades y obtienen para cada una el grado de \emph{inteligencia} según la puntuación obtenida por los indicadores.

Una revisión de los modelos y teorías sobre tráfico fue proporcionada en \cite{Hoogendoorn2001283} donde son clasificados según el nivel de detalle con que se describe el flujo de vehículos. Para cada categoría discuten varias cuestiones como precisión del modelo, su aplicabilidad o calibración y validación.  En esta línea \cite{Vlahogianni2004533} propone un marco de trabajo para el desarrollo de modelos de predicción de tráfico a corto plazo que consiste en desagregar el proceso de modelado en tres partes: determinar el ámbito, el proceso conceptual de especificación de la salida y el proceso de modelado en sí, que incluye la selección del enfoque metodológico apropiado, el tipo de entradas y salidas usadas junto a la calidad de los datos.

A nivel de hardware, los sistemas de detección inalámbricos son una promesa en la mejora de las soluciones de modelado, estimación y control de tráfico en el marco de los STI según sugiere \cite{tubaishat2009wireless}.



\subsection{Métodos convencionales}
En la literatura encontramos una gran variedad de métodos estadísticos y algunos métodos híbridos aplicados a la predicción del tráfico:
\begin{itemize}
\item En \cite{smith2002comparison} se examina si los métodos de regresión no paramétricos basados en heurísticas mejoradas se acercan al rendimiento de los modelos ARIMA estacionales para predecir tráfico a corto plazo.
\item \cite{hamed1995short} utiliza el enfoque Box-Jenkins para predicción a corto plazo (intervalos de un minuto) en las principales arterias urbanas de Amman, estableciendo ARIMA(0, 1, 1) como el modelo más adecuado.
\item ARIMA también fue utilizado en combinación con mapas auto-organizativos para predicción de horizontes de media hora y una hora, en \cite{van1996combining}. La idea es que cada clase obtenida del mapa posea su propio modelo ARIMA. 
\item El modelo SETAR es propuesto en \cite{ishak2002performance} para evaluar su sensibilidad en diferentes configuraciones y condiciones del tráfico.
\item Otro modelo lineal de cambio de régimen es propuesto en \cite{zhang2003short} 
\item En \cite{clark2003traffic} se usa una técnica de reconocimiento de patrones basada en un modelo de regresión multivariado no paramétrico.
\item El modelo STARIMA (\emph{Space-Time ARIMA}) es usado en \cite{kamarianakis2005space} para representar y predecir tráfico estacionario en consonancia con las teorías del equilibrio del flujo de tráfico de \cite{wardrop1952correspondence}. 
\item Un modelo similar STARMA es empleado en \cite{min2011real}.
\end{itemize}

\subsection{Redes neuronales}
La cuestión de cuándo usar una red neuronal o un modelo estadístico para predecir el tráfico es estudiada en \cite{kirby1997should} donde se discute la relación de estos modelos a la estructura de los datos de tráfico. Además de esta cuestión, en la literatura encontramos un uso extensivo de redes neuronales aplicadas a predicción de tráfico con enfoques muy variados en cuanto al modelo de red neuronal y los algoritmos de entrenamiento utilizados. Entre los trabajos explorados se enumeran a modo de ejemplo los siguientes:

\begin{itemize}
\item En \cite{Ishak2004452} se realiza una comparación del rendimiento de cuatro arquitecturas diferentes tipo MIMO, a saber: el perceptrón multi-capa (MLP), una red modular que consiste en varios MLPs conectados y recombinados, una red de dos capas ocultas con PCA a la entrada y un sistema de inferencia neuro-difuso denominado CANFIS.
\item En \cite{Innamaa2005649} se estudia la capacidad de predicción del tiempo de viaje mediante una MLP demostrando que un modelo relativamente sencillo puede obtener buenos resultados.
\item En \cite{Zhang2000472} se utiliza un enfoque recursivo para predecir con éxito las condiciones de tráfico mediante red neuronal, si bien destacan posibles problemas de sensibilidad en los parámetros y la necesidad de llevar a cabo suficientes experimentos.
\item Otro modelo de red neuronal consistente en una capa de entrada, una capa competitiva y otra de interpolación fue  propuesta en \cite{Dharia2003607} para reducir el tiempo de entrenamiento de backpropagation en un entorno simulado llamado TSIS (Traffic Software Integrated System). 
\item En \cite{abdulhai2002short} se emplea un modelo de red neuronal con retardos temporales (Time Delay Neural Network) evolucionado mediante algoritmos genéticos para predicción de tráfico a corto plazo.
\item El enfoque genético utilizado en \cite{vlahogianni2005optimized} permite seleccionar una estructura de red neuronal adecuada evaluando su rendimiento tanto en datos univariados como multivariados.
\item También se ha usado con éxito un modelo neuro-difuso en \cite{yin2002urban} para predecir el flujo de tráfico en una red urbana de calles señalizadas.
\item El modelo de redes neuronales de espacio de estados (SSNN) es evaluado en \cite{van2006reliable} para estudiar su fiabilidad arrojando buenos resultados. Si bien, en los casos de estudio empleados los detectores están poco espaciados y el entrenamiento se realiza fuera de línea.
\item \cite{chen2001use} estudia un algoritmo en linea para aprendizaje dinámico sobre red neuronal comparando su rendimiento con el método de entrenamiento basado en el filtro de Kalman.
\end{itemize}


\subsection{Máquinas de vectores soporte}

En \cite{castro2009online} se presentó un método de predicción basado en regresión mediante SVM para su uso en línea. Los resultados obtenidos fueron comparados con el modelo de máxima probabilidad gaussiano (GML), alisado exponencial y redes neuronales. Los resultados claramente son mejores que GML tanto en condiciones de tráfico normales como excepcionales.


\subsection{Redes bayesianas}
\cite{castillo2008predicting} abordan la cuestión de predicción utilizando una red bayesiana que proporciona la densidad conjunta de todas las variables no observadas cuyas densidades marginales y condicionadas permiten realizar predicciones conjuntas e intervalos de probabilidad.

En \cite{sun2006bayesian} utilizan información de carreteras contiguas para modelar el flujo mediante una red bayesiana abarcando la cuestión de predicción con datos incompletos.


\subsection{Otros}

Dion y Rakha (2005) \cite{dion2006estimating} utilizaron datos de identificación automática de vehículos para probar un algoritmo de filtrado paso-baja adaptativo capaz de seguir la pista del tiempo medio de viaje en un enlace suprimiendo señales de ruido de alta frecuencia.
En \cite{tam2008using} se realizan algunas modificaciones de mejora sobre este algoritmo para obtener estimaciones más precisas de los tiempos de viaje.

Lam y Yin (2001) \cite{lam2001activity} presentan un modelo de asignación de tráfico basado en actividades diarias. Mediante una regresión múltiple formulan el comportamiento de elección de actividad de los individuos y utilizan después esta información para describir el comportamiento de elección de ruta como la condición de equilibrio dinámica ideal para el usuario.


\section{Conclusión y tendencias}

En las secciones previas, se han examinado algunos de los trabajos clave en la historia de las series temporales. En tales trabajos resulta interesante observar las propuestas de investigación que fueron identificadas. 

\cite{Chatfield198819} remarcó la necesidad de investigar métodos multivariados más prácticos. \cite{Ord1988389} también advirtió que no se había realizado suficiente trabajo en modelos de múltiples series temporales, incluyendo alisado exponencial multivariado. Hasta la fecha la predicción de series temporales multivariadas todavía no es aplicada ampliamente, a pesar  de los considerables avances en este área. Se sospecha que existen dos razones para ello: una falta de investigación empírica sobre algoritmos de predicción robustos para modelos multivariados, y la escasez de software fácil de usar. Algunos de los métodos sugeridos (modelos VARIMA) son difíciles de estimar por el gran número de parámetros que incluyen. Otros como el alisado exponencial multivariado, no han adquirido la suficiente atención teórica para su aplicación hoy por hoy. Un enfoque para predicción de series temporales multivariadas consiste en usar modelos de factor dinámico. Estos modelos promete tanto teóricamente (\cite{Forni2005830} y \cite{Stock20021167}) como para su aplicación (\cite{Pena2004291}) y se cree que llegarán a usarse más en el futuro.

\cite{Ord1988389} también indicó la necesidad de investigar más profundamente en los métodos de predicción basados en modelos no lineales. Por ejemplo, no existe un claro consenso de que las predicciones de los modelos no lineales superen categóricamente las de los modelos lineales (\cite{Stock19991}). 

Otros temas incluyen la necesidad de desarrollar procedimientos de selección de modelo que hagan un uso efectivo tanto de los datos como del conocimiento a priori, o la necesidad de especificar objetivos para las predicciones y desarrollar sistemas de predicción dirigidos hacia estos objetivos. Estas áreas todavía necesitan atención y se espera que aparezcan herramientas para solventar estos problemas. % me gusta esto

Dado el frecuente mal uso de métodos basados en modelos lineales con distribuciones de error gaussianas independientes e idénticas, \cite{Cogger1988403} abogó por que los nuevos desarrollos del área de métodos estadísticos \emph{robustos} recibieran más atención dentro de la comunidad de la predicción de series temporales. Un procedimiento robusto se espera que funcione bien cuando existan valores atípicos o desplazamientos de ubicación en los datos que sean difíciles de detectar. La estadística robusta puede estar basada tanto en métodos paramétricos como no paramétricos. Un ejemplo de este último, es el concepto de cuantiles de regresión de \cite{Koenker197833} investigado por Cogger. En predicción, estos puede aplicarse como cuantiles condicionados univariados y multivariados. Un área de aplicación importante es a la hora de obtener herramientas de manejo de riesgo como el \emph{valor en riesgo}. Recientemente, \cite{Engle2004367} realizaron un comienzo en esta dirección, proponiendo un modelo de valor en riesgo condicional. Se espera que haya muchas más investigación en este área.

Un tema relacionado en donde ha habido bastante actividad investigadora es la densidad de predicción, donde el foco está en la densidad de probabilidad de observaciones futuras más allá de la media o la varianza. Por ejemplo, \cite{Yao1995395} propusieron el concepto de intervalo de predicción de percentil condicional. Su anchura ya no es una constante, como en el caso de los modelos lineales, y puede variar respecto a la posición del espacio de estado a partir del que se realizan las predicciones.

En años recientes, las series temporales no gaussianas, han comenzado a recibir una atención considerable y los métodos de predicción van siendo desarrollados lentamente. En particular las series no gaussianas con valores positivos tiene importantes aplicaciones. Dos áreas importantes son la volatilidad y la duración entre transacciones. Algunas contribuciones importantes hasta la fecha han sido el modelo de \emph{duración condicional autorregresivo} de \cite{Engle19981127} y  \cite{Andersen2003579}. Dada la importancia de estas aplicaciones, se espera mucho más trabajo en este área.

Si bien, las series temporales no gaussianas con espacio de muestreo continuo han comenzado a recibir mayor atención de investigación, especialmente en el contexto de las  finanzas, la predicción de series temporales con un espacio muestral discreto (tales como las series de recuentos) todavía está en su infancia. Tales datos son muy frecuentes en la industria y los negocios y existen muchos problemas asociados a la predicción de recuentos sin resolver teóricamente o de forma práctica; por tanto, se espera una mayor investigación productiva en este área en el futuro cercano.

Otros autores han intentado identificar temas de investigación importantes. Tanto \cite{deGooijer1990449} como \cite{Clements20031}, sugirieron más trabajo en predicciones combinadas. Aunque el tema ha recibido una cantidad de atención considerable, todavía existen algunas cuestiones abiertas. Por ejemplo, cuál es el mejor método de combinación para modelos no lineales y lineales y qué intervalo de predicción puede establecerse en torno a la predicción combinada. Un buen punto de partida para mayor investigación en este área es \cite{Terasvirta2006}; también \cite{Armstrong2001}. Recientemente, \cite{Stock2004405} discutieron el llamado \emph{puzle de combinación de predicción}, a saber, el hallazgo empírico repetido de que combinaciones simples como las medias superan combinaciones más sofisticadas en las que la teoría sugiere un mejor funcionamiento. Esta importante cuestión práctica sin duda recibirá mayor atención de investigación.

Los cambios en el almacenamiento y recolección de datos también conducirá a nuevas direcciones de investigación. Por ejemplo, en el pasado, los paneles de datos disponibles (llamados datos longitudinales en bioestadística) tenían la dimensión temporal de la serie \emph{t} pequeña mientras que la dimensión de sección cruzada \emph{n} era grande. Sin embargo, ahora en muchas áreas aplicadas como marketing, se pueden obtener fácilmente grandes conjuntos de datos con ambos \emph{n} y \emph{t} grandes. La extracción de características a partir de los paneles de datos es el objeto del \emph{análisis de datos funcionales}. Sin embargo, el problema de realizar predicciones multi-paso basadas en datos funcionales sigue abierto tanto a investigación teórica como práctica. Dado el predominio creciente de este tipo de datos, se espera que este área sea fructífera en el futuro.

Los conjuntos de datos grandes, también se prestan a métodos de computación intensiva. Mientras las redes neuronales se han usado en predicción durante más de dos décadas, existen muchas cuestiones excepcionales asociadas con su uso e implementación, incluyendo el cuándo van a mejorar probablemente otros métodos. Otros métodos que implican computación pesada (como bagging y boosting) son incluso menos entendidos en el contexto de predicción. Con la disponibilidad de conjuntos de datos muy grandes y potencia alta de computación se espera que ésta sea un área de investigación importante.

En cuanto al uso de técnicas de predicción en el ámbito del tráfico vehicular, los métodos utilizados varían en gran medida debido a las diferencias fundamentales del modelado, objetivos y características de los datos. Tales diferencias ponen de manifiesto un campo de aplicación novedoso y poco desarrollado en el que las técnicas de predicción son un factor clave para su crecimiento. En este sentido se espera que aparezcan diferentes trabajos de comparación empírica que permitan a los profesionales seleccionar las técnicas y modelos más adecuados a los objetivos teniendo en cuenta las tecnologías y sistemas de información existentes o en estado de implementación.


\bibliographystyle{apalike-es}

\bibliography{bibliography}


\end{document}

%%Está chulo el trabajo, el STATE OF ART es de premio :)
